\documentclass[10pt,a4paper]{report}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[font=small,format=plain,labelfont=bf,up,textfont=it,up]{caption}
\usepackage[nottoc]{tocbibind}
\usepackage{url}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage[titles]{tocloft}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}

\setcounter{secnumdepth}{5}
\setlength{\parindent}{0in}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{darkred}{rgb}{0.9,0,0}

\lstset{frame=tb,
  language=C++,
  aboveskip=5mm,
  belowskip=5mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  moredelim=**[is][\color{darkred}]{@}{@},
  breakatwhitespace=true,
  tabsize=3
}

\newcounter{example}
\newcounter{algorithmCode}

\makeatletter
\lstnewenvironment{example}[2]{
  \renewcommand\lstlistingname{Example}
  \let\c@lstlisting=\c@example
  \let\thelstlisting=\theexample
  \lstset{caption=#1, label=#2}
} {}

\lstnewenvironment{algorithmCode}[2]{
  \renewcommand\lstlistingname{Algorithm}
  \let\c@lstlisting=\c@algorithmCode
  \let\thelstlisting=\thealgorithmCode
  \lstset{caption=#1, label=#2}
} {}
\makeatother

\makeatletter
\newcommand\ackname{Acknowledgements}
\if@titlepage
  \newenvironment{acknowledgements}{
      \titlepage
      \null\vfil
      \@beginparpenalty\@lowpenalty
      \begin{center}%
        \bfseries \ackname
        \@endparpenalty\@M
      \end{center}}%
     {\par\vfil\null\endtitlepage}
\else
  \newenvironment{acknowledgements}{
      \if@twocolumn
        \section*{\abstractname}
      \else
        \small
        \begin{center}
          {\bfseries \ackname\vspace{-.5em}\vspace{\z@}}
        \end{center}
        \quotation
      \fi}
      {\if@twocolumn\else\endquotation\fi}
\fi
\makeatother

\title{CuEira, Gene-Environment Interaction Analysis on GPU, a first draft}
\author{Daniel Berglund}
\date{October 2014}

\begin{document}
\maketitle
\thispagestyle{empty}

\clearpage
\thispagestyle{empty}
\selectlanguage{english}
\begin{abstract}
This is a first draft of a report for a masters thesis in computer science. The aim is to make a program for gene-environment interaction search using GPUs to speed up the calculations. The preliminary results show that there likely is a speed gain when using larger datasets.
\end{abstract}

%\clearpage
%\thispagestyle{empty}
%\selectlanguage{english}
%\begin{abstract}
%Abstract p√• svenska
%\end{abstract}

%\clearpage
%\thispagestyle{empty}
%\begin{acknowledgements}
%Lorem ipsum
%\end{acknowledgements}

\clearpage
\tableofcontents
\thispagestyle{empty}

\clearpage
\setcounter{page}{1}
\chapter{Introduction}

\section{Outline}
The first chapter contains an short introduction to the area and some terminology. The background chapter continues with the terminology and areas related to the project. It starts with some statistics and algorithms and then talks about computer architecture and software design. The algorithm section explains the structure of the current algorithms and then the structure and design of the program that was developed for this thesis, CuEira. After all the background and design are results comparing the performances of the programs and then a discussions of the results.

%Appendix has list of variables and acronyms TODO

\section{Genome-wide association studies}
One type of study to find associations between genetic markers and diseases or other traits is genome-wide association studies(GWAS). Most GWAS do not study interactions between the genetic markers or with environmental factors \cite{cordell_detect_review, gene_enviroment_2013}. Investigating gene-gene interactions recently has become more common\cite{cordell_detect_review}, however gene-environment interactions are still uncommon in studies\cite{gene_enviroment_2013}. Interactions between genes and environmental factors are considered to be important for complex diseases such as cancer and autoimmune diseases\cite{cordell_detect_review, gene_enviroment_2013, geira, ra_smoking}. A complex disease develops due to a combination of factors, not a single gene or environmental factor\cite{rothman1998modern}. The environmental factors can be various things such as smoking, physical activity and so on.
\\
GWAS usually has a study design that is either cohort or case-control\cite{rothman1998modern}. In cohort study a sample of a population is followed and over time some individuals will develop the disease of interest\cite{mann_observational}. In case-control studies two groups are compared with each other to find risk factors\cite{mann_observational}. One group consists of individuals with the disease and the other of individuals that are similar to the cases but who do not have the disease\cite{mann_observational}.\\
\\
\newpage
A typical study consists of tens of thousands of individuals and hundred thousands up to millions of SNPs\cite{cordell_detect_review, burton2007genome}. Due to the high number of SNPs few algorithms are capable to investigate more than second order interaction in a reasonable time. There are some algorithms that can handle higher interaction orders however these have drawbacks\cite{gwis,high_order_2012,fast_high_order_cluster}.

\section{Genetics}
The genetic information is stored in \emph{chromosomes} each consisting of a \emph{DNA molecule}\cite{sadava_life}. Each chromosome comes in a pair where both chromosomes are nearly identical, except for the chromosomes related to sex\cite{sadava_life}. Females have two X chromosomes while males have one X and one Y\cite{sadava_life}. The DNA molecule is a double stranded helix of four nucleotide bases, \emph{adenine}(A),\emph{cytosine}(C),\emph{guanine}(G) and \emph{thymine}(T). They are always paired as A-T and G-C. The DNA contains many different regions, one type of such regions are \emph{genes}\cite{sadava_life}. Genes can \emph{expressed} meaning that they affect the \emph{phenotype}, the pair of genes from both chromosomes is the \emph{genotype}. The phenotype is the result of the genotype, e.g. eye colour, fur patters.\\
\\
The genetic markers mentioned above are commonly single nucleotide polymorphism(SNP, pronounced snip). SNPs are variations in the genome where a single nucleotide is different between individuals in a population\cite{sadava_life}.\\
\\
A position in the DNA is a \emph{locus}\cite{sadava_life}. A variation of the same gene or locus is an \emph{allele}\cite{sadava_life}. The effect of an gene can be either \emph{dominant}, \emph{recessive} or \emph{co-dominant}. Dominant means that the effect occurs if the gene only needs to be present in one of the chromosomes in the pair, recessive is when the allele has to be present in both chromosomes\cite{sadava_life}. Co-dominant is when both genes are expressed, when different alles are present this usually produce some kind of intermediate state\cite{sadava_life}.

\begin{table}[h]
\begin{tabular}{| l l l |}
  \hline
  Genotype & Dominant & Recessive\\
  \hline
  AA & Effect & Effect \\
  GA & Effect & No Effect \\
  GG & No effect & No effect \\
  \hline  
\end{tabular}
\caption{The genetic risk based on the genetic model and if the  allele with the effect is A}
\label{table:genetic}
\end{table}

\newpage
\section{Defining Interaction}
\label{interaction}
There are several ways to define interaction. The overall goal is often to detect if \emph{biological} interactions are present. \emph{Biological} interaction is when the factors co-operate through a physiological or biological mechanism and causes the effect, e.g. the disease. This information can be used to explain the mechanisms involved in causing the disease and possibly help to find cures for them. However biological interaction is not well defined and thus it is not possible to calculate it directly from data.\cite{rothman1998modern,rothman2002intro_epidemiology}\\
\\
\emph{Statistical} interaction on the other hand is well defined. However it is scale dependent, i.e. interactions can appear and disappear based on transformations of the data. Statistical interaction also depends on the model used. The common way to define statistical interaction is to consider the presence of a product term between the variables in the statistical model, this is referred to as \emph{multiplicative} interaction. For instance for a linear model
\begin{equation}
f(x,y)=ax+by+cxy+d
\end{equation}
$c$ is the product term that represents multiplicative interaction between variables x and y. Statistical interaction is often referred to as interaction which can make it a bit confusing.\cite{geira,rothman1998modern}\\
\\
\emph{Additive} interaction is a bit broader than multiplicative interaction. It is that the effect from the interaction is larger than the sum of the separate effects. It implies biological interaction as defined by Rothman\cite{rothman1998modern}, which is sometimes called \emph{causal interdependence} or \emph{causal interaction}. A more precise definition can be found in section \ref{additive}

\section{GEIRA, JEIRA and GEISA, the Aim of the Project}
\label{jeira}
\emph{GEIRA} is a tool for analysing gene-environment interaction. It uses logistic regression and additive(causal) interaction\cite{geira}. \emph{JEIRA} is a parallelised implementation of GEIRA in Java\cite{uvehag_master_thesis}. \emph{GEISA} is based on JEIRA with some changes\cite{geisa}. These three programs were the basis of this project. The aim of this project was to make the gene-environment interaction analysis faster by using GPUs and to be able to handle larger amounts of data. The program is written in C++ because it is generally faster than Java and CUDA is C/C++. Originally one goal was to implement it using a cluster, but it was cut due to lack of time.

%TODO graph or such of development?

\clearpage
\chapter{Background}
%TODO
To search for interaction contains elements of several subjects. This section will go through some of it starting with the algorithms and statistical background. And continuing with computer architecture and software design after that.

\section{Major, Minor and Risk Allele}
The the allele that is least common using all individuals(both cases and controls) is the \emph{minor allele}, the one that is most common is the \emph{major allele}. The frequency of the minor allele is the \emph{minor allele frequency}(MAF)\cite{uvehag_master_thesis, geira}. If the MAF is too low an analysis often will be of little value so it is commonly skipped if it is below a threshold. 5\% is common value of the threshold\cite{burton2007genome, geira}.\\
\\
To determine if the genetic risk is present for each individual one of the alleles needs to be chosen as the \emph{risk allele}. GEIRA and JEIRA determines the risk allele by comparing the MAFs of case and control\cite{geisa, uvehag_master_thesis}. If MAF of cases is greater or equal to the MAF of controls the minor allele is used as the risk otherwise the other allele\cite{geisa, uvehag_master_thesis}.\\
\\
%However the source code for GEIRA and JEIRA does not calculate the risk allele according to that definition. The current version of GEISA(0.13) has no changes to the part of the code that determines the risk allele compared to JEIRA. The code from the programs can be found in appendix \ref{geira_jeira_risk_allele}. What the code does in both cases is to compare the frequency of the most common allele in both groups and if the frequency 
%\\
A simpler equivalent definition is that the risk allele is the allele which frequency is higher in case than in controls. This definition means that the allele chosen as risk might not be the most common in either case or control, however the portion of the population that has the allele is higher among cases than controls. This hints that it might be a reason behind the outcomes. This definition is equivalent to the definition using MAF because if the minor allele has a higher frequency in cases then it the risk is the minor allele according to both definitions. If that is not the case the first definition sets the major allele as the risk. Also due to symmetrical reasons(the total frequency is always 1) this means that the major allele has higher frequency in cases than in controls, which according to the second definition means it is the risk allele.\\
\\
The presence of genetic risk based on the risk allele and the genetic model can be coded as a variable than then be can used in a statistical model. For dominant or recessive genetic model this means that the variable is binary since the risk is either present or not. See table \ref{table:genetic} for how the coding is based on the model. There is an alternative way to code using co-dominant genetic model. In this case the number of risk alleles in the individual used as the variable, so either 0, 1 or 2.

\section{Algorithmic Background}
This section will introduce the statistics and some of the algorithms used to search for interaction. The focus is on logistic regression, however some other methods are briefly examined also.\\
\\
There are many algorithms and programs proposed for searching for interaction, most have focused on gene-gene interaction as already mentioned\cite{gene_enviroment_2013}. One of the challenges of gene-environment interaction is that environmental factors can be of any variable type(i.e. binary, continuous, categorical) which creates problems in various ways\cite{gene_enviroment_2013}. Gene-gene interaction tools can sometimes be used to find gene-environment interaction, however they usually require the variables to be binary or have other problems since they weren't designed for environmental interaction\cite{gene_enviroment_2013}.\\
\\
Both groups of computers(i.e. clusters) using regular processors\cite{biforce} and graphic processors\cite{gwis,gboost,gmdr_gpu,cuda_lr,genie_2012,plink_gpu} have been used for GWAS. Graphic processors for computing have become more popular in the last ten years. They have been a popular choice for some GWAS methods because each combination of variables can commonly be considered independently from the others. More about graphic processors in section \ref{gpu} and why the are good for GWAS in section \ref{gpu_gwas}.\\
\\
The methods can be roughly classified into four categories, exhaustive, stochastic, machine learning/data mining and stepwise\cite{fast_high_order_cluster}.\\
\\
\emph{Exhaustive search} is the most direct approach, it compares all combinations of the SNPs in the dataset. Exhaustive search methods will not miss a significant combination because it didn't consider that specific combination. However it also means that they can be slow since they will spend time on combinations other methods would skip completely. Multifactor-Dimensionality Reduction(MDR)\cite{mdr_2001} and BOOST\cite{boost_gene_gene} are two examples of this type of algorithm.\\
\\
\emph{Stochastic} methods uses random sampling to iterate through the data. BEAM\cite{beam_2007} is one example and it uses Markov Chain Monte Carlo(MCMC) method.\\
\\
\emph{Data Mining} and \emph{Machine Learning} are methods that try to learn patterns from data and tries to generalize it. MDR\cite{mdr_2001} is a type of data mining method and is among the most common methods used in GWAS. See section \ref{data_machine_learning} for more details.\\
\\
\emph{Stepwise} methods uses a filtering stage and a search stage. At the filtering stage uninteresting combinations are filtered out by using some exhaustive method. The other SNPs are the examined more carefully in the search stage. BOOST\cite{boost_gene_gene} is an example which uses succinct data structures and a likelihood ratio test to filter the data before applying log-linear models.

\subsection{Confounders and Covariates}
\emph{Confounding} is one of the central issues in design of epidemiological studies. It is when the effect of the exposure is mixed with the effect of another variable. So if we do not measure the second variable, the effect of the first would be estimated as stronger than it really is. The second variable is then a \emph{confounder}. Several methods in epidemiology are about avoiding or adjusting for confounding. Sometimes these variables needs to be incorporated into the models. \emph{Covariates} are possible confounders or other variables that one wants to adjust for in the model. Sometimes covariates are called \emph{control variables}.\cite{rothman2002intro_epidemiology,rothman1998modern}

\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{Simple_Confounding_Case.png}
    \caption{Illustration of a simple case of confounding. If we do not observe Z we might falsely find an association between X and Y}
    \label{fig:confunding}
\end{figure}

For instance if a study would look into the effects of yellow teeth on lung cancer they would likely find an effect. However it would be due to the confounder smoking since smoking causes yellow teeth and lung cancer. %TODO cite

\subsection{The Multiple Testing Problem}
It is not uncommon to test many hypothesis on the same data and in the case of GWAS it can be billions of tests. If one continues to test one should eventually find something that is significant due to random chance. With the common significance threshold of 5\% it is expected to get 1 in 20 false positives under the assumption that the null hypothesis is true. The problem arises from the fact that the hypothesis tests are dependent on each other since they use parts of the same data. This is the multiple testing problem and if it is not corrected for many false positives might be found.\cite{bonferroni_multiple}\\
\\
Bonferroni correction is the simplest method and viewed as a conservative way to correct for this problem. It simply divides the significance threshold by the number of hypothesis tested. However the number of hypothesis made is not always clear. With a two-stage analysis, is the number of hypothesis the number of tests done in both stages combined, the number made in the first stage or the second stage?\cite{bonferroni_multiple}

\newpage
\subsection{Contingency Tables}
A contingency table is a matrix used to describe categorical data. Each cell contains a count of occurrences for a specific combination of variables. Table \ref{table:contingency_table} is an example of an 2 x 2 table. From this table we can for instance see that 688 smokers got lung cancer. Contingency tables are the basis for various statistical tests to model the data.\cite{agresti_categorical}

\begin{table}[h]
\begin{tabular}{| l c c |}
  \hline
  & Lung cancer & No lung cancer\\
  \hline
  Smoker & 688 & 650 \\
  Non smoker & 21 & 59 \\
  \hline  
\end{tabular}
\caption{Contingency table describing the outcome of a study, from \cite{agresti_categorical}, page 42}
\label{table:contingency_table}
\end{table}

\subsubsection{Relative Risk and Odds Ratio}
\label{rr_or}
From a contingency table it is possible to calculate some useful measures such as the risk of getting the outcome based on exposure. The risk for a row $i$ in the table will be referred to as $\pi_i$. For table \ref{table:contingency_table} the risks $\pi_1$ and $\pi_2$ is

\begin{equation}
\pi_1=\frac{688}{688+650}=0.51
\end{equation}

\begin{equation}
\pi_2=\frac{21}{21+59}=0.36
\end{equation}

To compare different risks, for instance between smokers and non smokers, the ratio of the risks is used\cite{agresti_categorical}. It is called \emph{relative} risk(RR) is defined as\cite{agresti_categorical} 

\begin{equation}
RR=\frac{\pi_1}{\pi_2}
\end{equation}

So for the table \ref{table:contingency_table} the relative risk of getting lung cancer based on exposure to smoking is

\begin{equation}
RR=\frac{0.52}{0.36}=1.96
\end{equation}

This means that the risk of getting lung cancer for a smoker is almost twice as high for a non smoker in this data.

Another useful measure is \emph{odds} and \emph{odds ratio}(OR). The odds is\cite{agresti_categorical} 

\begin{equation}\label{eq:odds}
\Omega=\frac{\pi}{1-\pi}
\end{equation}

The odds are non-negative and $\Omega>1$ when the outcome is more likely than not\cite{agresti_categorical}. So for $\pi=0.75$ the odds is $\Omega=\frac{0.75}{1-0.75}=3$. This means that the outcome is 3 times more likely to occur than not. ORs can be used as an approximation of RR in case control studies because RR can not be estimated in that type of studies\cite{or_mislead}. Cohort studies on the other hand can give estimates of RR\cite{or_mislead}. OR is the ratio of the odds just as RR is the ratio of the risks\cite{agresti_categorical}.

\begin{equation}\label{eq:odds_ratio}
\theta=\frac{\Omega_1}{\Omega_2}
\end{equation}

In the case when the outcome is a disease or similar variables with odds ratio below one are called \emph{protective} and when it is above one it is a \emph{risk factor}\cite{recoding_2011}.

\subsection{Logistic Regression}
One way to model the contingency tables is by using \emph{logistic regression}. Logistic regression is a type of linear regression model for classification that models a latent probability for the outcomes. The outcomes are binary, however the method can be extended to multiple outcomes. In this work we will only consider them as binary. Logistic regression transforms the probability by using the \emph{logit} transformation. The logit transformation with probability $\pi$ is \cite{agresti_categorical}

\begin{equation}\label{eq:logit}
\log(\frac{\pi}{1-\pi})
\end{equation}

The probability with a set of predictor variables \emph{X} is $\pi(X)=P(Y=1)$. The linear regression model with n predictors $X=(x_1,x_2,....,x_n)$, coefficients $\beta=(\beta_1, \beta_2,....,\beta_n)$ and by using the logit transformation is then\cite{agresti_categorical}
\begin{equation}\label{eq:logit_lr}
logit[\pi(X)]=\alpha+\beta X
\end{equation}

By moving the logit to the right side of the equation we get the model of the probability\cite{agresti_categorical}
\begin{equation}
\pi(X)=\frac{e^{\alpha+\beta X}}{1+e^{\alpha+\beta X}}
\end{equation}

The logit, equation \ref{eq:logit}, also happens to be the log of the odds(equation \ref{eq:odds})\cite{agresti_categorical}. By exponentiating both sides of equation \ref{eq:logit_lr} it shows that the odds is \cite{agresti_categorical}

\begin{equation}
e^{logit[\pi(X)]}=\frac{\pi(X)}{1-\pi(X)}=e^{\alpha+\beta X}=\Omega
\end{equation}

This means that $\exp{\beta}$ is the odds ratio since the odds increase by $\exp{\beta}$ for each unit increase of $X$\cite{agresti_categorical}. It can also been seen by taking the ratio of the odds using equation \ref{eq:odds_ratio} and  with $X=x+1$ and $X=x$

\begin{equation}
\theta=\frac{e^{\alpha+\beta (x+1)}}{e^{\alpha+\beta x}}=e^{\alpha+\beta (x+1)-\alpha-\beta x}=e^{\beta}
\end{equation}

Finding the $\beta$ coefficients are done in a similar way as with other linear regression models since they all are generalized linear models \cite{agresti_categorical}. It's usually done using  maximum likelihood(ML), via Newtons method\cite{agresti_categorical, uvehag_master_thesis}. It's an iterative method which means it can be relatively slow compared to non iterative methods. The pseudo code for the algorithm using Newtons method can be found in algorithm \ref{alg:lr} \cite{uvehag_master_thesis}. * is elemet by element multiplication. $X$ is an $NxM$ matrix that contains the variables, $Y$ is the outcomes with length $N$.

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{logit.png}
    \caption{Graph of the logit transformation. Wikipedia Commons}
    \label{fig:logit}
\end{figure}

\stepcounter{algorithmCode}
\begin{algorithm}
\begin{algorithmic}
\State $\boldsymbol{X}\gets
\begin{pmatrix}
\begin{matrix}
  1\\
  \vdots\\
  1
\end{matrix} & \boldsymbol{X}
\end{pmatrix}
$

\State $\beta\gets
\begin{pmatrix}
  0\\
  \beta
\end{pmatrix}
$

\State $iter\gets 0$
\State $diff\gets 1$
\\
\While{$iter < max\_iter \; \boldsymbol{and} \; diff > threshold$}
\State $\beta_{old}\gets\beta$
\State $p\gets \frac{e^{\boldsymbol{X} \cdot \beta}}{1+e^{\boldsymbol{X} \cdot \beta}}$
\State $s\gets \boldsymbol{X^T}\cdot (\boldsymbol{Y}-p)$
\State $\boldsymbol{J}\gets (\boldsymbol{X^T}\cdot (p*(1-p)))\cdot \boldsymbol{X}$
\State $\beta\gets \beta_{old}+\boldsymbol{J}^{-1} \cdot s$
\State $diff\gets \sum |\beta-\beta_{old}|$
\State $iter\gets iter+1$
\EndWhile
\State $log \; likelihood\gets \sum (\boldsymbol{Y}*ln p+(1-\boldsymbol{Y})*ln(1-p))$
\end{algorithmic}
  \caption{Logistic regression using maximum likelihood and Newtons method}
  \label{alg:lr}
\end{algorithm}

\newpage
$ $
\newpage
\subsubsection{Mathematical Definition of Interaction for LR}
\label{additive}
Interaction was talked about in section \ref{interaction}. The more precise definition of additive interaction is, the divergence from additive effects on a logarithmic scale, e.g\cite{rothman1998modern}.

\begin{equation}
OR_{both\:factors\:present}>OR_{first\:factor\:present}+OR_{second\:factor\:present}-1
\end{equation}

To get these specific ORs the variables are coded in the model according to table \ref{table:coding}\cite{uvehag_master_thesis}.

\begin{table}[h]
\begin{tabular}{| l | c c c|}
  \hline
  Factor present & First variable & Second variable & Interaction \\
  \hline
  None & 0 & 0 & 0 \\
  \hline 
  First & 1 & 0 & 0 \\
  \hline
  Second & 0 & 1 & 0 \\
  \hline
  Both & 0 & 0 & 1 \\
  \hline
\end{tabular}
\caption{Coding of the variables for LR}
\label{table:coding}
\end{table}

%math stuff for add

%However there is a problem when both variables are non binary. Because LR models how the risk increases for each increase in the variable the interaction variable should preferably be in increased order of risk. However without doing extensive testing for the possible combinations it is not possible to know the order. It is easy in the binary case because the interaction only has two levels, both present or not. For an example take two variables with 3 levels, 0,1,2. For a single variable it is easy to assume that 2 is worse than 1. The problem with interaction occurs here because it is hard to know if 1,2 is worse than 2,1. In the case of continues variables it gets worse because scale starts to matter more.

%However in gene-environment interaction the SNP variable is binary unless co-dominant model is used. This means that the coding isn't a problem for non binary environmental variables.


%Extending to non binary.

%OR>1 because recoding, see section \ref{recode}

%\begin{equation}
%OR_{both\:factors\:present}^x>OR_{first\:factor\:present}+OR_{second\:factor\:present}^x-1
%\end{equation}
%Where x is the number of steps from the base level.

%This means that the interaction depends on the level of the factor. There are four cases

%Using $\beta_{snp}$
%$\beta_{environment}$
%$\beta_{interaction}$

%\begin{table}[h]
%\begin{tabular}{| l | l | l |}
%  \hline
%  Case & Additive Interaction present, binary factor & Additive interaction present, non binary factor\\
%  \hline
%  $\beta_{interaction}>\beta_{snp} \; \beta_{interaction}>\beta_{environment}$ & Yes & Yes\\
%  $\beta_{interaction}<\beta_{snp} \; \beta_{interaction}<\beta_{environment}$ & No & No\\
%  $\beta_{interaction}>\beta_{snp} \; \beta_{interaction}<\beta_{environment}$ & No & For some x\\
%  $\beta_{interaction}<\beta_{snp} \; \beta_{interaction}>\beta_{environment}$ & No & For some x\\
%  \hline  
%\end{tabular}
%\caption{Additive interaction cases for non binary environmental factor}
%\label{table:additive_interaction}
%\end{table}

%Can also be written as

%\begin{equation}
%OR_{both\:factors\:present}^x-OR_{second\:factor\:present}^x>OR_{first\:factor\:present}-1
%\end{equation}

%In case 3 it is monotone increasing

%In case 4 it is monotone decreasing

%TODO make some plots of it

%Which means both pass 0 at one point.(Unless all equal 1 which means all 0)

%That point can be found by solving

%\begin{equation}
%OR_{both\:factors\:present}^x-OR_{second\:factor\:present}^x=+OR_{first\:factor\:present}-1
%\end{equation}

%Case 3 has interaction for x>the point
%Case 4 has interaction for x<the point

%Depending on where the point is interaction might not be relevant, e.g. needs to smoke a million cigarettes a day and case 3. Or disappears to fast, eg 1 cigarette a year case 4.

\subsubsection{Statistic Measures for interaction}
Based on the model and its corresponding ORs there are some measures that can be calculated. These measures show various properties of the additive interaction. They are defined using $RR$ however as mentioned before in section \ref{rr_or} $OR$ can be used to approximate $RR$.\\
\\
\emph{Relative  excess risk due to interaction}(RERI) is how much of the risk is due to interaction\cite{recoding_2011}. It is defined as\cite{recoding_2011}
\begin{equation}
RERI=RR_{11}-RR_{10}-RR_{01}+1
\end{equation}

\emph{Attributable proportion due to interaction}(AP) is similar to RERI however is the proportion relative to the interaction relative risk\cite{recoding_2011}.
\begin{equation}
AP=\frac{RERI}{RR_{11}}=\frac{1}{RR_{11}}-\frac{RR_{10}}{RR_{11}}-\frac{RR_{01}}{RR_{11}}+1
\end{equation}

\emph{Synergy index}(SI) is the ratio of the combined effects and the individual effects\cite{recoding_2011}.
\begin{equation}
\frac{RR_{11}-1}{RR_{10}+RR_{01}-2}
\end{equation}

By using the definition of additive interaction it can be shown that additive interaction is present when $RERI>0$, $AP>0$ and $SI>0$\cite{recoding_2011}.

\newpage
\subsubsection{Recoding of Protective Effects}
\label{recode}
%TODO
The measures for additive interaction RERI, AP and SI mentioned above are developed for risk factors, i.e. $OR>1$, which causes problems if a factor is protective\cite{recoding_2011}. This can be solved by \emph{recoding}, recoding makes sure that no $OR$ is below one\cite{recoding_2011}. Recoding switches the reference group and the group with the lowest risk\cite{recoding_2011}. Denoting the different groups with $(s,e)$ where $s$ is the presence of the risk of the SNP variable and $e$ is the presence of the environmental variable. The reference group is $(0,0)$ and there are three other groups $(1,0)$,$(0,1)$ and $(1,1)$. This means that there are three cases of recoding, one for each group. To recode a group is to transform the variable so that it is changed from protective to risk.

%\cite{recoding_2011} shows it for the binary environmental variable. However 

%Since the odds is an monotone function, switching lowest risk with highest means inverting the interval of the variable, e.g. minimum becomes maximum, maximum becomes minimum.

\subsection{Data Mining and Machine Learning Approaches}
\label{data_machine_learning}
Approaches based on Data Mining and Machine Learning have been a popular choice for GWAS. MDR\cite{mdr_2001} and Random Forest(RF)\cite{random_forest} are among the most common ones\cite{gene_enviroment_2013,cordell_detect_review}. There are other methods as well such as clustering approaches \cite{fast_high_order_cluster}. Most of them are used for screening the data for possible interactions\cite{gene_enviroment_2013,cordell_detect_review}.\\
\\
Their biggest advantage is that they are usually non-parametric and designed with high dimensional data in mind. However they are prone to overfitting and the usual way to try to prevent that is to use cross validation and sometimes permutation tests. It means that even if the method itself is fast it is repeated so many times that the whole algorithm can be slow in the end.\cite{cordell_detect_review}

\subsubsection{Multifactor-Dimensionality Reduction}
MDR is a method that reduces the number of dimensions(i.e. variable) by combining several dimensions into one. In GWAS it combines the variables that are suspected to interact. This new variable is then compared against the outcome. If the new variables predictability of the outcome is high enough then the variables that were combined are considered to interact. This process is usually repeated on all pair combinations of variables.\\
\\
The reduction from $n$ dimensions is done by calculating the ratio of cases versus controls for each combination of the possible values of the variables. If the ratio is above a certain threshold all the members of that groups get the value 1 for the new dimension, otherwise 0. Accuracy of the model is done by using cross validation and permutation tests, in simpler words it means that it reshuffles the data randomly and recalculates the model many times to get an estimate of the models certainty. Because of that MDR can be slow. However it is still usually faster than exhaustive search with regression methods.\cite{cordell_detect_review,mdr_2001}\\
\\
MDR can been used for gene-environment interaction but requires modifications since MDR can only handle binary variables. There are extensions that can use continues variables, however these are regression based so these will be slower than regular MDR.\cite{gene_enviroment_2013}\\
\\
A simple example of MDR using exclusive or (XOR). XOR is an logical operator that is true if one and only one of its two variables is true. We have 4 possible combinations and an occurrence for each of them, see table \ref{table:xor_table}. The combination (1,0) and (0,1) both have one case with outcome 1 so MDR will classify them as 1 in the new variable Z, the other two combinations have outcome 0 so will be classified with Z=0, see table \ref{table:xor_mdr_table}. From here it is easy to make an predictor from Z to the outcome Y by comparing the values.

\begin{table}[h]
\begin{tabular}{ | c | c | c | }
  \hline
  \textbf{Y} & $\mathbf{X_1}$ & $\mathbf{X_2}$ \\
  \hline
  1 & 1 & 0 \\
  \hline 
  1 & 0 & 1 \\
  \hline
  0 & 0 & 0 \\
  \hline
  0 & 1 & 1 \\
  \hline
\end{tabular}
\caption{XOR table with outcome $\mathbf{Y}$ and variables $\mathbf{X_1}$ and $\mathbf{X_2}$.}
\label{table:xor_table}
\end{table}

\begin{table}[h]
\begin{tabular}{ | c | c | }
  \hline
  \textbf{Y} & \textbf{Z} \\
  \hline
  1 & 1 \\
  \hline
  1 & 1 \\
  \hline
  0 & 0 \\
  \hline
  0 & 0 \\
  \hline
\end{tabular}
\caption{XOR table with $\mathbf{X_1}$ and $\mathbf{X_2}$ combined into $\mathbf{Z}$ using MDR.}
\label{table:xor_mdr_table}
\end{table}

\subsubsection{Random Forest}
RF is an ensemble learning method\cite{random_forest}. Ensemble methods combine multiple models to improve performance. RF takes randomized samples of the data and builds decision trees on each of them. These trees are then combined to form the classifier. Usually hundreds or thousands of trees are used depending on the problem\cite{random_forest}. One of the most popular variants of Random Forest for GWAS is Random Jungle\cite{random_jungle}.\\
\\
It has been shown in high dimensional data that RF tends to only rank interacting factors high if they have strong marginal effects\cite{winham_rf_2012}. Also the ranking of the variables does not indicate which factor it is interacting with either since it is based on the joint distributions\cite{gene_enviroment_2013}. How to incorporate the environmental factors in RF is also not obvious and using variables with very different scales can bias RFs results\cite{gene_enviroment_2013}.

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{Decision_Tree.png}
    \caption{A example of a decision tree}
    \label{fig:DecisionTree}
\end{figure}

\clearpage
\section{Computer Architecture}
This section will explain some of the architectures used to perform tasks in a computer. A large part is focused on optimization techniques that are in used today. For the purpose of this thesis it is easiest to think about the computer as three main parts, data storage(e.g. the disk), processor and the memory. Sometimes there is also an accelerator, for instance a graphic processor, which will be explained in section \ref{gpu}.
%Most computers today are based on the \emph{Von Neumann architecture}. It was first used in EDVAC in the late 1940s, it was one of the first stored program %computers\cite{von1993first}. Stored program computers uses electronic memory to store the program instructions\cite{computer_arch_2003}.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=7cm]{Von_Neumann_architecture.png}
%    \caption{Schematic of the Von Neumann architecture. Wikipedia Commons}
%    \label{fig:VonNeumann}
%\end{figure}

\subsection{CPU}
Central processing unit(CPU) is the part of the computer that performs most of the tasks\cite{introduction_hpc_hager}. It executes instructions one by one in its core. Most modern CPUs have multi-core architecture. These CPUs have multiple cores\cite{introduction_hpc_hager}. Each core can perform tasks independent of each other so the programs needs to be parallel to get maximum speed. Parallel means that the program issues multiple instructions at the same time, more about this in section \ref{concurrency}.\\
\\
The figures \ref{fig:cpu_scheme} and \ref{fig:core_die} shows how the CPU is divided into areas and that most of it is not used for calculations. A large part of the area is used for various optimizations. The next section \ref{optimizations} will explain some of these optimizations.

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{cpu_scheme_haswell.jpg}
    \caption{The layout of the Haswell architecture i7-5960x, from \cite{intel_haswell_2014}}
    \label{fig:cpu_scheme}
\end{figure}

\newpage
\subsection{Memory, Caching and Optimizations}
\label{optimizations}
Various optimizations have been introduce to the CPUs over the years\cite{drepper2007cpumemory, introduction_hpc_hager}. Some are common and almost all CPUs have them, others are unique to a specific vendor or CPU\cite{introduction_hpc_hager}. In this section some of the more common optimizations will be explained. As shown in figure \ref{fig:core_die} these optimizations take up the most of the cores area, the square in the upper left corner executes the instructions.

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{cpu_core_die.png}
    \caption{Layout of a CPU core, Intel Nehalem i7, from \cite{tomshardware_nehalem}}
    \label{fig:core_die}
\end{figure}

Retrieving data from memory is relatively slow compared to how fast the processor works\cite{introduction_hpc_hager,drepper2007cpumemory}. %TODO something about the  maginitude
To solve this problem a \emph{cache} was introduced\cite{drepper2007cpumemory}. It stores recently used data in a very small and very fast memory that resides on the CPU chip itself\cite{drepper2007cpumemory}. The caches varies in sizes, the latest ones are around 16-20mb, for instance Intels i7-5960x with 20mb \cite{intel_haswell_2014_5960x}. The cached data can then be reused fast without waiting for the main memory to fetch it. However when the data is not in the cache it takes the usual speed to fetch it from the main memory, that is a \emph{cache miss}\cite{drepper2007cpumemory}. Avoiding cache misses is important for speed because of how much time it can take to fetch the data from the main memory\cite{drepper2007cpumemory}. Modern CPUs commonly have more than one cache, most have three\cite{introduction_hpc_hager}. They are named L1 to L3, with L1 being the one fastest and smallest and L3 the largest and slowest\cite{introduction_hpc_hager}.\\
\\
Modern multicore processors several L1 caches, one for each core\cite{introduction_hpc_hager}. It is commonly one L2 per core too, however it is shared in some architectures\cite{introduction_hpc_hager}. L3 is almost always shared between all cores\cite{introduction_hpc_hager}. That multiple caches are used creates a problem called \emph{cache coherency}. It is that when the data is modified in one cache all the cores caches needs to be updated too so that they use the new value\cite{introduction_hpc_hager}.\\
\\
Another memory optimization somewhat related to caches is \emph{prefetching}\cite{introduction_hpc_hager, drepper2007cpumemory}. Instead of fetching just the data requested by the current operations it also fetches the surrounding data\cite{drepper2007cpumemory}. The chance that the surrounding data will be used soon is high since it is common to iterate over arrays and similar structures\cite{introduction_hpc_hager}. If a program iterates over an array unless it was used recently the first value will be a cache miss since it is not in the cache. However the following values will be due to them being prefetched and cached. At some point however the array is too long to be prefetched and cached completely and when the CPU hits that spot a new cache miss will happen\cite{drepper2007cpumemory}. Because everything is stored sequentially it is important in some cases to know how the it is stored so that the prefetching and caching can be used. One of these cases is for matrices, they are 2D but stored as one array. Different programming languages store them differently, either column by column or row by row. Row by row is called \emph{row major} and column by column is \emph{column major.}\\ %TODO cite row major and such
\\
In section 6.2.1 in What Every Programmer Should Know About Memory\cite{drepper2007cpumemory} an example of how much speed that can be gained by optimizing matrix matrix multiplication. The first version is shown in algorithm \ref{alg:matmatmult}.

\begin{algorithmCode}{Matrix matrix multiplication} {alg:matmatmult}
for (i = 0; i < N; ++i)
  for (j = 0; j< N; ++j)
    for (k = 0;k < N; ++k)
      res[i][j]+= mul1[i][k] * mul2[k][j];
\end{algorithmCode}

By transposing the mul2 matrix it becomes more cache friendly because it uses the prefectching of surrounding data the way that mul1 already is\cite{drepper2007cpumemory}. The transpose itself can be eliminated while also making it use the cache better by reading in the correct amount of data, this is the size SM in the code\cite{drepper2007cpumemory}. The optimized version is shown in \ref{alg:matmatmultopt}. It took 17.3\% of the original time\cite{drepper2007cpumemory}.

\begin{algorithmCode}{Matrix matrix multiplication optimized} {alg:matmatmultopt}
#define SM (CLS / sizeof (double))

for (i = 0; i < N; i += SM)
  for (j = 0; j < N; j += SM)
    for (k = 0; k < N; k += SM)
      for (i2 = 0, rres = &res[i][j], rmul1 = &mul1[i][k]; i2 < SM; ++i2, rres += N, rmul1 += N)
	for (k2 = 0, rmul2 = &mul2[k][j]; k2 < SM; ++k2, rmul2 += N)
	  for (j2 = 0; j2 < SM; ++j2)
	    rres[j2] += rmul1[k2] * rmul2[j2];
\end{algorithmCode}

On top of that by using some additional features of the the CPU do multiple instructions at once further speed is gained increasing it to 9.47\% of the original time\cite{drepper2007cpumemory}. \\
\\
If the operation instructions diverge, e.g. if statements, the CPU would have to wait for the results to see what instructions load and do next\cite{drepper2007cpumemory}. This can mean a significant loss of time, however thanks to a optimization called \emph{branch prediction} and \emph{speculative execution} that can be prevented in some cases\cite{drepper2007cpumemory}. A dedicated part of the CPU stores the results from these diverges and when they are repeated it uses them to make an guess of what do to next\cite{drepper2007cpumemory}. It then loads the instructions and tries to fetch the needed data\cite{drepper2007cpumemory}. It then speculatively executes the instructions, i.e it executes instructions that might not actually be needed\cite{drepper2007cpumemory}. If the guess was correct the results are kept, on the other hand if it was wrong the CPU starts again from the correct path and discards the incorrect results\cite{drepper2007cpumemory}.\\
\\
Sometimes there can be several independent instructions, e.g. when adding two vectors together, this allows optimizations for \emph{instruction level parallelism}\cite{introduction_hpc_hager}. The order of the instructions can be ignored which for instance means that if an instruction needs to wait for some data another instruction that has its data available could be executed while waiting\cite{introduction_hpc_hager}. It is an optimization called \emph{out of order execution}\cite{introduction_hpc_hager}. There are also other possible optimizations\cite{introduction_hpc_hager}.
\\
In the example \ref{ex:independent_instructions} the instructions are independent because they use different variables so they can be executed in any order. If the variables $b$ and $c$ needs to be fetched but $e$ and $f$ are available due to previous instructions then the CPU could execute the second addition first by using out of order execution. However if an operation later depends on the variable $a$ or $d$ then that instruction would have to wait until the additions have been executed.

\begin{example}{Independent Instructions}{ex:independent_instructions}
a = b + c;
d = e + f;
\end{example}

\emph{Pointer aliasing} can prevent the compiler from making certain optimizations, e.g. out of order execution mentioned above. Pointer aliasing is when the same memory area can be accessed using different variables\cite{introduction_hpc_hager}. This becomes a problem if an set of instructions use these variables in the same area of the program. The relative order of operations between these operations then has to be preserved. The compiler does have the information if aliasing occurs or not which means the compiler has to treat all situations where it can occur as if it does\cite{introduction_hpc_hager}.\\
\\
If the integers in example \ref{ex:independent_instructions} are changed to pointers then aliasing can occur because some of these pointers could point to the same memory. For instance if a and e points to the same memory then the result would be wrong if the second line is executed before the first. This would force the instructions to be executed in order or the result would likely be incorrect.

\begin{example}{Pointer Aliasing}{ex:aliasing}
//All variables are pointers
*a = *b + *c;
*d = *e + *f;
\end{example}

Some programming languages, e.g. Fortran, disallow some types of aliasing while others, e.g. C/C++, allows aliasing\cite{introduction_hpc_hager}. The compilers for the languages in the second case commonly have an option to disable aliasing. However for languages that don't allow aliasing, or if it is disabled, it is then the programmers responsibility to make sure that aliasing does not occur or the results can be wrong\cite{introduction_hpc_hager}.

\newpage
\subsection{Concurrency and Threads}
\label{concurrency}
\emph{Concurrency} means doing multiple things and the same time, instead of \emph{sequential}. This can cause various problems such as the same object being accessed at the same time. The dinning philosophers is an concurrency problem that can used to illustrate some of these problems. A group of philosophers is sitting at a round table, each has a plate of spaghetti in front of them and there is a fork between each pair of philosophers\cite{hoare1985communicating}. The philosophers alternate between thinking and eating. However they can only eat if they have both the right and left fork\cite{hoare1985communicating}.

\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{dining_philosophers.png}
    \caption{Illustration of the dining philosophers problem. Wikipedia Commons}
    \label{fig:dining_philosophers}
\end{figure}

A potential proposal\cite{hoare1985communicating} for behaviour instructions could be:
\begin{itemize}
 \item Think
 \item Wait for a fork to become available and pick up that fork
 \item Wait for and pick up the other fork
 \item Eat
 \item Put down the forks one by one
 \item Go back to thinking
\end{itemize}
The problem is that this set of instructions can lead to a state where everyone is holding one fork and waiting to get the other one\cite{hoare1985communicating}. But no one is done eating so no forks will become available. The system is then locked into its state, this is called a \emph{deadlock}\cite{hoare1985communicating, introduction_hpc_hager}. A potential solution to the problem in this case is to introduce a person that dictates if a philosopher is allowed to eat or not\cite{hoare1985communicating}.\\
\\
There are also other potential problems that can arise when using concurrency. A \emph{race condition} occurs when the result depends on what affects it first, it is a race between them on who can reach it first\cite{introduction_hpc_hager}. \emph{Locks} can be used to prevent situations as the ones described above by making sure only one thread at a time can access the object\cite{introduction_hpc_hager}.

\newpage
In computer science concurrency occurs when instructions are separated in different \emph{threads}. The architectures can be divided in a taxonomy with four categories including the sequential. They are illustrated in figure \ref{fig:flynn_taxonomy}.

\begin{itemize}
 \item \emph{SISD}(Single-Instruction,Single-Data) is the sequential and is as the name says, single instructions on single data\cite{introduction_hpc_hager}.
 \item \emph{SIMD}(Single-Instruction, Multiple-Data) is applying the same instruction on different data\cite{introduction_hpc_hager}. SIMD is also called vectorisation\cite{introduction_hpc_hager}.
 \item \emph{MIMD}(Multiple-Instruction, Multiple-Data) applies different instructions on different data\cite{introduction_hpc_hager}.
 \item \emph{MISD}(Multiple-Instruction, Single-Data) performs different operations at the same piece of data, this paradigm is uncommon\cite{introduction_hpc_hager, computer_arch_2003}.
\end{itemize}

\begin{figure}
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{SISD}
                \caption{SISD}
                \label{fig:SISD}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{MISD}
                \caption{MISD}
                \label{fig:MISD}
        \end{subfigure}

        \begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{SIMD}
                \caption{SIMD}
                \label{fig:SIMD}
        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{MIMD}
                \caption{MIMD}
                \label{fig:MIMD}
        \end{subfigure}
        \caption{Flynns taxonomy of parallel architectures\cite{introduction_hpc_hager}}\label{fig:flynn_taxonomy}
\end{figure}

\newpage
\subsection{Accelerators, GPUs and Xeon Phi}
\label{gpu}
Accelerators are separate parts made to do one task and do it well. They are usually highly specialised and perform badly outside of the tasks they were designed for. The two accelerators explained here is graphical processing unit(GPU) and Intel Xeon Phi\cite{cuda, jeffers2013intel}. GPUs have mostly been used for games and other graphics related problems however have become more popular for general computing the last 10 years due to their high number of cores\cite{cuda}.  Intel Mic is much more recent with prototypes in 2010 and the first generation released in 2010\cite{jeffers2013intel}. It is Intels response to the GPUs and has elements of both GPU and CPU\cite{jeffers2013intel}. This section will focus on explaining some parts of the architecture of the GPUs and Xeon Phi. How to use the GPUs are talked about in more detail in section \ref{cuda_programming_model}.

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{floating-point-operations-per-second.png}
    \caption{Theoretical throughput of some NVIDIA GPUs and Intel Processors, from\cite{cuda}. However the chart does not contain a Xeon Phi processor.}
    \label{fig:gpu_vs_cpu}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=11cm]{gpu_scheme.png}
    \caption{Layout of the Maxwell architecture, from \cite{nvidia_maxwell}}
    \label{fig:gpu_scheme}
\end{figure}

GPUs are made to process images in a fast manner so that games and similar tasks runs smoothly\cite{cuda}. Because GPUs are made for image processing they are GPUs have a large number of cores(~1000-2000)\cite{cuda, nvtesla}. Each core is slower than a core in a CPU, however the number of cores makes up for it. The downside is that it's slower than a normal CPU for sequential tasks so the program has to take advantage of the extreme parallelism to get good speed\cite{cuda_best_practice}. The memory of the GPU is separate from the computers main memory which means that the data needs to be transferred between the memories which increases the programs complexity\cite{cuda}.\\
\\
The GPU cores work in a manner that is similar to SIMD\cite{cuda}. The cores also have fewer optimizations than CPUs, this means that they can then devote a larger portion of the chip to the calculations as seen in figure \ref{fig:gpu_scheme}. For instance NVIDIAs GPUs lack optimizations such as branch prediction and speculative execution talked about in section \ref{optimizations} \cite{cuda}. Their caches are also much smaller than the CPUs\cite{cuda}. Another inheritance from the image processing is that they are much faster with single precision than with double precision datatypes\cite{cuda, nvtesla}. A CPU drops slightly in performance when using double precision but much less than a GPU as seen in figure \ref{fig:gpu_vs_cpu}. Single and double precision refers to how many bytes are used to store numbers, fewer bytes means less numerical precision.

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{knigths_corner_silicon_layout.png}
    \caption{Knights Corner silicon layout, from\cite{intel_knights_corner}}
    \label{fig:knights_corner_layout}
\end{figure}

Intels answer to the increasing popularity of GPUs for general computing is Xeon Phi\cite{jeffers2013intel}. It has elements of GPU architecture however is still more like a CPU. It has separate memory and many cores(50+)\cite{jeffers2013intel}. It is MIMD which can be good for codes that diverge a lot, for ease of use however it has the possibly to use more SIMD like structures\cite{jeffers2013intel}. It is possible to only use the Xeon Phis memory and avoid the memory transfers that way. The memory is relatively small compared to the main memory so if the program needs a lot of memory transfers are likely needed. The cores of the Xeon Phi lacks some of the normal optimizations, e.g. out of order execution\cite{jeffers2013intel}.

\subsubsection{GPU vs Xeon Phi, Why use GPUs for Interaction?}
\label{gpu_gwas}
What is most efficient depends largely on the algorithm and if there is any previous existing code. For instance if the algorithm contains many points of divergence Xeon Phi is likely faster because Xeon Phi is MIMD while GPU is SIMT. Xeon Phi can be applied to a broader set of problems due to using a MIMD architecture\cite{jeffers2013intel}. A one on one comparison of speed might not be entirely relevant because the algorithm used for the test might simply be better suited for one the architectures. Speed per cost, either power consumption or the price on the hardware, can also be more relevant. If one is half the cost of the other then comparing two versus one would be a better comparison. CUDA is also more mature than Xeon Phi since Xeon Phi is only a few years old while CUDA is almost ten\cite{cuda, jeffers2013intel}.\\
\\
So what suits best needs to be looked at using a case by case basis by examining the specifics of the algorithm and what type of computers that might already be available. It is also important to think about how important maximum speed is and how much time there is to spend. If extremely high speed is of importance and time is not a concern then a GPU is likely to get higher performance, unless the algorithm is badly suited for GPUs. However Xeon Phi might reach similar speed but with less effort.\\
\\
%Why GPU for GWAS
GPUs are good for interaction algorithms because most methods consider each combination independently and also in this case that LR consists of linear algebra operations. A new combination can be transferred while another is calculated. Linear algebra with sufficiently large matrices and vectors work very well on GPUs\cite{cublas, cuda}. Several studies got high gains from implementing their algorithms on GPU\cite{gwis,gboost,gmdr_gpu,cuda_lr,genie_2012,plink_gpu}. However most studies had CPU versions that were not parallel so it is likely that the gains would have been less if they would have been compared to an optimized and parallel CPU version, especially if a Xeon Phi co-processor was used. One study did make a comparison of a CPU cluster version and a GPU version of their algorithm, they found that 16 CPU nodes had the same performance as a single GTX 280 card\cite{jiang_accelerating}.

%\subsection{Clusters}
%\label{clusers}

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=10cm]{Cluster.png}
%    \caption{Basic overview of a cluster}
%    \label{fig:cluster}
%\end{figure}

%Clusters are collections of computers that can work together in several ways and are so tightly connected that they can usually be viewed as a single system. They communicate over a shared network but have separate memory and processors. Storage is usually shared. A computer in a cluster is called a \emph{node}. \cite{intro_hpc, introduction_hpc_hager}

%various programming models, shared memory and so on
%shared memory, cache coherrency and stuff

%mpi is commonly used for clusters\cite{introduction_hpc_hager}

%top 500\cite{TOP500}

%Few clusters used GPUs before 2009 but now mix of GPUs and CPUs are common among the top clusters. It was driven largely by demand for power saving while still giving high performance. The GPUs can do the heavy computation while other parts are used on CPU. \cite{introduction_hpc_hager}
%Something about Intel MIC

%Flops is not everything, bad performance compare to theoretical maximum
%Stuck in network, hard drives, etc. Latency
%Very few programs scale to the fastest clusters, peta scale.

%Tianhe 2 is current top. Theoretical performance 54.9 PetaFlops.\cite{TOP500}
%Max achived with LinPack 33 PetaFlops
%60\% efficiency

%\cite{introduction_hpc_hager}

\clearpage
\section{Performance Measures}
An important part in making fast and efficient programs is to know how fast the program is under certain conditions and which parts of the program are slow\cite{introduction_hpc_hager}. For instance the speed could suddenly drop when we start using too many threads, there might be a bottleneck in the network, and so on.\\
\\
There are two ways to measure how long a program takes to execute\cite{introduction_hpc_hager}. Wall clock time is how long real life time the program took. The other is to measure the number of processor cycles spent. A parallel program will have shorter execution time than it is serial version however it will likely have spent more processor cycles due to overhead from communication and initialization of the threads. These two measures are useful for different kinds of comparisons. Wall clock time is better for overall performance while number of cycles is useful for comparing different algorithms\cite{introduction_hpc_hager, cuda_best_practice}.\\
\\
Speed up is a measure of how much faster then program is with a certain number of threads compared to the serial version. It's defined as\cite{introduction_hpc_hager}
$$S(p)=\frac{T(1)}{T(p)}$$
Where $T(1)$ is execution time of serial program and $T(p)$ is execution time of parallel program with p threads. Linear speed up is when S(p)=p\cite{introduction_hpc_hager}.\\
\\
Efficiency reflects how efficient the program is using p threads. Linear speed has efficiency 1. It's defined as\cite{introduction_hpc_hager}
$$E(p)=\frac{S(p)}{p}=\frac{T(1)}{pT(p)}$$
\\
Strong scaling refers to how the program handles a fixed problem size and increased number of processors\cite{introduction_hpc_hager}. An program with strong scaling has linear speed up\cite{introduction_hpc_hager}. Weak scaling refers to the execution time of the program when there is a fixed problem size \emph{per processor} and the number of processors is increased\cite{introduction_hpc_hager, cuda_best_practice}.\\
\\
It can be a good idea to plot these measures while varying p, this can show when a bottleneck occurs. Looking at the measures at node level can also be useful to get an idea of how increased number of nodes and therefore increased communication over the network affects the performance.

\newpage
\subsection{Amdahl's Law and Gustafson's Law}
Amdahl's Law is used to find the expected speed up of a system when parts of it are made concurrent. Simply it says that as the number of processors increases the parts that aren't parallel will start taking up more and more of the wall clock time and that the speed up for adding more processors will decrease as more and more processors are added and more time is spent relatively on the non parallel parts\cite{introduction_hpc_hager}. It's closely related to strong scaling.\cite{cuda_best_practice,2010_reevaluating_amdahl}\\
\\
It says that the expected speed up with F fraction of the code parallel and p threads is\cite{introduction_hpc_hager} 
$$S(p)=\frac{1}{(1-F)+\frac{F}{p}(1-F)}$$
As the number of threads grow towards infinity $S(p)$ converges on $\frac{1}{1-F}$. If we have 90\% of a code parallel then even with infinite number of threads we won't get a better speed up than ten.\cite{2010_reevaluating_amdahl}

\begin{figure}[h]
    \centering
    \includegraphics[width=11cm]{AmdahlsLaw.png}
    \caption{Illustration of Amdahl's Law. Wikipedia Commons}
    \label{fig:AmdahlsLaw}
\end{figure}

There are limitation to Amdahl's Law since it makes a couple of assumptions.

\begin{itemize}
  \item The number of executing threads remain constant over the course of the program.
  \item The parallel portion has perfect speed up. Often not true due to shared resources,eg caches, memory bandwidth, and shared data.
  \item The parallel portion has infinite scaling, not true due to similar limits as above. More threads will not increase performance after a while or might even decrease it.
  \item There is no overhead for creation and destruction of threads.
  \item The length of the serial portion is independent of the number of threads. Often the serial work is to divided the work to the threads, this work will obviously increase as the number of threads go up. More threads can also lead to more communication overhead.
  \item The serial portion can't be overlapped by the parallel parts. For instance with producer consumer type pattern the consumer could be strictly serial but the time it takes could be overlapped by the parallel producers.
\end{itemize}

This means it is most accurate with programs that are of the fork-join type, e.g. both serial and parallel parts\cite{gustafson1988reevaluating}.\\
\\
Gustafson's Law is closely related to Amdahl's Law and can in some ways be more accurate than Amdahl's Law\cite{gustafson1988reevaluating}. Gustafson's Law makes similar assumptions as Amdahl's Law however it also makes two additional statements. It states that problems tends to expand when provided with more computational power, e.g. increased precision by reducing grid size for simulations, higher frame rate for graphics and so on\cite{gustafson1988reevaluating, introduction_hpc_hager}. The second is that the parallel portion of the program tends to expand faster than the serial part, e.g. for matrix multiplication the initialization scales linearly with the matrix size while the multiplication itself scales as $O(n^3)$\cite{gustafson1988reevaluating, introduction_hpc_hager}. The former means that it is closely related to weak scaling\cite{introduction_hpc_hager}. So in a way it says that the execution time remains constant rather than the amount of data. More precise it says that the expected speed up with p threads and F fraction of the code that is parallel is\cite{gustafson1988reevaluating, cuda_best_practice, introduction_hpc_hager}
$$S(p)=p+(1-F)(1-p)$$

\begin{figure}[h]
    \centering
    \includegraphics[width=13cm]{GustafsonsLaw.png}
    \caption{Illustration of Gustafson's Law}
    \label{fig:GustafsonsLaw}
\end{figure}

\subsection{Profilers}
There are applications called profilers that are made to assess the programs performance and resource consumption. They calculate some of the measures mentioned earlier and they also check hardware usage and how much time the program spends at various parts of the program. This is very useful for finding bottlenecks and other problems in the program. It does not matter if the algorithm is super fast if all the data is stuck in network transfers. The profilers can be hardware dependent so the manufactures usually provided them for their products.\cite{introduction_hpc_hager, cuda_best_practice}

%TODO lite om nsight

%TODO screenshot av nsight

\clearpage
\section{CUDA programming model}
\label{cuda_programming_model}
This section is about GPU programming in more detail using NVIDIAs CUDA(Compute Unified Device Architecture)\cite{cuda}. CUDA extends C and C++ with some additional functionality that can be used to perform operations on NVIDAs GPUs. It provides a separate compiler to compile the GPU code called nvcc\cite{cuda}. Different GPUs support different CUDA functions, each NVIDIA GPU has a value called computational capability. The higher the value the more features of CUDA are supported on that GPU. Some properties also vary between the different underlying architectures. This means that it is important to know what kind of capability the GPU to be used has so that the program does not need features that are not there. The things described here might not apply to GPUs with compute capability below 3. There are GPUs made specifically for calculations rather than games. This section is an summary of how CUDA works from the CUDA programming guide\cite{cuda} and CUDA best practices guide\cite{cuda_best_practice}.\\
\\
In CUDA the GPU is called \emph{device} and systems CPU and memory is the \emph{host}. To perform operations on the device a type of function called \emph{kernel} is used. A kernel is works mostly as a normal C/C++ function with the addition of some specifiers to provided options to set number of threads and so on. Example \ref{simple_kernel} is an example of a kernel and how it can be called from the host code. The kernel add each element of the arrays A and B storing it in C. Each thread performs on addition and knows why elements to use based on it's id. The call to the kernel is made by giving three arrays and the number of threads to be used as N. In this case N needs to be the length of the arrays.\\
\\
The \_\_global\_\_ keyword in front of the function declaration tells nvcc that it is a kernel. The $<<<N,M>>>$ specifier tells the compiler how many $N$ blocks and how many $M$ threads per block that the kernel should use\cite{cuda}. A block is a group of threads and shares some memory and resources. The blocks can be executed in any order so they need to be completely independent from each other but variables can be shared among threads inside a block\cite{cuda}. This allows the program to scale to different GPUs as shown in figure \ref{fig:blocks_scaling}. The blocks are  organized in a one, two or three dimensional \emph{grid}. These can be accessed by each thread so it knows which grid it is in as illustrated in figure \ref{fig:grid_2d}. This can be used to make it easier to assign the threads to the correct bit of the calculation. For instance using a two dimensional grid is good for matrices\cite{cuda, cuda_best_practice}.

\begin{example}{Simple Kernel}{simple_kernel}
// Kernel definition
@__global__@ void Add(float* A, float* B, float* C)
{
  int i = threadIdx.x;
  C[i] = A[i] + B[i];
}

// In the host code
// Kernel invocation with N threads
Add@<<<1, N>>>@(A, B, C);
\end{example}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{grid_scale.png}
    \caption{Blocks execution depending on the number of streaming multiprocessors}
    \label{fig:blocks_scaling}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{2D_grid.png}
    \caption{2D grid of blocks}
    \label{fig:grid_2d}
\end{figure}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{SIMT.png}
    \caption{SIMT architecture}
    \label{fig:SIMT}
\end{figure}

The streaming multiprocessors(SM) is the hardware that handles the execution of these blocks and can execute hundreds of threads concurrently. It uses \emph{SIMT}(Single-Instruction, Multiple-Thread) which is similar to SIMD described earlier in section \ref{concurrency}. SIMT works mostly as SIMD except that it can act as MIMD on collections of threads called \emph{warps}. Each warp consists of 32 threads. When an SM gets a block it is split into warps that are assigned to warp schedulers. Each warp scheduler gives one instruction to a warp so full efficiency is achieved when all the 32 threads perform the same instruction. If there is any divergence it has to disable unrelated threads, so divergence can be costly. However different groups of warps are on different warps schedulers so can diverge without problem.\cite{cuda}

\subsection{Device Memory}
The GPUs memory is physically on a different device separate from the computers main memory which means that they have separate memory spaces. An object in one memory is not accessible in the other memory. The computers main memory is the \emph{host memory} and the GPUs memory is the \emph{device memory}. Since they are separate data has to be transferred to the device memory and this is done by explicit calls to transfer sections of the hosts memory.\cite{cuda}\\
\\
The GPU also have several different types of memory\cite{cuda}. Correct usage can give increased speed\cite{cuda, cuda_best_practice}.

\begin{itemize}
  \item Register memory is located on the multiprocessor and usually costs zero cycles to access. The multiprocessor splits the available registers over its threads so if there are many threads that uses many variables not all of them will fit in the register. This is why a program sometimes can be faster with lower number of threads.\cite{cuda}
  \item Global memory is the main memory of the GPU and is accessible from all threads and blocks. However it is relatively slow to access.\cite{cuda}
  \item Shared memory is shared inside a block and is faster than global memory. However it is limited in size.\cite{cuda}
  \item Constant memory is small however it is read only which enables some optimizations. It is best used for small variables that all threads access.\cite{cuda}
  \item Local memory is tied to the threads scope, however it still resides off-chip so it has the same access time as global memory.\cite{cuda}
  \item Texture memory is read only and can be faster to access than global memory in some situations. This was more important in older GPUs when global memory was not cached.\cite{plink_gpu, cuda}
  \item Read-only cache is available on GPUs based on Kepler architecture and uses the same cache as the texture memory. The data as to be read only each multiprocessor can have up to 48kb of space depending on GPU.\cite{kepler_tuning_guide}
\end{itemize}

\subsection{Streams}
\label{streams}
The kernel calls can be made on a \emph{stream}. The easiest way to think of the stream is as queue. The kernels on a stream will be executed in the order they are made but kernels from different streams can be executed in any order given there is enough computational resources. In this way it is possible to execute up to 32 concurrent kernels depending on GPU.\cite{cuda}\\
\\
This is not entirely true for some GPU architectures because they only have one queue for the kernels. Because of this independent kernels from different streams can block each other. If we have a group of kernels and issue them on stream one first and then on stream two all the kernels in stream two will be stuck in the queue waiting for the kernels in stream one to finish. This is because the kernels queue only looks at the first kernels to see if they can be executed. The second kernel in the first stream then blocks the queue from moving forward so it will not see the kernels on stream two that could be executed. Newer architectures after Kepler on the other hand has several queues so they don't have this problem.\cite{cuda, cuda_best_practice, kepler_tuning_guide}\\
\\
Streams are also important for asynchronous transfers. These transfers are executed on a stream and just as kernels gets executed after the previous kernels on the same stream is done. The advantage is that other streams can do calculations as normal while the transfer happens. This can hide the time for transfers completely in some situations as shown in \ref{fig:ascynchronous}. However the host memory has to be pinned, pinned memory means that the operative system can not page that memory. Paging is that the operative system stores a part of the memory in another area, usually the disk memory, to save space in the memory. Too much pinned memory can slow down the computer.\cite{cuda, overlap_transfers_cuda}\\
\\
However there is problem that asynchronous transfers can cause depending on the GPUs architecture\cite{overlap_transfers_cuda, cuda_fortran_overlap}. Some older GPUs only have one copy engine while newer have one for each direction, one to the GPU and one from the GPU\cite{overlap_transfers_cuda, cuda_fortran_overlap}. This can affect how the calls should be ordered and using the wrong one can make the performance worse than without using asynchronous transfers\cite{overlap_transfers_cuda, cuda_fortran_overlap}.

\newpage
There is an example of this in \cite{cuda_fortran_overlap} which illustrates the problem. They have four versions of the same code, a sequential transfer version and three asynchronous transfer versions. Two different GPUs were used, one had one copy engine the other had two. In the asynchronous the data is split over four streams coloured differently in the figure \ref{fig:ascynchronous}. Version 1 initiates the calls by looping over the streams one by one and doing the transfer and kernel calls on that stream before moving on to the next\cite{cuda_fortran_overlap}. Version 2 makes all the host to device transfer calls for all streams first, then the kernels and then the device to host transfer call\cite{cuda_fortran_overlap}. Version 3 is the same as version 2 but with a dummy even after each kernel\cite{cuda_fortran_overlap}. The figure \ref{fig:ascynchronous} shows how the transfers and kernels are executed on the GPU.

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{streams_seq.jpg}
    \caption{Sequential versions}
    \label{fig:sequential}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{streams_async.jpg}
    \caption{Asynchronous versions. D2H=device to host transfer. H2D=host to device transfer. Each colour represents a stream.}
    \label{fig:ascynchronous}
\end{figure}

\newpage
\subsection{Efficient CUDA}
One of the main criticism against GPUs for general computing purposes is that it is hard to get good performance because it requires good knowledge about details of the GPU architecture, especially the memory architecture. This section is a summary of things that can be good to consider for CUDA programs from the CUDA programming guide\cite{cuda}, CUDA best practices guide \cite{cuda_best_practice} and a similar list as this one, however slightly outdated now since it is from 2010, from a master thesis about GPU in GWAS\cite{plink_gpu}.
\\
\begin{description}
  \item[Maximize parallelism] \hfill \\
  Structure the program and the algorithm in such a way that it is as parallel as possible and overlap the serial parts on CPU with calculations on the GPU.\cite{plink_gpu, cuda}
  \item[Minimize transfers between host and device] \hfill \\
  Moving data between host and device is expensive and should be avoided if possible. It can be better to run serial parts on the GPU rather than moving the data to the host to do the calculation on the CPU. The bandwidth between host and device is one of the large performance bottlenecks. This can be a problem when the data is to large to fit in the relatively small GPU dram.\cite{cuda, cuda_best_practice}
  \item[Find the optimal number of blocks and threads] \hfill \\
  There are many things affected by the number of blocks and threads so they should be considered carefully. It is a good idea to parametrize them so that they can be changed for future hardware and varied for optimization. NVIDIA has an occupancy calculator which can be helpful in determining the optimal numbers, however high occupancy does not mean high performance.\cite{cuda, cuda_best_practice}\\
  \\
  The number of blocks should be larger than the number of multiprocessors so that all multiprocessors have at least one block to execute. Having two blocks or more per multiprocessor can be good so that there are blocks that aren't waiting for a \_\_syncthreads() that can be executed. However this is not always possible due to shared memory usage and similar.\cite{cuda_best_practice}\\
  \\  
  The number of threads per block should be a multiplier of 32 but minimum 64. It's also important to remember that multiple concurrent blocks can reside on the same multiprocessor. Too large number of threads in a block and parts of the multiprocessor might be idle since there aren't a block small enough to use those threads. Between 128 and 256 threads is a good place to start.\cite{cuda_best_practice}
  \item[Use streams and asynchronous transfers] \hfill \\
  By using streams it is possible to overlap memory transfers with calculations as mentioned before. This means that the data for the next batch can be transferred while the current batch is calculated and when it is done it can start calculating on the next batch directly after the current one is done. This can hide the time for transfers completely in some situations. Depending on the time the transfers take versus the time the calculations take this can give significant speedup.\cite{cuda, overlap_transfers_cuda, kepler_tuning_guide}
  \item[Use the correct memory type and caches] \hfill \\
  Correct use of caches and memory is important for both CPU\cite{drepper2007cpumemory} and GPU. However it is more complicated on GPU since the caches are smaller and there are several types of memory as mentioned before\cite{cuda, cuda_best_practice}.
  \item[Avoid divergence] \hfill \\
  Each thread in a warp executes the same instruction at the same time so if some of threads diverge the rest will be ideal until they are at the same instruction again. This means it is important to use control structures such as if statements carefully to prevent threads from idling.\cite{cuda, cuda_best_practice}
  \item[Avoid memory bank conflicts when using shared memory] \hfill \\
  Shared memory is divided into equally-sized memory modules called banks that can be accessed at the same time for higher bandwidth. Bank conflicts occur when separate threads access the same bank. On some GPUs it is fine if all threads access the same bank. Bank conflicts are split into as many conflict-free requests as needed.\cite{cuda, cuda_best_practice}
  \item[Use existing libraries] \hfill \\
  Instead of writing everything from scratch it is usually a good idea to use already existing libraries. Especially when performance is important and most task are non trivial on GPUs so using an already optimized library is a good idea. Some of the most popular libraries for CUDA are:
  \begin{itemize}
    \item CUBLAS: BLAS implementation for CUDA. BLAS and LAPACK is a standard for a library that provides highly optimized functions for linear algebra.\cite{cublas}
    \item CULAtools: BLAS and LAPACK implementation for CUDA for both dense and sparse matrices\cite{culatools}
    \item MAGMA: BLAS and LAPACK implementation among other things that can distribute the work on both CPU and GPU\cite{magma_2010}
    \item Thrust: Template based library that tries to emulate C++ standard library\cite{thrust_gpu}
  \end{itemize}
  \item[Avoid slow instructions] \hfill \\
  There are some instructions that can be slow and should be avoided if possible, for instance type conversion, integer division and modulo. If a function is called with a floating point number that might be used as a double and require a conversion. By putting an $f$ at the end of the number it is told to be single precision float, for instance 0.5f. In some cases it is possible to use bitwise operations instead which is faster.\cite{cuda, cuda_best_practice}
  \item[Restricted pointers can give increased performance] \hfill \\
  Aliasing can be a problem as mentioned earlier. By using the \_\_restrict\_\_ keyword on pointers the compiler can be told that no aliasing will occur, however it is up to the programmer to make sure that is the case or there might be unexpected results. Not using aliasing reduces the number of memory accesses the CPU needs to make. However it increases register pressure so it can have an negative effect on performance.\cite{cuda}
  \item[Use fast math functions if precision isn't needed] \hfill \\
  There are three versions of the math functions. The double precision version is func() while the single precision function is funcf(). There is third faster but less accurate version \_funcf(). The option -use\_fast\_math makes the compiler change all the funcf() to \_funcf().\cite{cuda_best_practice}
  \item[Instruction level parallelism can increase speed]\hfill \\
  Just as for CPUs the GPUs can take advantage of instruction level parallelism. By unrolling loops this can give 2x the speed relatively simple\cite{volkov2011unrolling}.
\end{description}

\clearpage
\section{Software Design}
How to design software so it can be maintained over time has been a problem for a long time. Object oriented languages such as C and later Java and C++ arose because of problems with maintaining software. Badly organized and written code will cost productivity in the future when bugs and other problems stack up because of earlier mistakes. Fixing those bugs are likely to time consuming because it can be hard to find where they originated from. All code gather problems overtime, however a well designed system will degenerate significantly slower than one that was designed carelessly. The code will also be read by ourselves and others later which means readability is important if a future reader is to understand the code and find the parts they are interested in. \cite{cleancode2008, design_patterns}\\
\\
This is also important in science where others might wish to use the tools or repeat the experiments. A recent study\cite{comp_repro_2013} found that the repeatability in computer science is low. They used the term reproducibility however repeatability is a more fitting term because they didn't try to reproduce the results from scratch, they tried to repeat it using the same code. Of 513 articles they received the code from 231, 108 failed to build, 21 failed to run properly\cite{comp_repro_2013}. Just 102 worked properly\cite{comp_repro_2013}.\\
\\
There are ways to design the program and code in such a way that it is easier to read and maintain. Most of the things described here goes under the development technique called agile development and the concept of clean code and is a summary of some concepts from the book Clean Code\cite{cleancode2008}. SOLID is an acronym for five principles for object oriented programming and design\cite{cleancode2008}. When used together they are intended to make programs that are easy to maintain and extend\cite{cleancode2008}. As already mentioned readability is important. Correct names will make the code explain itself without other documentation, the code itself is the documentation\cite{cleancode2008}.

\begin{table}[h]
\begin{tabular}{ l l p{2in}}
  \hline
  Initial & Principal & Concept\\
  \hline
  S & Single responsibility principle & A class should only have a single responsibility \\
  O & Open/closed principle & A class should be open for extension but closed for modification \\
  L & Liskov substitution principle & If S is a subtype of T, then objects of type T may be replaced by objects of type S without without altering any of the desirable properties of the program(e.g. work performed)\\
  I & Interface segregation principle & Use several smaller and more specific interfaces instead of one large \\
  D & Dependency inversion principle & Depend on abstractions(e.g. interfaces) not details \\
  \hline  
\end{tabular}
\caption{The five SOLID principles, from \cite{cleancode2008}}
\label{table:solid_table}
\end{table}

Dependency injection is one way to implement dependency inversion principle\cite{cleancode2008}. Injection is passing the dependency to the dependent object. This is used instead of allowing the dependent object to construct or find the dependency.

\subsection{Unit Tests and Mocks}
\emph{Unit testing} involves testing the program in small units in isolation. Testing in isolation means that the test should only depend on the part of the program that is tested. If a part of the program is not working properly only its related tests should fail, not other tests for code that depends on it but otherwise works properly. This makes it easier to find errors when they do occur since the tests will pinpoint the unit which does not work.\cite{cleancode2008}\\
\\
It can be easy to denote test code as less important than the main code but they should be treated as equally important. Tests should also not be an inconvenience to use so they should be easy to run, take reasonable amount of time to complete and not require any outside interpretation whether they failed or not.\cite{cleancode2008}\\
\\
\emph{Mocking} is to replace a real object with a fake object called a \emph{mock} that for the code is indistinguishable from the real object. This allows one to create situations and test with more control and without depending on the real objects code. The second is important for unit tests since it enables one to test units that normally depend on others in isolation. In the first case it is useful in situations such as when one wants to test a class handling of a rare failure. Such failures can be hard and time consuming to induce. It is then easier to use a mock object that behaves like the failure has occurred. However mocking requires that the code for the class doesn't create the object itself since there is no way to replace the object with the mock. This is one of the reasons why dependency injection should be use.\cite{cleancode2008}

\subsection{Design Patterns}
\label{design_pattern}
A \emph{design pattern} in software design is a reusable general solution to a common recurring problem in a given context. It is templates and structures to solve the problem, however it is not code. However they are partially dependent on the programming language since different languages have different features and limitations.\cite{design_patterns}\\
\\
\emph{Consumer producer} is a concurrency pattern for when there are a number of consumers/workers and producers. They share a common queue for products. The producer generates some product and put it into the queue, while the the consumers consume the products. The problem is that the producers should not add to an non empty slot and that each product should only be consumed once.\cite{cleancode2008, design_patterns}

\newpage
\subsection{Version Control Software}
Backups for data, code, text and so on is important if something happens. One way to backup text and code projects is to use a version control software and is usually an important tool in developing\cite{git_book}. For this project Git was used and the source code can be found at \url{https://github.com/Berjiz/CuEira}. Version control software allows several developers to share a common \emph{repository} which allows them to work on different parts at the same time, it could even be in the same file\cite{git_book}. However changes at the same place causes \emph{merge conflicts} which usually needs to be solved manually\cite{git_book}. A version control software keeps track of all the changes made so if a part later turns out to be wrong that part can reverted to an older correct version\cite{git_book}. Version control software also have many other features\cite{git_book}.

\clearpage
\chapter{Algorithm}
%Algorithm (up till 20 pages)
%  - Current state - Basic algorithm, Data structure, memory consumption, parallelization, load balancing and scalability
%  - And the same for own your implementation

%TODO n√•n typ av intro h√§r

%OTHERS
%problem with exathsutive

%memory problems(GENIE, CUDALR and so on), possibly JEIRA/GEISA

%2 bit 3 bit data storage

%two stage analysis

%plink data format

\section{Current algorithm, JEIRA and GEISA}
JEIRA and GEISA uses Java and its built-in functions for concurrency. Since GEISA is based on JEIRA, as talked about in section \ref{jeira}, the underlying design is the same.\\
\\
The implementation uses the producer consumer pattern described in section \ref{design_pattern}. The main thread creates a queue of tasks. All the result producers iterates over this shared queue and outputs results. These results are placed in a queue a consumed by another thread that prints the results. The program reads all data at the start which means it can get memory problems for large datasets.

%risk allele diff

%strucuture picture

\section{CuEira}
CuEira(CUDA Environment Risk Analyser) was designed and developed as a part of this thesis. It is written in C++. Information about dependencies and how to compile can be found in appendix \ref{compile_cueira}.\\
\\
The program has two steps, an initialisation step and a calculation step. In the initialisation step it reads all the data except the information for the individuals in the SNPs, i.e. it doesn't read the bed file at that stage. For the calculations the main thread starts a number of threads, one for each CUDA stream. The number of streams depends on how many GPUs the computer have, CuEira starts three streams per GPU. The threads share the data that has been read in the initialisation, among them is an queue of the SNPs. The queue has an lock to prevent more than one thread at a time to fetch a SNP.\\
\\
Each thread asks the queue for a SNP, reads its information and launches the calculations. When its done it prints the results and asks the queue for a new SNP until the queue is empty. The calculations are done mostly on GPU however some small parts are done on the CPU. This was done for two reasons, to not to do small taks on the GPU and prevent it from doing larger kernels instead. When thread reaches parts where it mostly use the CPU another thread can use the GPU, this one of the reasons to use streams. Secondly one of the operations(the singular value decomposition, SVD) was not available in CUBLAS and turned out to be tricky to do. The matrix in question is small so it was decided to do the SVD operation using the CPU where a library was available. A future examination where more these parts are done on GPUs to would be interesting and could increase performance by minimizing transfers.

%structure picture

%why lr
%-less sever drawbakcs for non binary
%-can handle non binary data(however statistical problems needs to be solved)
%-slow but is not too slow for gene-environment interaction

%\subsection{Design of the Program}
%C++ is row major but CuEira is write as column major. This was easier because BLAS and CUBLAS are column major.

%In the initialisation step it reads all the data except the information for the individuals in the SNPs, i.e. it doesn't read the bed file at that stage.
%Shared blocking queue and a result writer
%3 threads per GPU
%Each thread has one CUDA stream
%Each thread gets a snp from the queue, reads in the snp data, calculates the models and writes the results

%Outcomes shared over whole GPU

%LR
%Some parts on GPU, some parts on CPU
%Which parts?
%Why?

%memory things, how many individuals before we get problems?
%modern GPUs for general computing got about 6gb ram\cite{nvtesla}
%what is needed to store one set of data?
%n number of persons
%m number of variables, 4+number of covariates
%a single float needs 4 bytes
%3 streams

%shared
%outcomes, vector length n

%per stream
%predictors, n*m
%probabilities n
%score m
%beta m
%work area n
%work area n*m
%information matrix m*m

%number of floats=n+3(2*n*m+2*n+2*m+m*m)=6nm+mm+6m+7n

%number of bytes with m=4(no covariates)
%124n+160 approx 124n since n large
%$6*10^9/124 approx 48*10^6$

\chapter{Results}
%What was used
%Zorn specs
%compiler intel 14.0
%cmake 2.8
%mkl 13.1
%cuda 5.5

%single vs double precision

%compiler stuff
%no aliasing
%fast math
%-opt2 and such options

%personHandler is slow

%TODO performance measures, 1,2,3,4 gpus
%vary streams per gpu 1-4, on several gpus and on ones

%kepler vs fermi stuff
%kepler vs fermi queue problem, problem described in section \ref{streams}

Some first preliminary results on a small dataset with 2000 individuals and 100 SNPs and a larger dataset with 3446 individuals and 133 648 SNPs. GEISA is much faster than CuEira on the small dataset. CuEira takes 12.17 seconds to complete using 3 streams on 4 C2050 GPUs with the calculations taking 0.3 seconds. GEISA takes 2.6 seconds with calculations taking less than a second. A closer look at CuEira reveals that a part in the reading of the input files takes 10 seconds.\\
\\
The larger dataset took 6 minutes and 28 seconds for GEISA, with 5 minutes and 24 seconds spent on calculations. CuEira takes 3 minutes and 55 seconds, with calculations taking 3 minutes and 40 seconds. The slow part mentioned in the earlier paragraph took 10.8 seconds. However the numbers for CuEira on this dataset should be taken with a grain of salt. CuEira skips SNPs with missing data currently while GEISA does not. A dataset without missing data will be used for the final report, together with some variations in size.\\
\\
The data structure for the individuals in CuEira likely needs to be changed, it would likely cost much less time as a vector instead of a hashmap. However it seems to remain fairly constant on 10 seconds so might not be a problem for large datasets.

\chapter{Discussion and Conclusions}
%Discussion and Conclusions (up to 3 pages)
%Need for novel methods for gene-environment interaction
%Need for methods for third order interaction and higher
%more statistics for non binary

%TODO look back att efficent cuda part

%TODO divergence problems, nothing now but could be with recoding on GPU or bootstrap
%idea for solution using pointer architmic

%Using two separate compilers did cost some time. For instance there was a problem with nvcc because it does not support c++11 host code. Mostly it is not a problem because the code %can be compiled with different compilers separately and then put together\cite{cuda}. However the interfaces(e.g. function signatures) that are included in the code for nvcc needs to %be c++11 free. However compared to the total time spent on the project it was not much.

\chapter{Outlook}
%Outlook (up to one page)
%more statistics for non binary

%need to change mkl to any blas and remove dependency on intel compiler

%clusters
%kinda possible by splitting the input files

%cublas not that much harder than regular blas. A wrapper could enable simple and easy use without losing too much features.
%however will get problems with large data amounts, can't fit in device memory
%own kernels more than simple elemntwise, probably hard to optimize. unknowish

\chapter{Appendix}
\section{List of Variables}

\begin{description}
%anything with hat is estimate
\item[$\theta_{ij}$]
  Odds ratio with exposure levels i and j
\item[$RR_{ij}$]
  Relative risk with exposure levels i and j
\item[X]
  Predictors
\item[Y]
  The outcome, binary variable
\item[$\pi(x)$]
  = $P(Y=1)$
\item[$\Omega$]
  Odds
\end{description}

\section{List of Abbreviations}

\begin{description}
\item[MAF] \hfill \\
  Minor allele frequency, the frequency of the least common allele.
\item[RF] \hfill \\
  Random Forest
%\item[MDR] \hfill \\
%  Random Forest
%\item[OR] \hfill \\
%  Random Forest
%\item[RR] \hfill \\
%  Random Forest
%\item[RERI] \hfill \\
%  Random Forest
%\item[AP] \hfill \\
%  Random Forest
%\item[SI] \hfill \\
%  Random Forest
%\item[GPU] \hfill \\
%  Random Forest
%\item[CPU] \hfill \\
%  Random Forest
%\item[SM] \hfill \\
%  Streaming multiprocessor
%\item[SIMD] \hfill \\
%  Random Forest
%\item[MIMD] \hfill \\
%  Random Forest
%\item[SIMT] \hfill \\
%  Random Forest
%\item[SISD] \hfill \\
%  Random Forest
%\item[LR] \hfill \\
%  Random Forest
%\item[MIDSD] \hfill \\
%  Random Forest

% a t g c
\end{description}

%\section{Risk Allele Code from GEIRA, JEIRA and GEISA}
%\label{geira_jeira_risk_allele}
%GEISA(vesion 0.13) has the same code as JEIRA to determine the risk allele.\\
%\\
%CaMaxn and CoMaxn is the frequency for the cases and controls. CaMax and CoMax is the name of the allele that has the highest frequency in cases and controls.
%\begin{algorithmCode}{GEIRA code to determine the risk allele} {alg:geira_risk}
%if (CaMaxn>CoMaxn & CaMax==CoMax){
%  crisk<-CaMax
%} else if (CaMaxn<=CoMaxn & CaMin==CoMin){
%  crisk<-CaMin
%} else if (CaMaxn>CoMaxn & CaMax!=CoMax){
%  crisk<-CaMax
%} else if (CaMaxn<=CoMaxn & CaMin!=CoMin){
%  crisk<-CaMin
%}
%\end{algorithmCode}

%\begin{algorithmCode}{JEIRA code to determine the risk allele} {alg:jeira_risk}
%if (caseMaxRatio > controlMaxRatio && caseMaxAllele.equals(controlMaxAllele))
%  summary.setRiskAllele(caseMaxAllele);
%else
%  summary.setRiskAllele(caseMinAllele);
%\end{algorithmCode}

\section{Environmental and Covariates File Format}
The environmental factors and covariates are stored in separate files with the same format, one file for environmental factors and one for the covariates if any. One of the columns needs to contain the individual ids, the rest of the columns should be data. The delimiter can be anything of the usual ones and there is an option to provided it to the program. The default is tab delimited.

%List of figs
%List of tables

%\section{Risk Allele Difference Results}
%geisa vs cueira on sample file, much more recode 1 and 3

\section{Encountered Bugs}
\label{found_bugs}
This is a small collection of various bugs and limitations encountered in the course of the project that can be good to know for anyone that wants to compile CuEira or if the libraries are used in other projects.\\
\\
There are several problems with Intel compilers and libraries. Most libraries work fine with gnu C/C++ compiler but have problems with Intel compilers. Boost 1.55 does not work with Intel compilers, 1.54 works. AllOf in Google Test also does not work together with Intel compilers.\\
\\
Nvcc can not compile c+11 code, no interface included in the code that nvcc compiles can use c++11. This problem can largely be avoided by using separate compilation. The CUDA find package module for CMake(included in version 2.8 and later) has a default flag that should be removed as it can cause problems. The flag tells nvcc to propagate the host code flags, if the compiler for the host code then uses c++11 that flag is propagated to nvcc and it will not compile The flag is disabled in CMake by using set(CUDA\_PROPAGATE\_HOST\_FLAGS off).

\section{How to Compile CuEira}
\label{compile_cueira}
The source code for the program is available at github, \url{https://github.com/Berjiz/CuEira}.

\begin{description}
    \item[List of dependencies. Listed version is the version used.]
\end{description}
\begin{itemize}
 \item Boost, tested with version 1.54, 1.55 does not work due to a bug with Intel compilers
 \item CMake, version 2.8
 \item CUDA, version 5.5
 \item MKL, version 13.1
 \item Intel compiler version 14.0
 \item Google Test and Google Mock, already included in the folder.
\end{itemize}

First install the dependencies, they might need to be compiled from source. Open a terminal and make a new directory where you want the program to be compiled. Enter the directory and write:\\
cmake /path/to/CuEira/\\
Then type:\\
make\\
\\
The program should now be compiling. CMake might complain about not finding a dependency. If it does you need to tell CMake where it is. The path to boost is set by using EXPORT BOOST\_ROOT = /path/to/boost/. The path to the compilers is done with CXX = cmake /path/to/source for the C++ compiler and C = cmake /path/to/source for the C compiler.

%TODO set cuda

\newpage
\bibliographystyle{ieeetr}
\bibliography{hpc.bib,statistics.bib,misc.bib}
\end{document}