\documentclass[10pt,a4paper]{report}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[font=small,format=plain,labelfont=bf,up,textfont=it,up]{caption}
\usepackage[nottoc,notlof,notlot]{tocbibind}
\usepackage{url}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage[titles]{tocloft}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}
%\usepackage[ruled,vlined,resetcount,algochapter]{algorithm2e}
\usepackage[ruled,vlined,resetcount]{algorithm2e}
\usepackage[toc,page]{appendix}
\usepackage{tocloft}

\setcounter{secnumdepth}{5}
\setlength{\parindent}{0in}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\definecolor{keyword}{RGB}{70,100,200}
\definecolor{kernel}{RGB}{139,0,0}

\newcounter{ExamplesCounter}%[chapter]

\makeatletter
%\@addtoreset{algorithmslist}{chapter}
%\@addtoreset{example}{chapter}
\makeatother  

\newcommand{\listexamplename}{List of Examples}
\newlistof{example}{ex}{\listexamplename}
\newcommand{\example}[1]{%
\refstepcounter{example}
\addcontentsline{ex}{example}
%{\protect\numberline{\thechapter.\theexample}#1}\par}
{\protect\numberline{\theexample}#1}\par}

\newcommand{\listalgorithmslistname}{List of Algorithms}
\newlistof{algorithmslist}{alg}{\listalgorithmslistname}
\newcommand{\algorithmslist}[1]{%
\refstepcounter{algorithmslist}
\addcontentsline{alg}{algorithmslist}
%{\protect\numberline{\thechapter.\thealgorithmslist}#1}\par}
{\protect\numberline{\thealgorithmslist}#1}\par}

\def\nese{\mathrel{%
    \mathchoice{\NESE}{\NESE}{\scriptsize\NESE}{\tiny\NESE}%
}}
\def\NESE{{%
    \setbox0\hbox{$\nearrow$}%
    \rlap{\hbox to \wd0{\hss $\searrow$ \hss}}\box0
}}

\def\nesecol{\mathrel{%
    \mathchoice{\NESECOL}{\NESECOL}{\scriptsize\NESECOL}{\tiny\NESECOL}%
}}
\def\NESECOL{{%
    \setbox0\hbox{$\nearrow$}%
    \rlap{\hbox to \wd0{\hss {$\boldsymbol{\searrow}$} \hss}}\box0
}}

\def\nwsw{\mathrel{%
    \mathchoice{\NWSW}{\NWSW}{\scriptsize\NWSW}{\tiny\NWSW}%
}}
\def\NWSW{{%
    \setbox0\hbox{$\boldsymbol{\nwarrow}$}%
    \rlap{\hbox to \wd0{\hss $\swarrow$ \hss}}\box0
}}

\makeatletter
\newcommand\ackname{Acknowledgements}
\if@titlepage
  \newenvironment{acknowledgements}{
      \titlepage
      \null\vfil
      \@beginparpenalty\@lowpenalty
      \begin{center}%
        \bfseries \ackname
        \@endparpenalty\@M
      \end{center}}%
     {\par\vfil\null\endtitlepage}
\else
  \newenvironment{acknowledgements}{
      \if@twocolumn
        \section*{\abstractname}
      \else
        \small
        \begin{center}
          {\bfseries \ackname\vspace{-.5em}\vspace{\z@}}
        \end{center}
        \quotation
      \fi}
      {\if@twocolumn\else\endquotation\fi}
\fi
\makeatother

\title{Gene-Environment Interaction Analysis Using Graphic Cards}
\author{Daniel Berglund}
\date{2 February 2015}

\begin{document}
\maketitle
\thispagestyle{empty}

\clearpage
\thispagestyle{empty}
\selectlanguage{english}
\begin{abstract} %TODO abstract in english

Genome-wide association studies(GWAS) are used to find associations between genetic markers and diseases. One part of GWAS is to study interaction between markers which can play an important role in the risk for the disease. The search for interactions can be computationally intensive. The aim of this thesis was to improve the performance of software used for gene-environment interaction by using parallel programming techniques on graphical processors. A study of the new programs performance, speedup and efficiency was made using multiple simulated datasets. The program shows significantly better performance compared with the older program.

\end{abstract}

\clearpage
\thispagestyle{empty}
\selectlanguage{english}
\renewcommand{\abstractname}{Sammanfattning}
\begin{abstract} %TODO abstract in swedish

Genome-wide association studies(GWAS) används för att hita associationer mellan genetiska markörer och sjukdommar. En del av GWAS är att studera interkation mellan markörer vilket kan spela en viktig roll för sjukdomsrisken. Målet med det här arbetet var att förbättra prestanda av mjukvaran som används för gen-miljö interaktion genom att använda tekniker för parrallel programmering på grafikkort. En studie av det nya programmets prestanda, uppsnabbning och effektivitet genomfördes på flera simulerade datasätt. Programmet har signifikant bättre prestanda jämfört med det äldre programmet.

\end{abstract}

\clearpage
\thispagestyle{empty}
\begin{acknowledgements}
%TODO acknowledgements
I would like to express to my thanks to Lilit Axner and Henrik Källberg for providing and supervising this project. I would like to thank KIRC for all the interesting discussions and welcoming atmosphere, it has been an interesting time from which I have learned a lot. I would like to thank Michael Schliephake and PDC for the support and allowing me to use the Zorn cluster.
\end{acknowledgements}

\clearpage
\tableofcontents
\thispagestyle{empty}

\clearpage
\setcounter{page}{1}
\chapter{Introduction}

\section{Outline}
The first chapter of this thesis contains a short introduction to the area and the terminology. The second chapter gives the theoretical background. It starts with the statistics and algorithms and then describes the computer architecture and software design. The third chapter explains the current algorithm and also explains the design of the program that was developed in this thesis called CuEira. Chapter four provides the results and comparisons of the program performance. The fifth chapter contains the discussion of these results. Finally the last chapter discusses future work.

\section{Genome-wide association studies}
One type of study to find associations between genetic markers and diseases or other traits in different individuals is genome-wide association study(GWAS). Most GWAS do not study interactions between the genetic markers or between the genetic markers and environmental factors~\cite{cordell_detect_review, gene_enviroment_2013}. Investigating gene-gene interactions recently has become more common~\cite{cordell_detect_review}, however gene-environment interactions are still uncommon in studies~\cite{gene_enviroment_2013}. Interactions between genes and environmental factors are considered to be important for complex diseases such as cancer or autoimmune diseases~\cite{cordell_detect_review, gene_enviroment_2013, geira, ra_smoking}. In general a complex disease develops due to a combination of factors, not just a single gene or environmental factor~\cite{rothman2008modern}. Examples of environmental factors are smoking, physical activity and so on.
\\
GWAS usually has a study design that is either cohort or case-control~\cite{rothman2008modern}. In cohort study a sample of a population is followed and over time some individuals will develop the disease of interest~\cite{mann_observational}. In case-control studies two groups are compared with each other to find the risk factors~\cite{mann_observational}. One of these two groups consists of individuals with the disease and the other group consists of individuals similar to the first group but who do not have the disease~\cite{mann_observational}.\\
\\
A typical GWAS consists of tens of thousands of individuals and up to several millions of genetic markers~\cite{cordell_detect_review, burton2007genome}. In gene-gene interaction GWAS   few studies investigate more than the second order interaction because of the high number of possible combinations. There are some algorithms that can investigate higher order interactions however these have drawbacks~\cite{gwis,high_order_2012,fast_high_order_cluster}.

%Problem vs gene-gene and gene-env
%focuses on gene-env, datatyp etc

\section{Genetics}
The genetic information is stored in \emph{chromosomes} each consisting of a \emph{DNA molecule}~\cite{sadava_life}. Each chromosome comes in a pair where both chromosomes are nearly identical, except for the chromosomes related to gender~\cite{sadava_life}. The DNA molecule is a double stranded helix of four nucleotide bases, \emph{adenine}(A), \emph{cytosine}(C), \emph{guanine}(G) and \emph{thymine}(T). They are always paired as A-T and G-C. \emph{Genes} are sections of these DNA molecules. The pair of genes in the chromosomes is a \emph{genotype}. The observable product of the genotype is called \emph{phenotype}, e.g. eye colour, fur patters.\\
\\
A position of the DNA or a gene is a \emph{locus}~\cite{sadava_life}. A variation of the same gene or locus is an \emph{allele}~\cite{sadava_life}. In classic(i.e. Mendelian) genetics the effect of an allele on a phenotype can be either \emph{dominant}, \emph{recessive} or \emph{co-dominant}. Dominant means that if the allele is present the phenotype is expressed. For recessive the allele needs to be present in both chromosomes to be expressed~\cite{sadava_life}. Co-dominant is when both alleles are expressed, when different alleles are present this usually produce some kind of intermediate state~\cite{sadava_life}. An example of co-dominance is the human blood groups~\cite{sadava_life}.

\begin{table}[h]
\centering
\begin{tabular}{| l l l |}
  \hline
  Genotype & Dominant & Recessive\\
  \hline
  AA & Effect & Effect \\
  GA & Effect & No Effect \\
  GG & No effect & No effect \\
  \hline  
\end{tabular}
\caption[The phenotype based on the genetic model]{The phenotype based on the genetic model and if the  allele with the effect is A}
\label{table:genetic}
\end{table}

\subsection{Major, Minor and Risk Allele}
\label{risk_allele}
The the allele that is least common using all individuals(both cases and controls) is the \emph{minor allele}, the one that is most common is the \emph{major allele}. The frequency of the minor allele is the \emph{minor allele frequency}(MAF)~\cite{uvehag_master_thesis, geira}. If the MAF is too low an analysis often will be of little value so all SNPs with MAF below a threshold are usually skipped. 5\% is common value of the threshold~\cite{burton2007genome, geira}.\\
\\
To determine if the genetic risk is present for each individual one of the alleles needs to be chosen as the \emph{risk allele}. GEIRA and JEIRA defines the risk allele as the minor allele in cases if the MAF of cases is greater than the MAF of controls, otherwise the major allele in cases is used~\cite{geisa, uvehag_master_thesis}. However JEIRA and GEISA does not calculate the risk allele according to that definition, see appendix~\ref{risk_allele_appendix} for the details. In this thesis the risk allele will be defined as the allele that has higher frequency in cases than controls. It is similar to the definition using MAF in most cases. Appendix~\ref{risk_allele_appendix} has more detailed comparisons.\\
\\
\newpage
The presence of genetic risk based on the risk allele and the genetic model can be coded as a variable than then be can used in a statistical model. For dominant or recessive genetic model this means that the variable is binary since the risk is either present or not~\cite{geira}. See table~\ref{table:genetic} for how the coding is based on the model. For co-dominant model the number of risk alleles in the individual is used as the variable, 0, 1 or 2~\cite{geira}.

\section{Interaction}
\label{interaction}
There are several ways to define interaction. The overall goal is often to detect if \emph{biological} interactions are present. \emph{Biological} interaction is when different factors co-operate through a physiological or biological mechanism and cause the effect, e.g. the disease. The information about the factors can then be used to explain the mechanisms involved in causing the disease and possibly help to find cures for them. However biological interaction is not well defined and thus it is not possible to calculate it directly from data.~\cite{interaction_confusion, rothman2008modern,rothman2002intro_epidemiology}\\
\\
That is why statistical interaction is used and it is assumed that the presence of statistical interaction implies the presence of biological interaction

\emph{Statistical} interaction on the other hand is well defined. However it is scale dependent, i.e. interactions can appear and disappear based on transformations of the given data. Statistical interaction also depends on the model used. The common way to define statistical interaction is to consider the presence of a product term between the variables in the model, this is referred to as \emph{multiplicative} interaction. For instance for a linear model
\begin{equation}
f(x,y)=ax+by+cxy+d
\end{equation}
$c$ is the product term that represents multiplicative interaction between variables x and y so the test for multiplicative interaction is to test if $c=0$~\cite{geira, interaction_confusion, rothman2008modern}. In some cases it is instead tested as $a=b=c=0$, this is testing for \emph{association allowing for interaction}~\cite{cordell_detect_review}.\\
\\
\emph{Additive} interaction is another kind of statistical interaction that is broader than multiplicative interaction. It is when the total effect of the interacting factors present is larger than the sum of their separate effects. A more precise definition can be found in section~\ref{additive}. It implies biological interaction as defined by Rothman~\cite{rothman2008modern}, which is sometimes called \emph{causal interdependence} or \emph{causal interaction}~\cite{causal_bounds_arvid}. Additive interaction can find more possible interactions than multiplicative and since it is closer to biological interaction it is often chosen in epidemiological studies~\cite{interaction_confusion}.  

\newpage
\section{GEIRA, JEIRA and GEISA, the Aim of the Project}
\label{jeira}
Various tools can be used to for searching for interaction in genetic data. \emph{GEIRA}~\cite{geira} is one such tool which uses logistic regression, additive interaction and bootstrap to asses the quality of the statistical models~\cite{geira}. It comes in two versions, one in R and one in SAS. To improve speedup it was later implemented in Java as \emph{JEIRA}~\cite{uvehag_master_thesis}. JEIRA is also parallel which made it significantly faster~\cite{uvehag_master_thesis}. \emph{GEISA}~\cite{geisa} is another tool based on JEIRA however it uses permutation testing instead of bootstrap to asses the certainty of the models~\cite{geisa}. The structure is explained in chapter~\ref{implementation}. Bootstrap calculates the certainty by resampling the data~\cite{agresti_categorical}. Permutation tests works in a similar way but only randomizes the outcomes~\cite{lindgren1993statistical}.\\
\\
The aim of this thesis is to make the gene-environment interaction analysis faster by using graphics processors and to be able to handle larger amounts of data. The program is written in C++ and CUDA.

%\\
%The genetic markers mentioned previously are commonly single nucleotide polymorphism(SNP, pronounced snip). SNPs are variations in the genome where a single nucleotide is different %between individuals in a population~\cite{sadava_life}.\\

\clearpage
\chapter{Background}
This chapter will explain some of the background starting with the statistical background, continuing with computer architecture and software design. Finally is a section about the algorithms.

\section{Statistical Background}
This section will explain some of the statistical background with focus on categorical data.

\subsection{Contingency Tables}
A contingency table is a matrix used to describe categorical data~\cite{agresti_categorical}. Each cell contains a count of occurrences for a specific combination of variables~\cite{agresti_categorical}. Table~\ref{table:contingency_table} is an example of a 2 x 2 table. From this table we can for instance see that 688 smokers got lung cancer. Contingency tables are the basis for various statistical tests to model the data~\cite{agresti_categorical}.

\begin{table}[h]
\centering
\begin{tabular}{| l c c |}
  \hline
  & Lung cancer & No lung cancer\\
  \hline
  Smoker & 688 & 650 \\
  Non smoker & 21 & 59 \\
  \hline  
\end{tabular}
\caption[Contingency table describing the outcome of a study]{Contingency table describing the outcome of a study, from~\cite{agresti_categorical}, page 42}
\label{table:contingency_table}
\end{table}

\subsection{Relative Risk and Odds Ratio}
\label{rr_or}
From a contingency table it is possible to calculate some useful measures such as the risk of getting the outcome based on exposure~\cite{agresti_categorical}. The risk for a row $i$ in the table will be referred to as $\pi_i$. For table~\ref{table:contingency_table} the risks $\pi_1$ and $\pi_2$ is

\begin{equation}
\pi_1=\frac{688}{688+650}=0.51
\end{equation}

\begin{equation}
\pi_2=\frac{21}{21+59}=0.36
\end{equation}

To compare different risks, for instance between smokers and non smokers, the ratio of the risks is used~\cite{agresti_categorical}. It is called \emph{relative} risk(RR) is defined as~\cite{agresti_categorical} 

\begin{equation}
RR=\frac{\pi_1}{\pi_2}
\end{equation}

So for the table~\ref{table:contingency_table} the relative risk of getting lung cancer based on exposure to smoking is

\begin{equation}
RR=\frac{0.52}{0.36}=1.96
\end{equation}

This means that the risk of getting lung cancer for a smoker is almost twice as high for a non smoker in this data. Another useful measure is \emph{odds} and \emph{odds ratio}(OR)~\cite{agresti_categorical}. The odds is 

\begin{equation}\label{eq:odds}
\Omega=\frac{\pi}{1-\pi}
\end{equation}

The odds are non-negative and $\Omega>1$ when the outcome is more likely than not~\cite{agresti_categorical}. So for $\pi=0.75$ the odds is $\Omega=\frac{0.75}{1-0.75}=3$. This means that the outcome is 3 times more likely to occur than not. ORs can be used as an approximation of RR in case control studies because RR can not be estimated in that type of studies~\cite{or_mislead}. Cohort studies on the other hand can give estimates of RR~\cite{or_mislead}. OR is the ratio of the odds just as RR is the ratio of the risks~\cite{agresti_categorical}.

\begin{equation}\label{eq:odds_ratio}
\theta=\frac{\Omega_1}{\Omega_2}
\end{equation}

In the case when the outcome is a disease or similar variables with odds ratio below one are called \emph{protective} and when it is above one it is a \emph{risk factor}~\cite{recoding_2011}.

\newpage
\subsection{Additive Interaction}
\label{additive}
The precise definition of additive interaction is the divergence from additive effects on a logarithmic scale, e.g~\cite{rothman2008modern}.

\begin{equation}\label{eq:additive_interaction}
OR_{both\:factors\:present}>OR_{first\:factor\:present}+OR_{second\:factor\:present}-1
\end{equation}

A third variable is used to model the interaction for both multiplicative and additive interaction. Multiplicative interaction as explained in section~\ref{interaction} does not include the effects from the factors, i.e. the main effects, in the interaction variable. However for additive interaction the interaction variable models the whole effect including the main effects. This means that the third variable for multiplicative and additive interaction are identical, however for additive interaction the first and second variable is zero when both factors are present~\cite{uvehag_master_thesis}. The coding for additive interaction is summarized in table~\ref{table:coding}~\cite{uvehag_master_thesis}.

\begin{table}[h]
\centering
\begin{tabular}{| l | c c c|}
  \hline
  Factor present & First variable & Second variable & Interaction \\
  \hline
  None & 0 & 0 & 0 \\
  \hline 
  First & 1 & 0 & 0 \\
  \hline
  Second & 0 & 1 & 0 \\
  \hline
  Both & 0 & 0 & 1 \\
  \hline
\end{tabular}
\caption{Coding of the variables for LR}
\label{table:coding}
\end{table}

\subsection{Statistic Measures for Additive Interaction}
\label{statistic_measures}
Based on the statistical model and its corresponding ORs there are some measures that can be calculated. These measures show various properties of the additive interaction~\cite{geira, recoding_2011}. They are defined using $RR$ however as mentioned before in section~\ref{rr_or} $OR$ can be used to approximate $RR$.\\
\\
\emph{Relative  excess risk due to interaction}(RERI) is how much of the risk is due to interaction~\cite{rothman2008modern, recoding_2011}. It is defined as~\cite{rothman2008modern, recoding_2011}
\begin{equation}
RERI=RR_{11}-RR_{10}-RR_{01}+1
\end{equation}

\emph{Attributable proportion due to interaction}(AP) is similar to RERI however is the proportion relative to the interaction relative risk~\cite{rothman2008modern, recoding_2011}.
\begin{equation}
AP=\frac{RERI}{RR_{11}}=\frac{1}{RR_{11}}-\frac{RR_{10}}{RR_{11}}-\frac{RR_{01}}{RR_{11}}+1
\end{equation}

\emph{Synergy index}(SI) is the ratio of the combined effects and the individual effects~\cite{rothman2008modern, recoding_2011}.
\begin{equation}
\frac{RR_{11}-1}{RR_{10}+RR_{01}-2}
\end{equation}

By using the definition of additive interaction it can be shown that additive interaction is present when $RERI>0$, $AP>0$ and $SI>0$~\cite{recoding_2011}.

\subsubsection{Recoding of Protective Effects}
\label{recode}
The measures for additive interaction RERI, AP and SI explained in the previous section are developed for risk factors, i.e. $OR>1$, which causes problems if a factor is protective~\cite{recoding_2011}. This can be solved by \emph{recoding}, it switches the group of individuals without both factors, i.e. the \emph{reference group}, with the group that has the lowest  $OR$~\cite{recoding_2011}. There are three possible combinations of the factors excluding the reference group shown in table~\ref{table:recoding_cases}. Switching a group changes the level of the factors for all in individuals in the group to the reference groups level, and switches the individuals in the reference group with the groups level.

\begin{table}[h]
\centering
\begin{tabular}{| l l l |}
  \hline
  Genetic factor & Environment factor & Recoding case\\
  \hline
  0 & 0 & N/A \\
  1 & 0 & 1 \\
  0 & 1 & 2 \\
  1 & 1 & 3 \\
  \hline
\end{tabular}
\caption{Cases of recoding}
\label{table:recoding_cases}
\end{table}

\subsection{Confounders and Covariates}
\emph{Confounding} is one of the central issues in design of epidemiological studies~\cite{rothman2002intro_epidemiology,rothman2008modern}. It is when the effect of the exposure is mixed with the effect of another variable~\cite{rothman2002intro_epidemiology,rothman2008modern, agresti_categorical}. So if we do not measure the second variable, the effect of the first would be estimated as stronger than it really is~\cite{rothman2002intro_epidemiology,rothman2008modern}. The second variable is then a \emph{confounder}~\cite{rothman2002intro_epidemiology,rothman2008modern}. Several methods in epidemiology are about avoiding or adjusting for confounding~\cite{rothman2002intro_epidemiology,rothman2008modern}. Sometimes these variables needs to be incorporated into the models. \emph{Covariates} are possible confounders or other variables that one wants to adjust for in the model. Sometimes covariates are called \emph{control variables}~\cite{rothman2002intro_epidemiology,rothman2008modern}.

\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{Simple_Confounding_Case.png}
    \caption[Illustration of a simple case of confounding]{Illustration of a simple case of confounding. If we do not observe Z we might falsely find an association between X and Y}
    \label{fig:confunding}
\end{figure}

For instance if a study would look into the effects of yellow teeth on lung cancer they would likely find an effect. However it would be due to the confounder smoking since smoking causes yellow teeth and lung cancer~\cite{smoking_teeth}.

\subsection{Multiple Testing}
It is not uncommon to test many hypothesis on the same data and in the case of GWAS it can be millions, or in the case of gene-gene interaction even trillions, of tests~\cite{cordell_detect_review}. If one continues to test one should eventually find something that is significant due to random chance~\cite{bonferroni_multiple}. With the common significance threshold of 5\% it is expected to get 1 in 20 false positives under the assumption that the null hypothesis is true. The problem arises from the fact that the hypothesis tests are dependent on each other since they use parts of the same data~\cite{bonferroni_multiple}. This is the multiple testing problem and if it is not corrected for many false positives might be found~\cite{bonferroni_multiple}.\\
\\
Bonferroni correction is one of the simplest method and viewed as a conservative way to correct for this problem~\cite{bonferroni_multiple}. It simply divides the significance threshold by the number of hypothesis tested~\cite{bonferroni_multiple}. However the number of hypothesis made is not always clear. With a two-stage analysis, is the number of hypothesis the number of tests done in both stages combined, the number made in the first stage or the second stage.

\newpage
\section{Computer Architecture Background}
This section explains some of the computer architectures used. A large part of this chapter is focused on the optimization techniques that are in use. For the purpose of this thesis it is best to think of the computer consisting of four main parts, the processor, the data storage, the memory and the accelerator.

\subsection{Central Processing Unit}
Central processing unit(CPU) is the part of the computer that executes instructions one by one~\cite{introduction_hpc_hager}. Most modern CPUs have multi-core architecture meaning they consist of several processors~\cite{introduction_hpc_hager}. Each core can perform tasks independent of each other. Multi-core architecture affects the way the programs are made because they need to be parallel to get maximum speed. More about this in section~\ref{concurrency}.\\
\\
The figures~\ref{fig:cpu_scheme} and~\ref{fig:core_die} shows how the CPU is divided into areas and that most of it is not used for calculations. A large part of the area is used for various optimizations.

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{cpu_scheme_haswell.jpg}
    \caption[The layout of the Haswell architecture i7-5960x]{The layout of the Haswell architecture i7-5960x, from~\cite{intel_haswell_2014}}
    \label{fig:cpu_scheme}
\end{figure}

\newpage
\subsection{Concurrency and Threads}
\label{concurrency}
\emph{Concurrency} means doing multiple things and the same time, instead of \emph{sequential}~\cite{intro_hpc, hoare1985communicating}. This can cause various problems such as the same object being accessed at the same time. The dinning philosophers is an concurrency problem that can used to illustrate some of these problems. A group of philosophers is sitting at a round table, each has a plate of spaghetti in front of them and there is a fork between each pair of philosophers~\cite{hoare1985communicating}. The philosophers alternate between thinking and eating. However they can only eat if they have both the right and left fork~\cite{hoare1985communicating}.

\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{dining_philosophers.png}
    \caption[Illustration of the dining philosophers problem]{Illustration of the dining philosophers problem. Wikipedia Commons}
    \label{fig:dining_philosophers}
\end{figure}

A potential proposal~\cite{hoare1985communicating} for behaviour instructions could be:

\begin{itemize}
 \item Think
 \item Wait for a fork to become available and pick up that fork
 \item Wait for and pick up the other fork
 \item Eat
 \item Put down the forks one by one
 \item Go back to thinking
\end{itemize}

The problem is that this set of instructions can lead to a state where everyone is holding one fork and waiting to get the other one~\cite{hoare1985communicating}. But no one is done eating so no forks will become available. The system is then locked into its state, this is called a \emph{deadlock}~\cite{hoare1985communicating, introduction_hpc_hager}. A potential solution to the problem in this case is to introduce a person that dictates if a philosopher is allowed to eat or not~\cite{hoare1985communicating}.\\
\\
There are also other potential problems that can arise when using concurrency. A \emph{race condition} occurs when the result depends on what affects it first, it is a race between them on who can reach it first~\cite{introduction_hpc_hager}. \emph{Locks} can be used to prevent situations as the ones described above by making sure only one thread at a time can access the object~\cite{introduction_hpc_hager}.\\
\\
In computer science concurrency occurs when instructions are separated in different \emph{threads}. The architectures can be divided in a taxonomy with four categories including the sequential. They are illustrated in figure~\ref{fig:flynn_taxonomy}.

\begin{itemize}
 \item \emph{SISD}(Single-Instruction,Single-Data) is the sequential and is as the name says, single instructions on single data~\cite{introduction_hpc_hager}.
 \item \emph{SIMD}(Single-Instruction, Multiple-Data) is applying the same instruction on different data~\cite{introduction_hpc_hager}. SIMD is also called vectorisation~\cite{introduction_hpc_hager}.
 \item \emph{MIMD}(Multiple-Instruction, Multiple-Data) applies different instructions on different data~\cite{introduction_hpc_hager}.
 \item \emph{MISD}(Multiple-Instruction, Single-Data) performs different operations at the same piece of data, this paradigm is uncommon~\cite{introduction_hpc_hager, computer_arch_2003}.
\end{itemize}

\begin{figure}
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{SISD}
                \caption{SISD}
                \label{fig:SISD}
        \end{subfigure}%
       ~
        \begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{MISD}
                \caption{MISD}
                \label{fig:MISD}
        \end{subfigure}

        \begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{SIMD}
                \caption{SIMD}
                \label{fig:SIMD}
        \end{subfigure}
       ~
        \begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{MIMD}
                \caption{MIMD}
                \label{fig:MIMD}
        \end{subfigure}
        \caption[Flynns taxonomy of parallel architectures]{Flynns taxonomy of parallel architectures~\cite{introduction_hpc_hager}}\label{fig:flynn_taxonomy}
\end{figure}

\newpage
\subsection{Memory, Caching and Optimizations}
\label{optimizations}
Various optimizations have been introduce to the CPUs over the years~\cite{drepper2007cpumemory, introduction_hpc_hager}. Some are common and almost all CPUs have them, others are unique to a specific vendor or CPU~\cite{introduction_hpc_hager}. Figure~\ref{fig:core_die} show the layout of a CPU core. This section explains some of the more common optimizations.

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{cpu_core_die.png}
    \caption[Layout of a CPU core, Intel Nehalem i7]{Layout of a CPU core, Intel Nehalem i7, from~\cite{tomshardware_nehalem}}
    \label{fig:core_die}
\end{figure}

In general retrieving data from memory is relatively slow compared to how fast the processor works~\cite{introduction_hpc_hager,drepper2007cpumemory}. To solve this problem a \emph{cache} was introduced~\cite{drepper2007cpumemory}. It stores recently used data in a very small and very fast memory that resides on the CPU chip itself~\cite{drepper2007cpumemory}. The caches varies in sizes, the latest ones are around 16-20 mB, for instance Intels i7-5960x with 20 mB~\cite{intel_haswell_2014_5960x}. The cached data can then be reused without waiting for the main memory to fetch it. However when the data is not in the cache it costs time to fetch it from the main memory, that is a \emph{cache miss}~\cite{drepper2007cpumemory}. Avoiding cache misses is important for the speedup~\cite{drepper2007cpumemory}. Modern CPUs commonly have more than one cache, most have three~\cite{introduction_hpc_hager}. They are named L1 to L3, with L1 being the one fastest and smallest and L3 the largest and slowest~\cite{introduction_hpc_hager}.\\
\\
Modern multi-core processors have several L1 caches, one per core~\cite{introduction_hpc_hager}. Commonly each core has a L2 cache as well, however it can be shared in some architectures~\cite{introduction_hpc_hager}. The L3 cache is almost always shared between all cores~\cite{introduction_hpc_hager}. The multiple caches used causes a problem called \emph{cache coherency}. It is when the data modified in one cache needs to be updated in all the caches in order for them to use the new value~\cite{introduction_hpc_hager}.\\
\\
One of the memory optimization methods somewhat related to caches is \emph{prefetching}~\cite{introduction_hpc_hager, drepper2007cpumemory}. Instead of fetching just the data requested by the current operations it also fetches the surrounding data~\cite{drepper2007cpumemory}. Even if the data is not needed at the given point in time, the chance that the surrounding data will be used soon is high since it is common to iterate over arrays and similar structures~\cite{introduction_hpc_hager}.\\
\\
When a program iterates over an array the first value is a cache miss since it is not in the cache. The following values are prefected and cached as well. However if the array is too long to be prefetched and cached completely and the CPU hits a position not in the cache then a new cache miss happens~\cite{drepper2007cpumemory}. Usually matrices are stored sequential in memory~\cite{drepper2007cpumemory}. For instance a two dimensional matrix is stored as a one dimensional array. Different programming languages store them differently, either column by column or row by row~\cite{drepper2007cpumemory}. Row by row is called \emph{row major} and column by column is \emph{column major.}\\
\\
Another optimization is \emph{branch prediction} and \emph{speculative execution}. When the operation instructions cause divergence, e.g. \emph{if} statements, the CPU has to wait for the previous instructions to finish to see which path it should diverge to~\cite{drepper2007cpumemory}. This can mean a significant loss of time, however because of \emph{branch prediction} and \emph{speculative execution} the loss of time can be prevented in some cases~\cite{drepper2007cpumemory}. A part of the CPU stores the outcomes from these divergences and when the same divergence is encountered again it uses the stored outcomes to make a guess of what do to next~\cite{drepper2007cpumemory}. The CPU then loads the instructions and speculatively executes the instructions, i.e it executes instructions that might not actually be needed~\cite{drepper2007cpumemory}. If the guess was correct the results are kept, on the other hand if it was wrong the CPU starts again from the correct path and discards the incorrect results~\cite{drepper2007cpumemory}.

\subsubsection{Cache-Friendly Code}
Correct and incorrect usage of caches can effect the performance significantly. In section 6.2.1 in What Every Programmer Should Know About Memory~\cite{drepper2007cpumemory} an example of how much speed that can be gained by optimizing matrix matrix multiplication. The variables here have been renamed for clarity. The simple version of the algorithm is shown in algorithm~\ref{alg:matmatmult} and the version with improved cache use is shown in~\ref{alg:matmatmultopt}.\\
\\
By transposing the \textit{matrix2} matrix it becomes more cache friendly by using prefectching of surrounding data the way that \textit{matrix1} already is~\cite{drepper2007cpumemory}. In the simple version, algorithm~\ref{alg:matmatmult} \textit{matrix1} is looped along the way it is stored in the memory while \textit{matrix2} is not. The transpose itself can be eliminated while also making it use the cache better by reading in the correct amount of data, this is the size $SM$ in the code~\cite{drepper2007cpumemory}. The optimized version is shown in~\ref{alg:matmatmultopt}. It took 17.3\% of the original time~\cite{drepper2007cpumemory}. On top of that by using some additional features of the CPU such the CPUs SIMD capabilities more speedup is gained increasing it to 9.47\% of the original time~\cite{drepper2007cpumemory}.

\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}%
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}%
\SetKwFor{For}{for}{}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
\SetKwFor{While}{while}{}{}%
\SetKwRepeat{Repeat}{repeat}{until}%
\AlgoDisplayBlockMarkers\SetAlgoNoLine%

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{Three $N \times N$ matrices, \textit{matrix1}, \textit{matrix2} and \textit{result}\\
$i$,$j$,$k$ are indices spanning from $0$ to $N-1$}

\BlankLine \BlankLine
\For{i = 0; i < N; ++i}{
  \For{j = 0; j< N; ++j}{
    \For{k = 0;k < N; ++k}{
      $result[i][j]+= matrix1[i][k] * matrix2[k][j]$;\;
    }
  }
}

\caption{Basic algorithm for matrix matrix multiplication of two $N \times N$ matrices}
\algorithmslist{Basic algorithm for matrix matrix multiplication of two $N \times N$ matrices}
\label{alg:matmatmult}
\end{algorithm}

\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}%
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}%
\SetKwFor{For}{for}{}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
\SetKwFor{While}{while}{}{}%
\SetKwRepeat{Repeat}{repeat}{until}%
\AlgoDisplayBlockMarkers\SetAlgoNoLine%

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{Three $N \times N$ matrices, \textit{matrix1}, \textit{matrix2} and \textit{result}\\
$rresult$, $rmatrix1$, $rmatrix2$ refers to parts of the matrices, they \; are used to eliminate common expressions\\
$SM$ is the number of matrix elements that fits in the cache
}

\BlankLine \BlankLine

\For{i = 0; i < N; i += SM}{
  \For{j = 0; j < N; j += SM}{
    \For{k = 0; k < N; k += SM}{
      \For{i2 = 0, rresult = \&result[i][j], rmatrix1 = \&matrix1[i][k]; i2 < SM; ++i2, rresult += N, rmatrix1 += N}{
        \For{k2 = 0, rmatrix2 = \&matrix2[k][j]; k2 < SM; ++k2, rmatrix2 += N}{
          \For{j2 = 0; j2 < SM; ++j2}{
            $rresult[j2] += rmatrix1[k2] * rmatrix2[j2]$;\;
          }
        }
      }
    }
  }
}
\caption{Matrix matrix multiplication algorithm optimized}
\algorithmslist{Matrix matrix multiplication algorithm optimized}
\label{alg:matmatmultopt}
\end{algorithm}

\clearpage
\subsubsection{Independent Instructions and Aliasing}
Sometimes there can be several independent instructions, e.g. when adding two vectors together, this allows optimizations for \emph{instruction level parallelism}~\cite{introduction_hpc_hager}. The order of the instructions can be ignored which for instance means that if an instruction needs to wait for some data another instruction that has its data available could be executed while waiting~\cite{introduction_hpc_hager}. It is an optimization called \emph{out of order execution}~\cite{introduction_hpc_hager}. There are also other possible optimizations~\cite{introduction_hpc_hager}.\\
\\
In example~\ref{ex:independent_instructions} the instructions are independent because they use different variables so they can be executed in any order. If the variables $b$ and $c$ needs to be fetched but $e$ and $f$ are available due to previous instructions then the CPU could execute the second addition first by using out of order execution. However if an operation later depends on the variable $a$ or $d$ then that instruction would have to wait until the additions have been executed.

\stepcounter{ExamplesCounter}
\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}%
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}%
\SetKwFor{For}{for}{}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
\SetKwFor{While}{while}{}{}%
\SetKwRepeat{Repeat}{repeat}{until}%
\AlgoDisplayBlockMarkers\SetAlgoNoLine%

\SetAlgoRefName{\arabic{ExamplesCounter}} %\SetAlgoRefName{\arabic{chapter}.\arabic{ExamplesCounter}}
\SetAlgorithmName{Example}{List of Examples}

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{Six int variables, $a,..,f$}

\BlankLine \BlankLine
$a = b + c$\;
$d = e + f$\;

\caption{Independent Instructions}
\example{Independent Instructions}
\label{ex:independent_instructions}
\end{algorithm}

\emph{Pointer aliasing} can prevent the compiler from making certain optimizations, e.g. out of order execution mentioned above. Pointer aliasing is when the same memory area can be accessed using different variables~\cite{introduction_hpc_hager}. This becomes a problem if a set of instructions use these variables in the same area of the program. The relative order of operations between these operations then has to be preserved. The compiler does have the information if aliasing occurs or not which means the compiler has to treat all situations where it can occur as if it does~\cite{introduction_hpc_hager}.\\
\\
If the integers in example~\ref{ex:independent_instructions} are changed to pointers then aliasing can occur because some of these pointers could point to the same memory. For instance if a and e points to the same memory then the result would be wrong if the second line is executed before the first. This would force the instructions to be executed in order or the result would likely be incorrect.

\stepcounter{ExamplesCounter}
\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}%
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}%
\SetKwFor{For}{for}{}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
\SetKwFor{While}{while}{}{}%
\SetKwRepeat{Repeat}{repeat}{until}%
\AlgoDisplayBlockMarkers\SetAlgoNoLine%

\SetAlgoRefName{\arabic{ExamplesCounter}} %\SetAlgoRefName{\arabic{chapter}.\arabic{ExamplesCounter}}
\SetAlgorithmName{Example}{List of Examples}

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{Six int pointers, $a,..,f$}

\BlankLine \BlankLine
$*a = *b + *c$\;
$*d = *e + *f$\;

\caption{Pointer Aliasing}
\example{Pointer Aliasing}
\label{ex:pointer_aliasing}
\end{algorithm}

Some programming languages, e.g. Fortran, disallow some types of aliasing while others, e.g. C/C++, allows aliasing~\cite{introduction_hpc_hager}. The compilers for the languages in the second case commonly have an option to disable aliasing. However for languages that don't allow aliasing, or if it is disabled, it is then the programmers responsibility to make sure that aliasing does not occur or the results can be wrong~\cite{introduction_hpc_hager}.

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{floating-point-operations-per-second.png}
    \caption[Theoretical throughput of some NVIDIA GPUs and Intel Processors]{Theoretical throughput of some NVIDIA GPUs and Intel Processors, from~\cite{cuda}. However the chart does not contain a Xeon Phi processor.}
    \label{fig:gpu_vs_cpu}
\end{figure}

\newpage
\subsection{Accelerators, GPU and Xeon Phi}
\label{gpu}
Accelerators are separate parts made to do one task and do it well. They are usually highly specialised and perform badly outside of the tasks they were designed for. The two accelerators explained here is graphical processing unit(GPU) and Intel Xeon Phi~\cite{cuda, jeffers2013intel}. GPUs have mostly been used for games and other graphics related problems however have become more popular for general computing the last 10 years due to their high number of cores~\cite{cuda}.  Intel Mic is much more recent with prototypes in 2010 and the first generation released in 2010~\cite{jeffers2013intel}. It is Intels response to the GPUs and has elements of both GPU and CPU~\cite{jeffers2013intel}. This section will focus on explaining some parts of the architecture of the GPUs and Xeon Phi. How to use the GPUs are talked about in more detail in section~\ref{cuda_programming_model}.\\
\\
GPUs are made to process images in a fast manner so that games and similar tasks runs smoothly~\cite{cuda}. Because GPUs are made for image processing they are GPUs have a large number of cores(~1000-2000)~\cite{cuda, nvtesla}. Each core is slower than a core in a CPU, however the number of cores makes up for it. The downside is that it's slower than a normal CPU for sequential tasks so the program has to take advantage of the extreme parallelism to get good speed~\cite{cuda_best_practice}. The memory of the GPU is separate from the computers main memory which means that the data needs to be transferred between the memories which increases the programs complexity~\cite{cuda}.\\
\\
The GPU cores work in a manner that is similar to SIMD~\cite{cuda}. The cores also have fewer optimizations than CPUs, this means that they can then devote a larger portion of the chip to the calculations as seen in figure~\ref{fig:gpu_scheme}. For instance NVIDIAs GPUs lack optimizations such as branch prediction and speculative execution talked about in section~\ref{optimizations}~\cite{cuda}. Their caches are also much smaller than the CPUs~\cite{cuda}. Another inheritance from the image processing is that they are much faster with single precision than with double precision data types~\cite{cuda, nvtesla}. A CPU drops slightly in performance when using double precision however much less than a GPU as seen in figure~\ref{fig:gpu_vs_cpu}. Single and double precision refers to how many bytes are used to store the decimal numbers, fewer bytes means less numerical precision.\\
\\
Intels answer to the increasing popularity of GPUs for general computing is Xeon Phi~\cite{jeffers2013intel}. It has elements of GPU architecture however is still more like a CPU. It has separate memory and many cores(50+)~\cite{jeffers2013intel}. It is MIMD which can be good for codes that diverge a lot, for ease of use however it has the possibly to use more SIMD like structures~\cite{jeffers2013intel}. It is possible to only use the Xeon Phis memory and avoid the memory transfers that way. The memory is relatively small compared to the main memory so if the program needs a lot of memory transfers are likely needed. The cores of the Xeon Phi lacks some of the normal optimizations, e.g. out of order execution~\cite{jeffers2013intel}.\\
\\
The choice between different accelerators depends largely on the algorithm, performance requirements, etc. For instance if the algorithm contains many points of divergence Xeon Phi is likely faster because Xeon Phi is MIMD while GPU is SIMT. That Xeon Phi is MIMD also means it can be applied to a broader set of problems~\cite{jeffers2013intel}. A one on one comparison of speed might not the correct choice either because of the different prices of the hardware and power consumption. Speed per power consumption or price can be more relevant.

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{knigths_corner_silicon_layout.png}
    \caption[Knights Corner silicon layout]{Knights Corner silicon layout, from~\cite{intel_knights_corner}}
    \label{fig:knights_corner_layout}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=11cm]{gpu_scheme.png}
    \caption[Layout of the Maxwell architecture]{Layout of the Maxwell architecture, from~\cite{nvidia_maxwell}}
    \label{fig:gpu_scheme}
\end{figure}

\clearpage
\subsection{Clusters}
\label{clusers}

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{Cluster.png}
    \caption{Basic overview of a cluster}
    \label{fig:cluster}
\end{figure}

Clusters are collections of computers that can work together in several ways and are so tightly connected that they can usually be viewed as a single system. They communicate over a shared network but have separate memory and processors. Storage is usually shared. A computer in a cluster is called a \emph{node}.~\cite{intro_hpc, introduction_hpc_hager}

%various programming models, shared memory and so on
%shared memory, cache coherrency and stuff

%mpi is commonly used for clusters~\cite{introduction_hpc_hager}

%top 500~\cite{TOP500}

%Few clusters used GPUs before 2009 but now mix of GPUs and CPUs are common among the top clusters. It was driven largely by demand for power saving while still giving high performance. The GPUs can do the heavy computation while other parts are used on CPU.~\cite{introduction_hpc_hager}
%Something about Intel MIC

%Flops is not everything, bad performance compare to theoretical maximum
%Stuck in network, hard drives, etc. Latency
%Very few programs scale to the largest clusters

%~\cite{introduction_hpc_hager}

\clearpage
\section{CUDA programming model}
\label{cuda_programming_model}
This section is about GPU programming in more detail using NVIDIAs CUDA (Compute Unified Device Architecture)~\cite{cuda}. CUDA extends C and C++ with some additional functionality that can be used to perform operations on NVIDAs GPUs. It provides a separate compiler to compile the GPU code called nvcc~\cite{cuda}. Different GPUs support different CUDA functions, each NVIDIA GPU has a value called computational capability~\cite{cuda, cuda_best_practice}. The higher the value the more features of CUDA are supported on that GPU. Some properties also vary between the different underlying architectures. This means that it is important to know what kind of capability the GPU to be used has so that the program does not need features that are not there. The subjects explained here might not apply to GPUs with compute capability below 3. There are GPUs made specifically for calculations rather than games, for instance the Tesla series~\cite{nvtesla}. This section is a summary of how CUDA works mostly from the CUDA programming guide~\cite{cuda} and the CUDA best practices guide~\cite{cuda_best_practice}.\\
\\
In CUDA the GPU is called \emph{device} and systems CPU and memory is the \emph{host}~\cite{cuda}. To perform operations on the device a type of function called \emph{kernel} is used. A kernel is works mostly as a normal C/C++ function with the addition of some specifiers to provided options to set number of threads and so on~\cite{cuda}. Example~\ref{ex:simple_kernel_device} is an example of a simple kernel definition and example~\ref{ex:simple_kernel_host} shows how it can be called from the host code. The kernel add each element of the arrays A and B storing it in C. Each thread performs on addition and knows why elements to use based on it's id. The call to the kernel is made by giving three arrays and the number of threads to be used as N. In this case N needs to be the length of the arrays.\\
\\
The \_\_global\_\_ keyword in front of the function declaration tells nvcc that it is a kernel. The $<<<N,M>>>$ specifier tells the compiler how many $N$ blocks and how many $M$ threads per block that the kernel should use~\cite{cuda}. A block is a group of threads and shares some memory and resources. The blocks can be executed in any order so they need to be completely independent from each other but variables can be shared among threads inside a block~\cite{cuda}. This allows the program to scale to different GPUs as shown in figure~\ref{fig:blocks_scaling}. The blocks are  organized in a one, two or three dimensional \emph{grid}. These can be accessed by each thread so it knows which grid it is in as illustrated in figure~\ref{fig:grid_2d}. This can be used to make it easier to assign the threads to the correct bit of the calculation. For instance using a two dimensional grid is good for matrices~\cite{cuda, cuda_best_practice}.

\stepcounter{ExamplesCounter}
\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}
\SetKwFor{For}{for}{}{}
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}
\SetKwFor{While}{while}{}{}
\SetKwRepeat{Repeat}{repeat}{until}
\AlgoDisplayBlockMarkers
\SetAlgoNoLine
\SetFuncSty{}
\SetArgSty{}

\SetAlgoRefName{\arabic{ExamplesCounter}} %\SetAlgoRefName{\arabic{chapter}.\arabic{ExamplesCounter}}
\SetAlgorithmName{Example}{List of Examples}

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{Vectors \textit{A, B, C} as C-style arrays}

\BlankLine \BlankLine

\SetKwFunction{KwFn}{\textcolor{kernel}{\_\_global\_\_} void \KwSty{Add}}

\Fn(){\KwFn{\textcolor{keyword}{float}* $A$, \textcolor{keyword}{float}* $B$, \textcolor{keyword}{float}* $C$}}{
\textcolor{keyword}{int} $i$ = threadIdx.$x$;\;
$C[i]$ = $A[i]$ $+$ $B[i]$;\;
}

\caption{Example of a declaration of a simple kernel}
\example{Example of a declaration of a simple kernel}
\label{ex:simple_kernel_device}
\end{algorithm}

\stepcounter{ExamplesCounter}
\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}%
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}%
\SetKwFor{For}{for}{}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
\SetKwFor{While}{while}{}{}%
\SetKwRepeat{Repeat}{repeat}{until}%
\AlgoDisplayBlockMarkers\SetAlgoNoLine%

\SetAlgoRefName{\arabic{ExamplesCounter}} %\SetAlgoRefName{\arabic{chapter}.\arabic{ExamplesCounter}}
\SetAlgorithmName{Example}{List of Examples}

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{Vectors \textit{A, B, C} of length $N$ as C-style arrays}

\BlankLine \BlankLine

\textbf{Add}\textcolor{kernel}{$<<<$1,$N>>>$}($A, B, C$);\;

\caption{Host code to call the kernel in example~\ref{ex:simple_kernel_device} with $N$ threads}
\example{Host code to call the kernel in example~\ref{ex:simple_kernel_device} with $N$ threads}
\label{ex:simple_kernel_host}
\end{algorithm}

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{grid_scale.png}
    \caption{Blocks execution depending on the number of streaming multiprocessors}
    \label{fig:blocks_scaling}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{2D_grid.png}
    \caption{2D grid of blocks}
    \label{fig:grid_2d}
\end{figure}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{SIMT.png}
    \caption{SIMT architecture}
    \label{fig:SIMT}
\end{figure}

The streaming multiprocessors(SM) is the hardware that handles the execution of these blocks and can execute hundreds of threads concurrently~\cite{cuda}. It uses \emph{SIMT}(Single-Instruction, Multiple-Thread) which is similar to SIMD described earlier in section~\ref{concurrency}~\cite{cuda}. SIMT works mostly as SIMD except that it can act as MIMD on collections of threads called \emph{warps}~\cite{cuda}. Each warp consists of 32 threads~\cite{cuda}. When a SM gets a block it is split into warps that are assigned to warp schedulers~\cite{cuda}. Each warp scheduler gives one instruction to a warp so full efficiency is achieved when all the 32 threads perform the same instruction. If there is any divergence it has to disable unrelated threads, so divergence can be costly. However different groups of warps are on different warps schedulers so can diverge without problem~\cite{cuda}.

\clearpage
\subsection{Device Memory}
The GPUs memory is physically on a different device separate from the computers main memory which means that they have separate memory spaces~\cite{cuda}. An object in one memory is not accessible in the other memory. The computers main memory is the \emph{host memory} and the GPUs memory is the \emph{device memory}~\cite{cuda}. Since they are separate data has to be transferred to the device memory and this is done by explicit calls to transfer sections of the hosts memory~\cite{cuda}.\\
\\
The GPU also have several different types of memory~\cite{cuda}. Correct usage can give increased speed~\cite{cuda, cuda_best_practice}.

\begin{itemize}
  \item Register memory is located on the multiprocessor and usually costs zero cycles to access. The multiprocessor splits the available registers over its threads so if there are many threads that uses many variables not all of them will fit in the register. This is why a program sometimes can be faster with lower number of threads.~\cite{cuda}
  \item Global memory is the main memory of the GPU and is accessible from all threads and blocks. However it is relatively slow to access.~\cite{cuda}
  \item Shared memory is shared inside a block and is faster than global memory. However it is limited in size.~\cite{cuda}
  \item Constant memory is small however it is read only which enables some optimizations. It is best used for small variables that all threads access.~\cite{cuda}
  \item Local memory is tied to the threads scope, however it still resides off-chip so it has the same access time as global memory.~\cite{cuda}
  \item Texture memory is read only and can be faster to access than global memory in some situations. This was more important in older GPUs when global memory was not cached.~\cite{plink_gpu, cuda}
  \item Read-only cache is available on GPUs based on Kepler architecture and uses the same cache as the texture memory. The data as to be read only each multiprocessor can have up to 48kb of space depending on GPU.~\cite{kepler_tuning_guide}
\end{itemize}

\newpage
\subsection{Streams}
\label{streams}
The kernel and transfer calls can be made on a \emph{stream}~\cite{cuda}. The easiest way to think of the stream is as queue. All the kernels and transfers on the same stream will be executed in the order their calls are made, however calls from different streams can be executed in any order~\cite{cuda, cuda_best_practice}. If the kernels use a small enough amount of resources this allows up to 32 kernels to be executed concurrently(i.e. at the same time) depending on GPU~\cite{cuda, cuda_best_practice}. The transfers on the other hand can overlap even with large kernels. The overlapping of the transfers is called \emph{synchronous transfers}. These transfers are executed on a stream and just as kernels gets executed after the previous kernels on the same stream is done. The advantage is that other streams can do calculations as normal while the transfer happens. This can hide the time for transfers completely in some situations. However the host memory that is to be transferred has to be \emph{pinned}. Pinned memory means that the operative system can not page that piece of the memory. \emph{Paging} means that the operative system stores a part of the memory in another area to save space in the memory, usually in the disk memory. Too much pinned memory can slow down the computer.~\cite{cuda, overlap_transfers_cuda}\\
\\

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{StreamsQueue.png}
    \caption{Three kernels, rectangle, circle, hexagon, executed on each of three streams, plain, dashed, dotted}
    \label{fig:streams_queue}
\end{figure}

\newpage
However some GPU architectures only have one combined queue for the streams~\cite{kepler_tuning_guide}. Because of this independent calls from different streams can block each other. If the queue first contains two kernels from one stream and then one kernel from another stream the GPU will execute the first kernel but not the second one until the first is complete since they are from the same stream. The GPU then does not see that the kernel on the second stream could be executed because it is blocked by the second kernel on the first stream. This affects how the kernel and transfer calls should be ordered~\cite{cuda, cuda_best_practice, kepler_tuning_guide}. Figure~\ref{fig:streams_queue} shows three kernels(rectangle, circle, hexagon) on three streams(plain, dashed, dotted). If all kernels on one stream are called, then the next streams kernels then they will queue as shown in figure~\ref{fig:streams_queue_stream_loop}. If they are performed by calling the first kernel on each stream they will queue as shown in figure~\ref{fig:streams_queue_kernel_loop}. In the former case the kernels from the first stream will block the others while in the later all the rectangle kernels could be executed concurrently~\cite{cuda, cuda_best_practice, kepler_tuning_guide}. Newer architectures from Kepler and onward has several queues so they do not have this problem~\cite{kepler_tuning_guide}.\\

\begin{figure}
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
        \centering
                \includegraphics[width=2.4cm]{StreamsQueueStreamLoop.png}
                \caption{Looped over stream, i.e. for each stream do all kernels on stream}
                \label{fig:streams_queue_stream_loop}
        \end{subfigure}%
       ~
        \begin{subfigure}[b]{0.48\textwidth}
        \centering
                \includegraphics[width=2.4cm]{StreamsQueueKernelLoop.png}
                \caption{Looped over kernel, i.e for each kernel do kernel on all streams}
                 \label{fig:streams_queue_kernel_loop}
        \end{subfigure}
        \caption{Queues of the kernels in figure~\ref{fig:streams_queue} called in two different ways, looped first over stream or kernel. In \emph{a} the kernels are queued stream by stream while \emph{b} is queued kernel by kernel.}
        \label{fig:streams_queue_loop}
\end{figure}

The optimal ordering also depends on the number of copy engines~\cite{overlap_transfers_cuda, cuda_fortran_overlap}. The copy engines are responsible for performing the transfers to and from the GPU. Most modern GPUs have one for each direction(i.e. to device, from device) while some of the older GPUs only have one copy engine~\cite{overlap_transfers_cuda, cuda_fortran_overlap}. This can affect how the calls should be ordered and for GPUs with only one copy engine using the wrong order can make the performance worse than without using asynchronous transfers~\cite{overlap_transfers_cuda, cuda_fortran_overlap}.\\
\\
There is an example of this in~\cite{cuda_fortran_overlap} which illustrates the problem. They have four versions of the same code, a sequential transfer version and three asynchronous transfer versions. Two different GPUs were used, one had one copy engine the other had two. In the asynchronous the data is split over four streams coloured differently in the figure~\ref{fig:ascynchronous}. Version 1 initiates the calls by looping over the streams one by one and doing the transfer and kernel calls on that stream before moving on to the next~\cite{cuda_fortran_overlap}. Version 2 makes all the host to device transfer calls for all streams first, then the kernels and then the device to host transfer call~\cite{cuda_fortran_overlap}. Version 3 is the same as version 2 but with a dummy even after each kernel~\cite{cuda_fortran_overlap}. The figure~\ref{fig:ascynchronous} shows how the transfers and kernels are executed on the GPU.

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{streams_seq.jpg}
    \caption{Sequential versions}
    \label{fig:sequential}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{streams_async.jpg}
    \caption[Asynchronous versions]{Asynchronous versions. D2H=device to host transfer. H2D=host to device transfer. Each colour represents a stream.}
    \label{fig:ascynchronous}
\end{figure}

\clearpage
\subsection{Efficient CUDA}
\label{efficent_cuda}
One of the main criticism against GPUs for general computing purposes is that it is hard to get good performance because it requires good knowledge about details of the GPU architecture, especially the memory architecture. This section is a summary of suggestions that can be good to consider for CUDA programs from the CUDA programming guide~\cite{cuda}, CUDA best practices guide~\cite{cuda_best_practice} and a master thesis about GPU in GWAS~\cite{plink_gpu}.
\\
\begin{description}
  \item[Maximize parallelism] \hfill \\
  Structure the program and the algorithm in such a way that it is as parallel as possible and overlap the serial parts on CPU with calculations on the GPU.~\cite{plink_gpu, cuda}
  \item[Minimize transfers between host and device] \hfill \\
  Moving data between host and device is expensive and should be avoided if possible. It can be better to run serial parts on the GPU rather than moving the data to the host to do the calculation on the CPU. The bandwidth between host and device is one of the large performance bottlenecks. This can be a problem when the data is to large to fit in the relatively small GPU dram.~\cite{cuda, cuda_best_practice}
  \item[Find the optimal number of blocks and threads] \hfill \\
  There are many things affected by the number of blocks and threads so they should be considered carefully. It is a good idea to parametrize them so that they can be changed for future hardware and varied for optimization. NVIDIA has an occupancy calculator which can be helpful in determining the optimal numbers, however high occupancy does not mean high performance.~\cite{cuda, cuda_best_practice}\\
  \\
  The number of blocks should be larger than the number of multiprocessors so that all multiprocessors have at least one block to execute. Having two blocks or more per multiprocessor can be good so that there are blocks that aren't waiting for a \_\_syncthreads() that can be executed. However this is not always possible due to shared memory usage and similar.~\cite{cuda_best_practice}\\
  \\  
  The number of threads per block should be a multiplier of 32 but minimum 64. It's also important to remember that multiple concurrent blocks can reside on the same multiprocessor. Too large number of threads in a block and parts of the multiprocessor might be idle since there aren't a block small enough to use those threads. Between 128 and 256 threads is a good place to start.~\cite{cuda_best_practice}
  \item[Use streams and asynchronous transfers] \hfill \\
  By using streams it is possible to overlap memory transfers with calculations as mentioned before. This means that the data for the next batch can be transferred while the current batch is calculated and when it is done it can start calculating on the next batch directly after the current one is done. This can hide the time for transfers completely in some situations. Depending on the time the transfers take versus the time the calculations take this can give significant speedup.~\cite{cuda, overlap_transfers_cuda, kepler_tuning_guide}
  \item[Use the correct memory type and caches] \hfill \\
  Correct use of caches and memory is important for both CPU~\cite{drepper2007cpumemory} and GPU. However it is more complicated on GPU since the caches are smaller and there are several types of memory as mentioned before~\cite{cuda, cuda_best_practice}.
  \item[Avoid divergence] \hfill \\
  Each thread in a warp executes the same instruction at the same time so if some of threads diverge the rest will be ideal until they are at the same instruction again. This means it is important to use control structures such as if statements carefully to prevent threads from idling.~\cite{cuda, cuda_best_practice}
  \item[Avoid memory bank conflicts when using shared memory] \hfill \\
  Shared memory is divided into equally-sized memory modules called banks that can be accessed at the same time for higher bandwidth. Bank conflicts occur when separate threads access the same bank. On some GPUs it is fine if all threads access the same bank. Bank conflicts are split into as many conflict-free requests as needed.~\cite{cuda, cuda_best_practice}
  \item[Use existing libraries] \hfill \\
  Instead of writing everything from scratch it is usually a good idea to use already existing libraries. Especially when performance is important and most task are non trivial on GPUs so using a already optimized library is a good idea. Some of the most popular libraries for CUDA are:
  \begin{itemize}
    \item CUBLAS: BLAS implementation for CUDA. BLAS and LAPACK is a standard for a library that provides highly optimized functions for linear algebra.~\cite{cublas}
    \item CULAtools: BLAS and LAPACK implementation for CUDA for both dense and sparse matrices~\cite{culatools}
    \item MAGMA: BLAS and LAPACK implementation among other things that can distribute the work on both CPU and GPU~\cite{magma_2010}
    \item Thrust: Template based library that tries to emulate C++ standard library~\cite{thrust_gpu}
  \end{itemize}
  \item[Avoid slow instructions] \hfill \\
  There are some instructions that can be slow and should be avoided if possible, for instance type conversion, integer division and modulo. If a function is called with a floating point number that might be used as a double and require a conversion. By putting a $f$ at the end of the number it is told to be single precision float, for instance 0.5f. In some cases it is possible to use bitwise operations instead which is faster.~\cite{cuda, cuda_best_practice}
  \item[Restricted pointers can give increased performance] \hfill \\
  Aliasing can be a problem as mentioned earlier. By using the \_\_restrict\_\_ keyword on pointers the compiler can be told that no aliasing will occur, however it is up to the programmer to make sure that is the case or there might be unexpected results. Not using aliasing reduces the number of memory accesses the CPU needs to make. However it increases register pressure so it can have a negative effect on performance.~\cite{cuda}
  \item[Use fast math functions if precision isn't needed] \hfill \\
  There are three versions of the math functions. The double precision version is func() while the single precision function is funcf(). There is third faster but less accurate version \_funcf(). The option -use\_fast\_math makes the compiler change all the funcf() to \_funcf().~\cite{cuda_best_practice}
  \item[Instruction level parallelism can increase speed]\hfill \\
  Just as for CPUs the GPUs can take advantage of instruction level parallelism. By unrolling loops this can give 2x the speed relatively simple~\cite{volkov2011unrolling}.
\end{description}

\clearpage
\section{Software Design}
How to design software so it can be maintained over time has been a problem for a long time~\cite{cleancode2008, design_patterns}. Object oriented languages such as C and later Java and C++ arose because of problems with maintaining software~\cite{cleancode2008}.Badly organized and written code will cost productivity in the future when bugs and other problems stack up because of earlier mistakes~\cite{cleancode2008, design_patterns}. Correcting those bugs are likely to be time consuming because it can be hard to find where they originated from. All code gather problems overtime, however a well designed system will degenerate significantly slower than one that was designed carelessly~\cite{cleancode2008}. The code will also be read by ourselves and others later which means readability is important if a future reader is to understand the code and find the parts they are interested in~\cite{cleancode2008}.\\
\\
This is also important in science where others might wish to use the tools, repeat the experiments or build upon it. A study~\cite{comp_repro_2013} found that the repeatability in computer science is low. Their goal was to get the code from articles in computer science and compile it, meaning that they did not test if the code actually ran properly or even at all. Their results are shown in~\ref{fig:comp_repro}. Of 515 articles they received the code from 231, of those 102 compiled~\cite{comp_repro_2013}.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{comp_repro.png}
    \caption[Repeatability in computer science]{Repeatability in computer science,from~\cite{comp_repro_2013}} %TODO text for fig 
    %TODO fix all fig texts
    \label{fig:comp_repro}
\end{figure}

\newpage
There are ways to design the program and code in such a way that it is easier to read and maintain. Most of the concepts described here goes under the development technique called agile development~\cite{cleancode2008}. SOLID is an acronym for five principles for object oriented programming and design~\cite{cleancode2008}. When used together they are intended to make programs that are easy to maintain and extend~\cite{cleancode2008}. As already mentioned readability is important. Correct names will make the code explain itself without other documentation(e.g. comments), the code itself is the documentation~\cite{cleancode2008}.

\begin{table}[h]
\centering
\begin{tabular}{ l l p{2in}}
  \hline
  Initial & Principal & Concept\\
  \hline
  S & Single responsibility principle & A class should only have a single responsibility \\
  O & Open/closed principle & A class should be open for extension but closed for modification \\
  L & Liskov substitution principle & If S is a subtype of T, then objects of type T may be replaced by objects of type S without without altering any of the desirable properties of the program(e.g. work performed)\\
  I & Interface segregation principle & Use several smaller and more specific interfaces instead of one large \\
  D & Dependency inversion principle & Depend on abstractions(e.g. interfaces) not details \\
  \hline  
\end{tabular}
\caption[The five SOLID principles]{The five SOLID principles, from~\cite{cleancode2008}}
\label{table:solid_table}
\end{table}

Dependency injection is one way to implement dependency inversion principle~\cite{cleancode2008}. Injection is passing the dependency to the dependent object. This is used instead of allowing the dependent object to construct or find the dependency~\cite{cleancode2008}. \emph{Factories} are used to allow for dependency injection when the class has some function that creates new instances of the class~\cite{cleancode2008}. Since the class creates them they can not be directly replace so instead the construction is delegated to a factory and the factory class can be used with dependency injection~\cite{cleancode2008}. There are also other uses for factories such as eliminating code repetition from construction of classes that are constructed from a chain of classes(e.g. a house consists of windows, doors, walls, etc) or when there are several classes that inherit from the same base class~\cite{cleancode2008,design_patterns}.

\newpage
\subsection{Unit Tests and Mocks}
\emph{Unit testing} involves testing the program in small units in isolation~\cite{cleancode2008}. Testing in isolation means that the test should only depend on the part of the program that is tested~\cite{cleancode2008}. If a part of the program is not working properly only its related tests should fail, not other tests for code that depends on it but otherwise works properly~\cite{cleancode2008}. This makes it easier to find errors when they do occur since the tests will pinpoint the unit which does not work. \emph{Integration testing} is used to test several units together to find possible errors that occur when the smaller units are combined.\\ %TODO cite integration tests
\\
It can be easy to denote test code as less important than the main code but they should be treated as equally important~\cite{cleancode2008}. Tests should also not be an inconvenience to use so they should be easy to run, take reasonable amount of time to complete and not require any outside interpretation whether they failed or not~\cite{cleancode2008}.\\
\\
\emph{Mocking} is to replace a real object with a fake object called a \emph{mock} that for the code is indistinguishable from the real object~\cite{cleancode2008}. This allows one to create situations and test with more control and without depending on the real objects code~\cite{cleancode2008}. The second is important for unit tests since it enables one to test units that normally depend on others in isolation~\cite{cleancode2008}. In the first case it is useful in situations such as when one wants to test a class handling of a rare failure~\cite{cleancode2008}. Such failures can be hard and time consuming to induce. It is then easier to use a mock object that behaves like the failure has occurred. However mocking requires that the code for the class doesn't create the object itself since there is no way to replace the object with the mock~\cite{cleancode2008}. This is one of the reasons why dependency injection should be use~\cite{cleancode2008}.

\subsection{Design Patterns}
\label{design_pattern}
A \emph{design pattern} in software design is a reusable general solution to a common recurring problem in a given context~\cite{design_patterns}. It is templates and structures to solve the problem, however it is not code~\cite{design_patterns}. However they are partially dependent on the programming language since different languages have different features and limitations~\cite{design_patterns}.\\
\\
\emph{Consumer producer} is a concurrency pattern for when there are a number of consumers/workers and producers~\cite{cleancode2008}. They share a common queue for products. The producer generates some product and put it into the queue, while the the consumers consume the products. The problem is that the producers should not add to a non empty slot and that each product should only be consumed once~\cite{cleancode2008, design_patterns}.\\
\\
An \emph{wrapper} is an interface used to \emph{wrap} another interface, it is also called \emph{adapter} pattern~\cite{cleancode2008, design_patterns}. The wrapper does not perform the work, it delegates it to the other interface~\cite{cleancode2008}. This is useful for instance when a 3rd party interface is not in the same style as the program or has other problems but otherwise suits the needs~\cite{cleancode2008}.

\subsection{Version Control Software}
Backups for data, code, text and so on is important if something happens. One way to backup text and code projects is to use a version control software and is usually an important tool in developing~\cite{git_book}. For this project Git was used and the source code can be found at \url{https://github.com/Berjiz/CuEira}. Version control software allows several developers to share a common \emph{repository} which allows them to work on different parts at the same time, it could even be in the same file~\cite{git_book}. However changes at the same place causes \emph{merge conflicts} which usually needs to be solved manually~\cite{git_book}. A version control software keeps track of all the changes made so if a part later turns out to be wrong that part can reverted to an older correct version~\cite{git_book}. Version control software also have many other features~\cite{git_book}.

\clearpage
\section{Performance Measures}
An important part in creating fast and efficient programs is to know how fast the program is under certain conditions and which parts of the program are slow~\cite{introduction_hpc_hager}. For instance the speed could suddenly drop when too many threads, there might be a bottleneck in the communication, and so on.\\
\\
There are two ways to measure how long a program takes to execute~\cite{introduction_hpc_hager}. Wall clock time is how long real life time the program took~\cite{introduction_hpc_hager}. The other is to measure the number of processor cycles spent~\cite{introduction_hpc_hager}. A parallel program will have shorter execution time than it is serial version however it will likely have spent more processor cycles due to overhead from communication and initialisation of the threads. These two measures are useful for different kinds of comparisons. Wall clock time is better for overall performance while number of cycles is useful for comparing different algorithms~\cite{introduction_hpc_hager, cuda_best_practice}.\\
\\
Speed up is a measure of how much faster then program is with a certain number of threads compared to the serial version. It's defined as~\cite{introduction_hpc_hager}

\begin{equation}\label{eq:speedup}
S(p)=\frac{T(1)}{T(p)}
\end{equation}

Where $T(1)$ is execution time of serial program and $T(p)$ is execution time of parallel program with p threads. Linear speedup is when S(p)=p~\cite{introduction_hpc_hager}.\\
\\
Efficiency reflects how efficient the program is using p threads. Linear speed has efficiency 1. It's defined as~\cite{introduction_hpc_hager}

\begin{equation}\label{eq:efficiency}
E(p)=\frac{S(p)}{p}=\frac{T(1)}{pT(p)}
\end{equation}

Strong scaling refers to how the program handles a fixed problem size and increased number of processors~\cite{introduction_hpc_hager}. A program with strong scaling has linear speedup~\cite{introduction_hpc_hager}. Weak scaling refers to the execution time of the program when there is a fixed problem size \emph{per processor} and the number of processors is increased~\cite{introduction_hpc_hager, cuda_best_practice}.\\
\\
It can be a good idea to plot these measures while varying p, this can show when a bottleneck occurs. Looking at the measures at node level can also be useful to get an idea of how increased number of nodes and therefore increased communication over the network affects the performance.

\newpage
\subsection{Amdahl's Law and Gustafson's Law}
Amdahl's Law is used to find the expected speedup of a system when parts of it are made concurrent~\cite{2010_reevaluating_amdahl}. Simply it says that as the number of processors increases the parts that aren't parallel will start taking up more and more of the wall clock time and that the speedup for adding more processors will decrease as more and more processors are added and more time is spent relatively on the non parallel parts~\cite{introduction_hpc_hager, 2010_reevaluating_amdahl}. It's closely related to strong scaling~\cite{cuda_best_practice,2010_reevaluating_amdahl}.\\
\\
It says that the expected speedup with F fraction of the code parallel and p threads is~\cite{introduction_hpc_hager} 

\begin{equation}\label{eq:amdahl}
S(p)=\frac{1}{(1-F)+\frac{F}{p}(1-F)}
\end{equation}

As the number of threads grow towards infinity $S(p)$ converges on $\frac{1}{1-F}$. If we have 90\% of a code parallel then even with infinite number of threads we won't get a better speedup than ten~\cite{2010_reevaluating_amdahl}. There are limitation to Amdahl's Law since it makes a couple of assumptions~\cite{2010_reevaluating_amdahl, gustafson1988reevaluating}.

\begin{itemize}
  \item The number of executing threads remain constant over the course of the program.
  \item The parallel portion has perfect speedup. Often not true due to shared resources,eg caches, memory bandwidth, and shared data.
  \item The parallel portion has infinite scaling, not true due to similar limits as above. More threads will not increase performance after a while or might even decrease it.
  \item There is no overhead for creation and destruction of threads.
  \item The length of the serial portion is independent of the number of threads. Often the serial work is to divided the work to the threads, this work will obviously increase as the number of threads go up. More threads can also lead to more communication overhead.
  \item The serial portion can't be overlapped by the parallel parts. For instance with producer consumer type pattern the consumer could be strictly serial but the time it takes could be overlapped by the parallel producers.
\end{itemize}

This means it is most accurate with programs that are of the fork-join type, e.g. both serial and parallel parts~\cite{gustafson1988reevaluating}.\\
\\
Gustafson's Law is closely related to Amdahl's Law and can in some ways be more accurate than Amdahl's Law~\cite{gustafson1988reevaluating}. Gustafson's Law makes similar assumptions as Amdahl's Law however it also makes two additional statements. It states that problems tends to expand when provided with more computational power, e.g. increased precision by reducing grid size for simulations, higher frame rate for graphics and so on~\cite{gustafson1988reevaluating, introduction_hpc_hager}. The second is that the parallel portion of the program tends to expand faster than the serial part, e.g. for matrix multiplication the initialisation scales linearly with the matrix size while the multiplication itself scales as $O(n^3)$~\cite{gustafson1988reevaluating, introduction_hpc_hager}. The former means that it is closely related to weak scaling~\cite{introduction_hpc_hager}. So in a way it says that the execution time remains constant rather than the amount of data. More precise it says that the expected speedup with p threads and F fraction of the code that is parallel is~\cite{gustafson1988reevaluating, cuda_best_practice, introduction_hpc_hager}

\begin{equation}\label{eq:gustafson}
S(p)=p+(1-F)(1-p)
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{AmdahlsLaw.png}
    \caption[Illustration of Amdahl's Law]{Illustration of Amdahl's Law. Wikipedia Commons}
    \label{fig:AmdahlsLaw}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=13cm]{GustafsonsLaw.png}
    \caption{Illustration of Gustafson's Law}
    \label{fig:GustafsonsLaw}
\end{figure}

\clearpage
\subsection{Profilers}
There are applications called profilers that are made to assess the programs performance and resource consumption~\cite{introduction_hpc_hager, cuda_best_practice}. They calculate some of the measures mentioned earlier and they also check hardware usage and how much time the program spends at various parts of the program~\cite{introduction_hpc_hager, cuda_best_practice}. This is very useful for finding bottlenecks and other problems in the program. It does not matter if the algorithm is super fast if all the data is stuck in network transfers. The profilers can be hardware dependent so the manufactures usually provided them for their products~\cite{introduction_hpc_hager, cuda_best_practice}.\\
\\
Nsight~\cite{nvidia_nsight} is a combined profiler and integrated development environment(IDE) based on either Visual Studio or Eclipse. For this thesis Nsight Eclipse edition was used. It works as the usual Eclipse but has added functionality for CUDA and CUDA profiling~\cite{nvidia_nsight}.

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{nsight_profiler.png}
    \caption[Screenshot of Nsight Eclipse Editions profiling section]{Screenshot of Nsight Eclipse Editions profiling section, from~\cite{nvidia_nsight}}
    \label{fig:nsight_profiler}
\end{figure}

\clearpage
\section{Algorithms}
\label{algorithm_background}
There are many algorithms and programs proposed for searching for interaction and most have focused on gene-gene interaction~\cite{gene_enviroment_2013}. One of the challenges of gene-environment interaction is that environmental factors can be of any variable type(binary, continuous, categorical) which creates problems in various ways~\cite{gene_enviroment_2013}. Gene-gene interaction tools can sometimes be used to find gene-environment interaction, however they usually require the variables to be binary or have other problems since they were not designed for environmental interaction~\cite{gene_enviroment_2013}.\\
\\
Both clusters using only CPUs~\cite{biforce} and CPUs together with GPUs~\cite{gwis,gboost,gmdr_gpu,cuda_lr,genie_2012,plink_gpu, snpsyn_gpu_mic} have been used for gene-gene interaction. SNPsyn also used Xeon Phi, they got 1.5 to 2 times speedup with a K20 GPU compared to the Xeon Phi~\cite{snpsyn_gpu_mic}. The studies got high gains from implementing their algorithms on GPU compared to their CPU versions~\cite{gwis,gboost,gmdr_gpu,cuda_lr,genie_2012,plink_gpu}. However most studies CPU versions were not parallel so it is likely that the gains would have been smaller if they had compared to an optimized and parallel CPU version. One study made a comparison of their algorithm between using a CPU cluster and using a GPU~\cite{jiang_accelerating}. They found that 16 CPU nodes had the same performance as a single GTX 280 card, however the study is from 2009~\cite{jiang_accelerating}.\\
\\
The methods can be roughly classified into four categories, exhaustive, stochastic, machine learning/data mining and stepwise~\cite{fast_high_order_cluster}.\\
\\
\emph{Exhaustive search} is the most direct approach, it compares all combinations of the SNPs in the dataset. Exhaustive search methods will not miss a significant combination because it didn't consider that specific combination. However it also means that they can be slow since they will spend time on combinations other methods would skip completely. Multifactor-Dimensionality Reduction(MDR)~\cite{mdr_2001} and BOOST~\cite{boost_gene_gene} are two examples of this type of algorithm.\\
\\
\emph{Stochastic} methods uses random sampling to iterate through the data. BEAM~\cite{beam_2007} is one example and it uses Markov Chain Monte Carlo(MCMC) method.\\
\\
\emph{Data Mining} and \emph{Machine Learning} are methods that try to learn patterns from data and tries to generalize it. MDR~\cite{mdr_2001} is a type of data mining method and is among the most common methods used in GWAS. See section~\ref{data_machine_learning} for more details.\\
\\
\emph{Stepwise} methods uses a filtering stage and a search stage. At the filtering stage uninteresting combinations are filtered out by using some exhaustive method. The other SNPs are the examined more carefully in the search stage. BOOST~\cite{boost_gene_gene} is an example which uses succinct data structures and a likelihood ratio test to filter the data before applying log-linear models.

\newpage
\subsection{Logistic Regression}
\label{logistic_regression}
One way to model the contingency tables is by using \emph{logistic regression}. Logistic regression is a type of linear regression model for classification that models a latent probability for the outcomes~\cite{agresti_categorical}. The outcomes are binary, however the method can be extended to multiple outcomes. In this work we will only consider them as binary. Logistic regression transforms the probability by using the \emph{logit} transformation~\cite{agresti_categorical}. The logit transformation with probability $\pi$ is~\cite{agresti_categorical}

\begin{equation}\label{eq:logit}
\log(\frac{\pi}{1-\pi})
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{logit.png}
    \caption[Graph of the logit transformation]{Graph of the logit transformation. Wikipedia Commons}
    \label{fig:logit}
\end{figure}

The probability with a set of predictor variables \emph{X} is $\pi(X)=P(Y=1)$~\cite{agresti_categorical}. The linear regression model with n predictors $X=(x_1,x_2,....,x_n)$, coefficients $\beta=(\beta_1, \beta_2,....,\beta_n)$ and by using the logit transformation is then~\cite{agresti_categorical}
\begin{equation}\label{eq:logit_lr}
logit[\pi(X)]=\alpha+\beta X %TODO logit function fix
\end{equation}

By moving the logit to the right side of the equation we get the model of the probability~\cite{agresti_categorical}
\begin{equation}
\pi(X)=\frac{e^{\alpha+\beta X}}{1+e^{\alpha+\beta X}}
\end{equation}

The logit, equation~\ref{eq:logit}, also happens to be the log of the odds(equation~\ref{eq:odds})~\cite{agresti_categorical}. By exponentiating both sides of equation~\ref{eq:logit_lr} it shows that the odds is~\cite{agresti_categorical}

\begin{equation}
e^{logit[\pi(X)]}=\frac{\pi(X)}{1-\pi(X)}=e^{\alpha+\beta X}=\Omega
\end{equation}

This means that $\exp{\beta}$ is the odds ratio since the odds increase by $\exp{\beta}$ for each unit increase of $X$~\cite{agresti_categorical}. It can also been seen by taking the ratio of the odds using equation~\ref{eq:odds_ratio} and  with $X=x+1$ and $X=x$

\begin{equation}
\theta=\frac{e^{\alpha+\beta (x+1)}}{e^{\alpha+\beta x}}=e^{\alpha+\beta (x+1)-\alpha-\beta x}=e^{\beta}
\end{equation}

Finding the $\beta$ coefficients are done in a similar way as with other linear regression models since they all are generalized linear models~\cite{agresti_categorical}. It's usually done using  maximum likelihood(ML), via Newtons method~\cite{agresti_categorical, uvehag_master_thesis}. It's an iterative method which means it can be relatively slow compared to non iterative methods. The pseudo code for the algorithm using Newtons method can be found in algorithm~\ref{alg:lr}~\cite{uvehag_master_thesis}. Line 13 is sometimes inside the loop, however since it is not needed for the actual iteration calculating it for every loop wastes time.

\begin{algorithm}
\DontPrintSemicolon
\SetKwInOut{data}{Data}
\BlankLine \BlankLine
\SetKwInOut{note}{Note}
\LinesNumbered

\data{$N$ number of data points\\
$M$ number of variables, excluding the intercept\\
$\boldsymbol{X}$ is a $N \times M$ matrix that contains the variables\\
$\boldsymbol{Y}$ is the outcomes with length $N$\\
$\beta$ has length $M$ and contains the $\beta$ coefficients\\
$p$ has length $N$ and contains the probability for the outcome for \; each individual\\
$s$ has length $M$ and contains the scores of the coefficients\\
$\boldsymbol{J}$ is a $M \times M$ matrix called the Jacobian, also called \; information matrix}

\note{$*$ is element by element multiplication}

\BlankLine \BlankLine
$\boldsymbol{X}\longleftarrow
\begin{pmatrix}
\begin{matrix}
  1\\
  \vdots\\
  1
\end{matrix} & \boldsymbol{X}
\end{pmatrix}
$\;

\BlankLine \BlankLine
$\beta\longleftarrow
\begin{pmatrix}
  0\\
  \beta
\end{pmatrix}
$\;

\BlankLine \BlankLine
$iter\longleftarrow 0$\;
$diff\longleftarrow 1$\;

\BlankLine \BlankLine
\While{$iter < max\_iter \; \boldsymbol{and} \; diff > threshold$}{
  $\beta_{old}\longleftarrow\beta$\;
  $p\longleftarrow \frac{e^{\boldsymbol{X} \cdot \beta}}{1+e^{\boldsymbol{X} \cdot \beta}}$\;
  $s\longleftarrow \boldsymbol{X^T}\cdot (\boldsymbol{Y}-p)$\;
  $\boldsymbol{J}\longleftarrow (\boldsymbol{X^T}\cdot (p*(1-p)))\cdot \boldsymbol{X}$\;
  $\beta\longleftarrow \beta_{old}+\boldsymbol{J}^{-1} \cdot s$\;
  $diff\longleftarrow \sum |\beta-\beta_{old}|$\;
  $iter\longleftarrow iter+1$\;
}
\BlankLine \BlankLine
$log \; likelihood\longleftarrow \sum (\boldsymbol{Y}*ln p+(1-\boldsymbol{Y})*ln(1-p))$\;

\caption{Logistic regression using maximum likelihood and Newtons method}
\algorithmslist{Logistic regression using maximum likelihood and Newtons method}
\label{alg:lr}
\end{algorithm}

\newpage
\newpage
\subsubsection{Matrix Inverse and Matrix Decomposition}
\label{matrix_inverse}
The inverse of the \emph{information matrix}, \textbf{J}, on line 10 in algorithm~\ref{alg:lr} is generally not possible to do with normal matrix inversion because it is not defined for a general matrix~\cite{albert1972regression}. However the \emph{pseudoinverse} is defined for a general matrix, it is denoted as $A^+$ for the matrix $A$~\cite{albert1972regression, golub1970singular}. There are several types of pseudoinverse, one of the more common is the \emph{Moore-Penrose pseudoinverse} where $A^+$ is the matrix that satisfies equations~\ref{eq:pseudo_1} to~\ref{eq:pseudo_4}~\cite{albert1972regression}.

\begin{equation}\label{eq:pseudo_1}
AA^+A=A
\end{equation}

\begin{equation}\label{eq:pseudo_2}
A^+AA^+=A^+
\end{equation}

\begin{equation}\label{eq:pseudo_3}
(AA^+)^T=AA^+
\end{equation}

\begin{equation}\label{eq:pseudo_4}
(A^+A)^T=A^+A
\end{equation}

\emph{Matrix decomposition}, also called \emph{matrix factorisation}, are methods to factorize a matrix into products of matrices~\cite{golub1970singular}. Some of the decomposition methods can be used to find the pseudoinverse. \emph{Singular value decomposition}(SVD) is one of them and works for general matrices~\cite{golub1970singular}. It's factorization of a  $n \times m$ matrix $A$ is shown in equation~\ref{eq:svd}~\cite{golub1970singular}.

\begin{equation}\label{eq:svd}
A=U \Sigma V^T
\end{equation}

Where $\Sigma$ is a diagonal matrix with non negative numbers, its diagonal values are the \emph{singular values} of $A$~\cite{golub1970singular}. Using SVD the pseudoinverse is shown in equation~\ref{eq:pseudo_svd}~\cite{golub1970singular}. $\Sigma^+$ is the pseudoinverse of $\Sigma$. This is done by inverting each non zero element of the diagonal and transposing the matrix~\cite{golub1970singular}.

\begin{equation}\label{eq:pseudo_svd}
A^+=V \Sigma^+ U^T
\end{equation}

\newpage
\subsection{Data Mining and Machine Learning Approaches}
\label{data_machine_learning}
Approaches based on Data Mining and Machine Learning have been a popular choice for GWAS. Random Forest(RF)~\cite{random_forest} and MDR~\cite{mdr_2001} are among the most common ones~\cite{gene_enviroment_2013,cordell_detect_review}. There are other methods as well such as clustering approaches~\cite{fast_high_order_cluster}. Most of them are used for filtering the data for possible interactions and using another method after the filtering~\cite{gene_enviroment_2013,cordell_detect_review}.\\
\\
One of their larger advantage is that they are usually non-parametric and designed with high dimensional data in mind~\cite{cordell_detect_review}. However they are prone to overfitting and the usual way to try to prevent that is to use cross validation or permutation tests~\cite{cordell_detect_review}. It means that even if the method itself is fast the algorithm is repeated so many times that the whole algorithm can be slow in the end~\cite{cordell_detect_review}.

\subsubsection{Random Forest}
RF is an ensemble learning method~\cite{random_forest}. Ensemble methods combine multiple models to improve performance. RF takes randomized samples of the data and builds decision trees on each of them~\cite{random_forest}. These trees are then combined to form the classifier. Usually hundreds or thousands of trees are used depending on the problem~\cite{random_forest}. One of the most popular variants of Random Forest for GWAS is Random Jungle~\cite{random_jungle}.\\
\\
It has been shown in high dimensional data that RF tends to only rank interacting factors high if they have strong marginal effects~\cite{winham_rf_2012}. Also the ranking of the variables does not indicate which factor it is interacting with either since it is based on the joint distributions~\cite{gene_enviroment_2013}. How to incorporate the environmental factors in RF is also not obvious and using variables with very different scales can bias the results~\cite{gene_enviroment_2013}.

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{Decision_Tree.png}
    \caption{An example of a decision tree}
    \label{fig:DecisionTree}
\end{figure}

\subsubsection{Multifactor-Dimensionality Reduction}
MDR is a method that reduces the number of dimensions(i.e. variable) by combining several dimensions into one~\cite{mdr_2001}. In GWAS it combines the variables that are suspected to interact. This new variable is then compared against the outcome~\cite{mdr_2001}. If the new variables predictability of the outcome is high enough then the variables that were combined are considered to interact~\cite{mdr_2001}. This process is usually repeated on all pair combinations of variables~\cite{cordell_detect_review, mdr_2001}.\\
\\
The reduction from $n$ dimensions is done by calculating the ratio of cases versus controls for each combination of the possible values of the variables~\cite{mdr_2001}. If the ratio is above a certain threshold all the members of that group get the value 1 for the new dimension, otherwise 0~\cite{mdr_2001}. Accuracy of the model is measured by using cross validation and permutation tests, in other words it reshuffles the data randomly and recalculates the model a large number of times to get an estimate of the models certainty~\cite{cordell_detect_review, mdr_2001}. Because of that MDR can be slow~\cite{cordell_detect_review, mdr_2001}. However it is still usually faster than exhaustive search with regression methods~\cite{cordell_detect_review, mdr_2001}.\\
\\
MDR can been used for gene-environment interaction but requires modifications since MDR can only handle binary variables~\cite{gene_enviroment_2013}. There are extensions that can use continues variables, however these are regression based so these will be slower than regular MDR~\cite{gene_enviroment_2013}.\\
\\
A simple example of MDR using exclusive or (XOR). XOR is a logical operator that is true if one and only one of its two variables is true. We have 4 possible combinations and an occurrence for each of them, see table~\ref{table:xor_table}. The combination (1,0) and (0,1) both have one case with outcome 1 so MDR will classify them as 1 in the new variable Z, the other two combinations have outcome 0 so will be classified with Z=0, see table~\ref{table:xor_mdr_table}. From here it is easy to make a predictor from Z to the outcome Y by comparing the values.

\begin{table}[h]
\centering
\begin{tabular}{ | c | c | c | }
  \hline
  \textbf{Y} & $\mathbf{X_1}$ & $\mathbf{X_2}$ \\
  \hline
  1 & 1 & 0 \\
  \hline 
  1 & 0 & 1 \\
  \hline
  0 & 0 & 0 \\
  \hline
  0 & 1 & 1 \\
  \hline
\end{tabular}
\caption{XOR table with outcome $\mathbf{Y}$ and variables $\mathbf{X_1}$ and $\mathbf{X_2}$.}
\label{table:xor_table}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ | c | c | }
  \hline
  \textbf{Y} & \textbf{Z} \\
  \hline
  1 & 1 \\
  \hline
  1 & 1 \\
  \hline
  0 & 0 \\
  \hline
  0 & 0 \\
  \hline
\end{tabular}
\caption{XOR table with $\mathbf{X_1}$ and $\mathbf{X_2}$ combined into $\mathbf{Z}$ using MDR.}
\label{table:xor_mdr_table}
\end{table}

\clearpage
\chapter{Implementation}
\label{implementation}
This chapter will explain the implementation of the program starting with a short summary of how the older programs, JEIRA and GEISA, work. After that, a more thorough look at the structure and implementation of CuEira. The input files for all these programs are PLINK data files. However CuEira can only read the PLINK files in binary format. These programs also read a separate tab delimited file that contains the environmental data and covariates if any. For CuEira the covariates and environmental data are in two separate tab delimited files.

\section{JEIRA and GEISA}
The descriptions and structures in this section applies to both JEIRA and GEISA since their underlying structure is the same, however the focus is on GEISA. Both programs are written in Java and uses Javas features for concurrency.\\
\\
GEISA has three dependencies, two are provided as JAR files in the distribution so the user doesn't need to get install them separately. The dependency Apache Maven is only needed if the user wants to compile the program from source.

\begin{description}
  \item[Apache Commons CLI] \hfill \\
  Provides command line interface, e.g. it parses the options when using the program.
  \item[Apache Commons Math] \hfill \\
  Provides matrix and vector classes, linear algebra, etc.
  \item[Apache Maven] \hfill \\
  Used to compile Java programs.
\end{description}

The implementation uses the producer consumer pattern described in section~\ref{design_pattern}. The main thread creates a queue of tasks which all of the result producers iterate over and outputs results. These results are placed in a queue and consumed by another thread that writes them to a file. The program reads all data at the start which means it can get memory problems for large datasets. JEIRA and GEISA are both largely without tests, however there are a few unit tests for some of the basic storage classes and the binary file reader.

\newpage
\section{CuEira}
\label{cueira}
This section will explain the structure of CuEira and how it works starting with a general overview. Later the parts are explained and finally how the parts are combined to form the program. C++ was chosen because it is generally fast and have good control of the memory. The downside is that it is more platform dependent than Java. Java can also be easier to work with in several ways.\\
\\
The basis of the program is that each SNP is handled independently and a SNPs data is only read from the file when its needed. This reduces memory usage since only a few SNPs are stored in the memory at the same time. A number of worker threads share a queue of the SNPs and with each worker thread corresponding to a stream on a GPU. The program is split into two stages, initialisation and calculations. The initialisation part is done by the main thread and reads all the information except the SNP data. In the calculation step each worker threads fetches a SNP from the queue, reads its data, calculates the model, writes the results and then starts again with the next SNP until the queue is empty.\\
\\
An alternative solution would be to use \emph{callbacks} to control the execution of the kernels. A callback is a function in CUDA which after a kernel or transfer is complete starts a thread that calls a global function~\cite{cuda}. These new threads can not call new kernels or transfers because it could create infinite loops~\cite{cuda}. However the callbacks can be used to change a state tracked by the main thread which then tells the thread to execute the next kernel or transfer. This solution was not used because it is more complex without providing any significant advantages.\\
\\
Copying of the classes is avoided to save performance. Most of the classes have the copy and move constructor/operator deleted to prevent accidental usage. Accidental usage would likely cause errors because the default copy and move constructor/operator are naive and this will causes problems with more complex classes~\cite{c++_primer}.\\
\\
Factories are used at several places to help satisfy the dependency inversion principle. Without the factories some objects can not be mocked and therefore a proper unit test can not be performed.\\
\\
To improve the codes clarity several things were done. There are several namespaces to separate different parts of the program and to reduce clutter. For the same reason all classes have appropriate names and abbreviations were avoided in most of the cases. Comments were used sparsely because the names should be clear enough and explain what the code does. Comments are also easy to forget to update when the code is changed.\\
\\
There are timers at various places that can be used to profile parts of the program. Some of them count how much time is spent waiting at locks or specific parts of the calculations.

\newpage
\subsection{Dependency and Compiling}
CuEira have some dependencies. The dependency on MKL and Intels compiler is something that can be changed. BLAS shares the interface so another BLAS library could work with updates to the CMake file.

\begin{description}
  \item[Make] \hfill \\
  Used for compiling
  \item[CMake] \hfill \\
  Used to create Make files
  \item[Boost] \hfill \\
  Boost is a collections of C++ libraries. For CuEira it is used for file handling, string operations and parsing the command line options among others.
  \item[CUDA] \hfill \\
  NVIDIAs library for GPU, explained in section~\ref{cuda_programming_model}
  \item[CUBLAS] \hfill \\
  BLAS for CUDA
  \item[MKL] \hfill \\
  Intels BLAS library
  \item[Intel Compiler] \hfill \\
  Intels C/C++ compiler
  \item[Google Test and Google Mock] \hfill \\
  Unit test and mocking framework. They are provided in the CuEira source so the user does not need to install them separably.
\end{description}

CuEira is compiled by using \emph{CMake} and \emph{Make}. CMake generates Make files from a CMakeLists file. Make files sets rules for how the program should be compiled. Compiling can be complicated and using CMake is one way to make it easier. One of the main advantages of CMake is the modules and the command \emph{find package} which can find libraries by searching for them using common paths. The modules can also help with linking the library with the program and can sometimes also provided other functions. For instance the find package for CUDA has commands to compile CUDA code. There is also support for cross platform compiling which is something that could be used in the future to compile CuEira for other operative systems than Linux.\\
\\
The main CMake file needs to know what and where all the source files are. This is done by using nested CMake files, each directory has a CMakeLists file that adds it's sub directories and sets all the source files to the previous level. There are four types of lists of source files each is compiled into a library and then linked together. There is one list for all the tests, one for the files that needs to be compiled with nvcc, one for other GPU related code and then one with the host code.\\
\\
The CMake file contains several options that controls how CuEira is compiled. For instance one of them sets if it should be single or double precision. More details on how to compile and use CuEira can be found in appendix~\ref{compile_cueira}.

\subsection{Storage}
A set of container classes holds the data. There are two types of them, the basic matrix and vector classes and data containers. The data containers are responsible for handling the data for the model and performing the recoding. They use the vector classes for the actual storage.\\
\\
There are three types of matrix and vector classes which are all stored in column major format. The set with the Device prefix is for storage on the device and the other two are for storage on the host and shares a common interface. The difference between the host sets is that the ones with Pinned prefix uses pinned memory while Regular does not.\\
\\
Other basic information such as the information about individuals, environmental factors and SNPs are stored in simple classes where each individual, environmental factor or SNP is a single object. \emph{Enums} are used to store some of them.\\
\\
The SNPs are stored in the class \textbf{DataQueue}. \textbf{DataQueue} provides the function \textbf{next} that fetches a SNP from the queue and returns it. It has a lock so it can be shared among all worker threads.\\
\\
The \textbf{EnvironmentFactorHandler} is responsible for keeping track of the \textbf{EnvironmentFactors} and it's corresponding data. When constructed it updates the EnvironmentFactors variable type. This is done because it can sometimes be needed to know if a variable is binary or not. It also provides functions to access the \textbf{EnvironmentFactors} and the corresponding vectors.\\
\\
There are three data container classes, \textbf{SNPVector}, \textbf{EnvironmentVector} and \textbf{InteractionVector}. They all have the responsibility to keep track of the recoded data and to perform the recoding when their \textbf{recode} function is used. The \textbf{recode} function takes a \textbf{Recode} enum which tells it which type of recoding it should perform. The \textbf{SNPVector} also holds the original SNP data. \textbf{EnvironmentVector} doesn't need to since \textbf{EnvironmentFactorHandler} has that responsibility and \textbf{InteractionVector} does not have any kind of original data.\\
\\
The \textbf{DataHandler} is responsible for keeping track of the current combination of data containers being used and to iterate to the next one when its \textbf{next} function is called. When the \textbf{next} function is used a couple of things happen. If all environmental factors have been calculated together with the current SNP it deletes the SNPs information, asks the queue for a new one and reads its data and switches to the first environmental factor. Otherwise it moves on to the next environmental factor. It then updates the InteractionVector and the Statistics classes to create the contingency table and the allele frequencies. After that the next set of variables is ready to be used.

\newpage
\subsection{File Input and Output}
CuEira reads five files which are in four different formats. There are five classes for reading the files, one for each file.\\
\\
Three of the files are PLINK files in binary format. The fam file has the information about the individuals, the bim file has the names of the SNPs and their alleles. The genetic information for each individual is in the binary bed file. More details on PLINK files can be found in the appendix~\ref{plink_file}. The classes used to read them are \textbf{BimReader}, \textbf{BedReader} and \textbf{FamReader}.\\
\\
The other two files are CSV files. One of them contains the covariates and the other the environmental factors. \textbf{CSVReader} is responsible for reading them and storing the data in a matrix. The fifth reader class is \textbf{EnvironmentCSVReader} which inherits \textbf{CSVReader}. Its purpose is to convert the matrix read by \textbf{CSVReader} with the environmental data to \textbf{EnvironmentFactorHandler}.\\
\\
All the information except the genetic data in the bed file is read during the programs initialisation. When a new SNP is being used for calculations its data is read from the bed file. Most of the memory needed to store the data is in the bed file so only reading it when needed save a lot of memory.\\
\\
There is only one writer, the \textbf{ResultWriter}, and as its name suggests its responsibility is to write the results. The results are written immediately and then discarded to save memory. There is a lock on the writing so several threads can share it. The results will therefore be written in the order they are completed. The format of the output file is described in appendix~\ref{file_formats}.

\subsection{Initialisation of the Variables, Recoding and Statistic Model}
The initialisation of the \textbf{SNPVector} and \textbf{EnvironmentVector} is done by calling the recoding with ALL\_RISK from the \textbf{Recode} enum. The \textbf{recode} function calls different functions based on the enum. If it is ALL\_RISK it calls a function that copies the data from the vector holding the original data to vector that holds the data that will be used for the model. \textbf{InteractionVector} does recoding and initialisation the same way because in both cases the interaction data is the multiplication of the elements of the \textbf{SNPVector} and \textbf{EnvironmentVector}. The algorithms for the recode functions are shown in algorithm~\ref{alg:snp_recode},~\ref{alg:env_recode} and~\ref{alg:inter_recode}.\\
\\
For the additive statistical model all the elements in \textbf{SNPVector} and \textbf{EnvironmentVector} need to be set to 0 when the corresponding element in \textbf{InteractionVector} is 1. Both classes have a function named \textbf{applyStatisticModel} that performs it. The function takes the \textbf{InteractionVector} as a parameter and the algorithm is the same for both \textbf{SNPVector} and \textbf{EnvironmentVector}. The function is shown in algorithm~\ref{alg:app_stat_model}.

\newpage
\textbf{DataHandler} has the overall responsibility for recoding and applying the statistical model, it calls the data containers functions as needed. It also updates the \textbf{ContingencyTable} after recoding since the distribution of the groups changes when recoding.

\begin{algorithm}
\DontPrintSemicolon

\SetKwInOut{data}{Data}

\BlankLine \BlankLine

\data{Two vectors of length $N$, $originalData$ and $recodedData$\\
$snpData0$, $snpData1$, $snpData2$ stores the value for the final \; vector that corresponds to the original vectors values of 0,1 and 2\\
$i$ is an index from $0$ to $N-1$}

\BlankLine \BlankLine

\If{Dominant genetic model}{
  \If{Risk allele is allele 1}{
    $snpData0 \leftarrow 1$\;
    $snpData1 \leftarrow 1$\;
    $snpData2 \leftarrow 0$\;
  }\ElseIf{Risk allele is allele 2}{
    $snpData0 \leftarrow 0$\;
    $snpData1 \leftarrow 1$\;
    $snpData2 \leftarrow 1$\;
  }
}\ElseIf{Recessive genetic model}{
  \If{Risk allele is allele 1}{
    $snpData0 \leftarrow 1$\;
    $snpData1 \leftarrow 0$\;
    $snpData2 \leftarrow 0$\;
  }\ElseIf{Risk allele is allele 2}{
    $snpData0 \leftarrow 0$\;
    $snpData1 \leftarrow 0$\;
    $snpData2 \leftarrow 1$\;
  }
}

\BlankLine \BlankLine

\For{$i \leftarrow 0$ \KwTo $N$}{
  \If{$originalData[i] = 0$}{
    $recodedData[i] = snpData0$\;
  }\ElseIf{$originalData[i] = 1$}{
    $recodedData[i] = snpData1$\;
  }\ElseIf{$originalData[i] = 2$}{
    $recodedData[i] = snpData2$\;
  }
}

\caption{SNPVector protective recoding}
\algorithmslist{SNPVector protective recoding}
\label{alg:snp_recode}
\end{algorithm}

\begin{algorithm}
\DontPrintSemicolon

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{Two vectors of length $N$, $originalData$ and $recodedData$\\
$i$ is an index from $0$ to $N-1$}

\BlankLine \BlankLine

\For{$i \leftarrow 0$ \KwTo $N$}{
  \If{$originalData[i] == 0$}{
    $recodedData[i] = 1$\;
  }\Else{
    $recodedData[i] = 0$\;
  }
}

\caption{EnvironmentVector protective recoding}
\algorithmslist{EnvironmentVector protective recoding}
\label{alg:env_recode}
\end{algorithm}

\begin{algorithm}
\DontPrintSemicolon

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{Three vectors of length $N$, $interactionData$, $envData$ and $snpData$\\
$i$ is an index from $0$ to $N-1$}

\BlankLine \BlankLine

\For{$i \leftarrow 0$ \KwTo $N$}{
  $interactionData[i] = envData[i] * snpData[i]$\;
}

\caption{InteractionVector recoding}
\algorithmslist{InteractionVector recoding}
\label{alg:inter_recode}
\end{algorithm}

\begin{algorithm}
\DontPrintSemicolon

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{Two vectors of length $N$, $interactionData$ and $dataVector$\\
$i$ is an index from $0$ to $N-1$}

\BlankLine \BlankLine

\For{$i \leftarrow 0$ \KwTo $N$}{
  \If{$interactionData[i]$ \textbf{not} $0$}{
    $dataVector[i] = 0$\;
  }
}

\caption{Applying statistic model}
\algorithmslist{Applying statistic model}
\label{alg:app_stat_model}
\end{algorithm}

\clearpage
\subsection{Wrappers}
CUDA and BLAS functions and interfaces are in C style. To make it easier to use and replace if needed they were put in wrappers. These wrappers are in C++ style and do the actual function calls to CUDA and BLAS. Since CUDA and BLAS contain a lot of various functions functions were added to the wrappers when needed.\\
\\
There are several reasons to use wrappers. One is that some of the functions does not use exceptions but return an error code instead. This forces the program to check these error codes. This is hidden by using a wrapper that can throw the correct exception based on the error code. Exceptions are also better for performance because these remove the need for the \emph{if} statements checking the returned error codes. The compiler can optimize better with checks for exceptions(e.g. try catch statements) because it can assume that the occurrence of exceptions is low. However usually checks for exceptions are not needed because they commonly signify an error that the program can not recover from.\\
\\
Another reason is the lack of object orientation. For instance the BLAS interface standard is not object oriented which causes its functions to have a lot of parameters. Matrix vector multiplication with MKL(Intels version of BLAS) is shown in~\ref{ex:mkl_mv}. The function call uses 12 parameters, most of which are of similar types. One time during the thesis it took a day to find an error that was in one of the CUBLAS calls. Due to a parameter being 1 instead of 0 it performed the operation $y=z*x+y$ instead of $y=z*x$. These types of errors are much easier to make when a function has a larger number of similar parameters. MKL is wrapped in \textbf{MKLWrapper}. Its wrapped version of matrix vector multiplication, shown in~\ref{ex:mkl_wrap_mv}, has five parameters of which two have default values.\\
\\
As is sometimes common BLAS function names are heavily abbreviated. For instance a function for matrix vector multiplication is \textbf{sgemv}, the $s$ stands for single precision, $ge$ for general, $m$ for matrix and $v$ for vector. General means that it is standard matrices and vectors, e.g. not triangular or sparse. The equivalent function in \textbf{MKLWrapper} can be found in example~\ref{ex:mkl_wrap_mv}. The name matrixVectorMultiply instantly tells the user what it does. As mentioned earlier in section~\ref{cueira} the CuEira is either single or double precision depending on an option when compiling so \textbf{MKLWrapper} doesn't mention the precision. All matrices and vectors are also general. However even if the function name would contain that information too it would still not be so long that it was in the way. A possible name by using namespaces for the precision and type could be General::SinglePrecision::matrixVectorMultiply.\\

\stepcounter{ExamplesCounter}
\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}%
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}%
\SetKwFor{For}{for}{}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
\SetKwFor{While}{while}{}{}%
\SetKwRepeat{Repeat}{repeat}{until}%
\AlgoDisplayBlockMarkers\SetAlgoNoLine%

\SetAlgoRefName{\arabic{ExamplesCounter}} %\SetAlgoRefName{\arabic{chapter}.\arabic{ExamplesCounter}}
\SetAlgorithmName{Example}{List of Examples}

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{$layout$, storage type of the matrix, either column major(CblasColMajor) or row major(CblasRowMajor)\\
$trans$, use transpose, conjugate or neither on the matrix\\
$matrix$, $vector$ and $result$ are float pointers\\
$\alpha$ and $\beta$ are constants\\
$m$, number of rows in the matrix\\
$n$, number of columns in the matrix\\
$ld\_matrix$, leading dimension of the matrix\\
$inc\_vector$ and $inc\_result$ are the increments of the elements
}

\BlankLine \BlankLine

\textbf{cblas\_sgemv}($layout$, $trans$, $m$, $n$, $\alpha$, $matrix$, $ld\_matrix$, $vector$, $inc\_vector$,
      $\beta$, $result$, $inc\_result$);

\caption{Single precision matrix vector multiplication using MKL, $result=\alpha*matrix*vector+\beta*result$}
\example{Single precision matrix vector multiplication using MKL}
\label{ex:mkl_mv}
\end{algorithm}

\stepcounter{ExamplesCounter}
\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}%
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}%
\SetKwFor{For}{for}{}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
\SetKwFor{While}{while}{}{}%
\SetKwRepeat{Repeat}{repeat}{until}%
\AlgoDisplayBlockMarkers\SetAlgoNoLine%

\SetAlgoRefName{\arabic{ExamplesCounter}} %\SetAlgoRefName{\arabic{chapter}.\arabic{ExamplesCounter}}
\SetAlgorithmName{Example}{List of Examples}

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{$matrix$, HostMatrix\\
$vector$, HostVector\\
$result$, HostVector\\
$\alpha$, constant, default 1\\
$\beta$, constant, default 0\\
}

\BlankLine \BlankLine

\textbf{matrixVectorMultiply}($matrix$, $vector$, $result$, $\alpha$, $\beta$);

\caption{Matrix vector multiplication using MKLWrapper, $result=\alpha*matrix*vector+\beta*result$}
\example{Matrix vector multiplication using MKLWrapper}
\label{ex:mkl_wrap_mv}
\end{algorithm}

\clearpage
CUBLAS is wrapped together with the kernels made for CuEira in \textbf{KernelWrapper} in a similar way to how MKL is wrapped in \textbf{MKLWrapper}. \textbf{KernelWrapper} is explained in section~\ref{kernelwrapper}. Other CUBLAS utility and CUDA functions are wrapped in \textbf{CudaAdapter}. The \textbf{CudaAdapter} has a functions to convert CUBLAS error codes to strings, allocate memory and so on.\\
\\
The CUDA streams are managed by a \textbf{Stream} class and its factory. When the \textbf{StreamFactory} creates a new Stream object it creates a new CUDA stream, a new CUBLAS handle and associates the CUBLAS handle with the new stream. The \textbf{Stream} class has functions to retrieve them and if the Stream object is destroyed, it destroys both the CUDA stream and the CUBLAS handle. Each Stream object is associated with a \textbf{Device} object. The \textbf{Device} class represents a device(i.e. GPU) and since a thread can only issue commands to one device at a time the \textbf{Device} class has functions to set it as the active one and check if itself is the device active or not. The outcomes of the individuals does not change so it can be shared between all streams on the device and the \textbf{Device} class is responsible for the vector with the outcomes on the device.\\
\\
The transfers to and from the device are done by two classes, \textbf{DeviceToHost} and \textbf{HostToDevice}. They perform asynchronous transfers on the corresponding matrix and vector class. It can either create a new container to transfer to or transfer to a given place in the memory. The former is useful for instance when transferring a group of vectors into a combined matrix.

\newpage
\subsection{Kernels}
\label{kernelwrapper}
All the kernels are wrapped together with the CUBLAS kernels in \textbf{KernelWrapper}. All the kernels made for CuEira are in a similar style. They all do operations on each element of vectors. This makes their structure simple, each thread does the operations on one index. An example of one of the kernels and its wrapper function can be found in algorithm~\ref{alg:kernel_elemenwise_add_device} and algorithm~\ref{alg:kernel_elemenwise_add_host}.

\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}
\SetKwFor{For}{for}{}{}
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}
\SetKwFor{While}{while}{}{}
\SetKwRepeat{Repeat}{repeat}{until}
\AlgoDisplayBlockMarkers
\SetAlgoNoLine
\SetFuncSty{}
\SetArgSty{}

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{$vector1$, $vector2$ and $result$ are vectors as pointers,\\
$length$ is the length of the vectors}

\BlankLine \BlankLine

\SetKwFunction{KwFn}{\textcolor{kernel}{\_\_global\_\_} void \KwSty{ElementWiseAddition}}

\Fn(){\KwFn{const \textcolor{keyword}{PRECISION*} $vector1$, const \textcolor{keyword}{PRECISION*} $vector2$, \textcolor{keyword}{PRECISION*} $result$,
    const \textcolor{keyword}{int} $length$}}{
    
 \textcolor{keyword}{int} threadId = blockDim.x * blockIdx.x + threadIdx.x;

 \BlankLine \BlankLine
 
  \If{threadId < $length$}{
    $result$[threadId] = $vector1$[threadId] + $vector2$[threadId];
  }
}

\algorithmslist{Kernel for vector addition}
\caption{Kernel for vector addition}
\label{alg:kernel_elemenwise_add_device}
\end{algorithm}

\begin{algorithm}
\DontPrintSemicolon
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}%
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}%
\SetKwFor{For}{for}{}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
\SetKwFor{While}{while}{}{}%
\SetKwRepeat{Repeat}{repeat}{until}%
\AlgoDisplayBlockMarkers\SetAlgoNoLine%
\SetAlgoNoLine
\SetFuncSty{}
\SetArgSty{}

\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{$vector1$, $vector2$ and $result$ are DeviceVectors of same length\\
cudaStream is the stream for the kernel\\
numberOfThreadsPerBlock is 256 }

\BlankLine \BlankLine

\SetKwFunction{KwFn}{void \KwSty{elementWiseAddition}}

\Fn(){\KwFn{const \textcolor{keyword}{DeviceVector\&} $vector1$, const \textcolor{keyword}{DeviceVector\&} $vector2$,
  \textcolor{keyword}{DeviceVector\&} $result$}}{
  
  \textit{//Error check for the length of the vectors in the source code removed}\;
  
  \BlankLine \BlankLine
    
  const \textcolor{keyword}{int} $numberOfBlocks$ = std::ceil(((\textcolor{keyword}{double}) vector1.getNumberOfRows()) / numberOfThreadsPerBlock);\;

  \BlankLine \BlankLine
  
  \textbf{Kernel::ElementWiseAddition}\textcolor{kernel}{$<<<<numberOfBlocks, numberOfThreadsPerBlock, 0, cudaStream>>>$}\;
  \BlankLine
  ($vector1$.getMemoryPointer(), $vector2$.getMemoryPointer(), $result$.getMemoryPointer(), $vector1$.getNumberOfRows());\;
}

\algorithmslist{Wrapper for the kernel in algorithm~\ref{alg:kernel_elemenwise_add_device}}
\caption{Wrapper for the kernel in algorithm~\ref{alg:kernel_elemenwise_add_device}}
\label{alg:kernel_elemenwise_add_host}
\end{algorithm}

\newpage
\subsection{Model}
The calculations are done by using a group of \textbf{Model} classes. A list of these classes and their responsibilities is shown below.

\begin{description}
  \item[Model] \hfill \\
  The model to calculate.
  \item[ModelConfiguration] \hfill \\
  Holds the data for the Model.
  \item[ModelResult] \hfill \\
  Contains the results from one Model on one set of data.
  \item[CombinedResults] \hfill \\
  The results for one set can consist of more than one ModelResult. CombinedResults have that responsibility.
  \item[ModelHandler] \hfill \\
  Has a \textbf{next} function to iterate over the data delegating it to DataHandler. It also has a pure virutal function \textbf{calculate} which should be used for calculating the Models and collecting the results.
  \item[ModelInformation] \hfill \\
  Contains the ContingencyTable and AlleleStatistics. 
\end{description}

The point of these classes is that they provided common interfaces and are easy to extend. This makes it easy to later add or change which models are calculated and how their results are handled. The next section will explain how the Model base classes were extended to form the classes for the LR model.

\subsubsection{Logistic Regression}
The LR model is implemented using the \textbf{Model} base classes talked about in the previous section. There is a CPU version and a GPU version using CUDA. Some classes are shared between them because of common elements. The structure is shown in figure~\ref{fig:lr_inheritance}. A more detailed layout of the \textbf{ModelHandler} for \textbf{LogisticRegression} is shown in figure~\ref{fig:lr_modelhandler}.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{lr_structure.png}
    \caption[Overview of the logistic regression classes]{Overview of the logistic regression classes. White arrow is inheritance. Diamond is that the class with the diamond has instances of the other class.}
    \label{fig:lr_inheritance}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=6cm]{modelhandler_lr_layout.png}
    \caption{Layout of the LogisticRegressionModelHandler}
    \label{fig:lr_modelhandler}
\end{figure}

The LR algorithm is split into several protected functions so that each part can be tested individually. The parts are then used in the \textbf{calculate} function. For \textbf{CudaLogisticRegression} the parts corresponding to line 10 and 11 in algorithm~\ref{alg:lr} are done on the host. All the other parts of the LR algorithm are done by using CUBLAS or the kernels explained in section~\ref{kernelwrapper}. The calculations for line 10 and 11 are in the \textbf{LogisticRegression} class so the code is shared between \textbf{CpuLogisticRegression} and \textbf{CudaLogisticRegression}.\\
\\
Line 10 and 11 are done on the host instead of the device because the inverse of the information matrix(variable \textbf{J} in the algorithm) is done by SVD and no suitable GPU function for it was found. The reason for using SVD instead of normal matrix inverse was previously explained in section~\ref{logistic_regression}. CUBLAS does not provide a SVD function. However there are other libraries that do, CULA Tools~\cite{culatools} and MAGMA~\cite{magma_2010}, however neither could be used.

\clearpage
CULA Tools could not be used because it uses streams internally in the functions so it can not be executed on streams~\cite{cula_forum_q}. It also uses device synchronisation which means that the all streams will be synchronised every time the function is used~\cite{cula_forum_q}. MAGMA can use streams for its BLAS functions, however it does not for the other functions including the SVD~\cite{magma_stream}.\\
\\
The size of the vectors and matrices used in that part are also small, their size is 4 + the number of covariates. However it can sometimes be better to calculate even small matrices and vectors on the device to avoid the transfers that would be needed otherwise~\cite{cuda_best_practice}. A suitable GPU SVD kernel might be faster.\\
\\
After transfers it is necessary to wait for the transfer to finish to prevent errors with using the data before it has been transferred completely. This is done by syncing the thread with the stream, it forces the thread to wait until all current kernels and transfers on the stream is complete. However due to the problem with asynchronous transfers and older architectures described in section~\ref{streams} there is also an option when compiling to synchronise after each kernel.

\subsection{Worker Thread}
A number of worker threads use the previously explained classes to perform the work. One worker thread corresponds to one stream on a GPU. Normally three streams are used per GPU but it can easily be changed by changing a parameter in the main \textbf{Configuration} class. The layout of the worker thread is shown in figure~\ref{fig:thread_layout}.\\
\\
The thread has its own \textbf{DataHandler}, \textbf{ModelHandler}, \textbf{ModelConfiguration} and Model. Some objects are shared with the other workers, most notably the \textbf{ResultFileWriter} and the \textbf{DataQueue}. Each thread loops over the \textbf{ModelHandlers} \textbf{next} function, tells the \textbf{ModelHandler} to calculate and then sends the results to the \textbf{ResultFileWriter}.

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{thread_layout.png}
    \caption{Structure of the worker thread}
    \label{fig:thread_layout}
\end{figure}

\clearpage
\subsection{Testing}
Almost all classes have a corresponding unit test that tests its functionality in isolation. However the matrix and vector classes are never mocked out because they are too basic and fundamental. It does mean that if there are multiple errors when testing and the tests for the matrix and vectors failed the fault likely is with those. However the number of integration tests are few and there is a need to create more.\\
\\
Since mocks are created by using inheritance from the class to be mocked all functions need to be virtual. Some classes also have empty protected constructors to be used by the mock, others have helper functions to construct the mock. This goes against some parts of clean code but is necessary because of how C++ and mocking works. It could potentially be improved by using private constructors and friends to give the mock access to the private constructor.\\
\\
The data for the tests where calculated by hand or by using Matlab.

\subsection{Memory Usage}
The memory usage is low because only the SNPs whose models currently is being calculated is stored in memory. This also means that most of the memory usage depends on the number of individuals not the number of SNPs. The memory is also bound by the GPU rather than host memory because of the relative larger size of the host memory. The amount used depends on the precision, single precision floating point numbers are four bytes while double precision numbers are eight bytes.

\subsubsection{Device Memory}
The variables used on the device and their number of elements are shown in table~\ref{table:gpu_variable_sizes}. The memory need with $T$ number of streams is shown in equation~\ref{eq:device_memory}.The amount of individuals than can fit inside 6GB(i.e. a typical GPU) with three streams for different number of covariates and precision is shown in table~\ref{table:gpu_mem}. The number of individuals possible is one magnitude or more than the in a usual study.

\begin{equation}\label{eq:device_memory}
(N+T(2NM+2N+2M+MM))*Element Size
\end{equation}

\begin{table}[h]
\centering
\begin{tabular}{| l c c |}
  \hline
  Matrix/vector & Size & Instances\\
  \hline
  Outcomes & $N$ & 1 \\
  Predictors & $N \times M$ & One per stream \\
  Probabilities & $N$ & One per stream \\
  Scores & $M$ & One per stream \\
  Beta & $M$ & One per stream \\
  Work Area & $N$ & One per stream \\
  Work Area & $N \times M$ & One per stream \\
  Information Matrix & $M \times M$ & One per stream \\
  \hline  
\end{tabular}
\caption[Sizes of the variables stored on the GPU]{Sizes of the variables stored on the GPU, $N=$ number of individuals, $M=4+$ number of covariates}
\label{table:gpu_variable_sizes}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{| l | l l |}
  \hline
  Number of covariates & Single precision & Double precision\\
  \hline
  0 & 48.4 & 24.2 \\
  5 & 24.6 & 12.3 \\
  10 & 16.5 & 8.2 \\
  20 & 9.0 & 5.0 \\
  100 & 2.4 & 1.2 \\
  \hline  
\end{tabular}
\caption{The number of individuals in millions needed to fill 6GB with the given precision, number of covariates and three streams}
\label{table:gpu_mem}
\end{table}

\newpage
\subsubsection{Host Memory}
%TODO host memory
The matrix and vector variables are shown in table~\ref{table:host_mv_variable_sizes}. However there are also other data related information that needs to be stored and that potentially uses a significant amount of memory, they are shown in table~\ref{table:host_variable_sizes}. Classes that only exist in few instances and similar are ignored because their effect on the memory is minimal compared to the actual data. With the sizes from the tables the needed amount of host memory is shown in~\ref{eq:host_memory}.\\
\\
Equation~\ref{eq:host_memory} shows how much memory is used with $N$ individuals, $E$ environmental factors, $S$ SNPs, $C$ covariates and $T$ streams. $str$ is the size of a string, it can vary however due to overhead they can reach up to 100 bytes. Assuming 100 bytes for each string the memory requirement is low. With extreme values of 10 million SNPs, 10 million individuals, 1000 covariates and double precision it needs 88 GB, which is large, but not impossible to fit inside the memory of a modern cluster.

\begin{equation}\label{eq:host_memory}
ElementSize*(N(1+E+C)+T(60+6N+31C+C^2))+str*(N+E+3*S)+12N+4E+28S
\end{equation}

\begin{table}[h]
\centering
\begin{tabular}{| l c c |}
  \hline
  Matrix/vector & Size & Instances\\
  \hline
  Outcomes & $N$ & 1 \\
  Environment data & $N \times E$ & 1 \\
  Covariates data & $N \times C$ & 1 \\
  SNPVector & $2N$ & One per stream \\
  EnvironmentVector & $N \times (4+C)$ & One per stream \\
  InteractionVector & $N \times (4+C)$ & One per stream \\
  Beta & $(4+C)$ & One per stream \\
  Scores & $(4+C)$ & One per stream \\
  Sigma & $(4+C)$ & One per stream \\
  uSVD & $(4+C) \times (4+C)$ & One per stream \\
  vSVD & $(4+C) \times (4+C)$ & One per stream \\
  Work area & $(4+C) \times (4+C)$ & One per stream \\
  \hline  
\end{tabular}
\caption{Sizes of the matrix/vector variables stored on the host}
\label{table:host_mv_variable_sizes}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{| l c c |}
  \hline
  Object & Contains & Number\\
  \hline
  SNP & 3 strings, 6 int, 1 bool & $S$\\
  EnvironmentFactor & 1 string, 1 int & $E$\\
  Persons & 1 string, 2 int, 1 bool & $N$\\
  \hline  
\end{tabular}
\caption{Sizes of the other variables stored on the host}
\label{table:host_variable_sizes}
\end{table}

\chapter{Results}
%TODO results
This chapter contains the results of profiling the program. The data used consists of randomized data with equal number of cases and controls. The number of SNPs, individuals and covariates was varied. The specifications for the hardware used is shown in table~\ref{table:hardware}. The GPUs are of the Fermi architecture so they have one queue for the kernels and transfers.\\
\\
The expectations were that CuEira would have linear efficiency with multiple GPUs since there is no communication between the GPUs and that each gene-environment combination is calculated independently. 
%\\
%\\
%The data used are AcpaPos and simulated datasets.

%TODO typical problem size
%likely to increase more on snp sided than individuals?

\begin{table}[h]
\centering
\begin{tabular}{| l | l |}
  \hline
  CPU & 2 Intel Xeon E5620, 4 cores per CPU\\
  \hline
  GPU & 4 Tesla C2050\\
  \hline
  Memory & 48 GB ram\\
  \hline  
\end{tabular}
\caption{Hardware specifications}
\label{table:hardware}
\end{table}

%TODO explain what the different times are, calc init etc
All the execution times used here are the times the calculation part of the program took. This is because the initialisation part of the program varies a lot in time. The part of the initialisation that varies greatly is when creating the vector of the outcomes(i.e. the individuals phenotypes) which. The total time for initialisation and the time to create the outcomes are shown in figure~\ref{fig:10stream_init}. Except for the first execution the order does not seem to matter, figure~\ref{fig:10stream_init_backward} shows the initialisation times for the same executions as figure~\ref{fig:10stream_init} however they were performed in opposite order. To prevent a bias from this in the results all the plots use the calculation time instead of total run time.

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{less_sync_init.png}
    \caption{10 000 SNPs and individuals with 1-4 GPUs with 1-10 streams. It was performed starting with 1 GPU and 1 stream and then increasing the streams and then the GPUs. I.e. from left to right in the figure.}
    \label{fig:10stream_init}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{less_sync_init_backward.png}
    \caption{Same as figure~\ref{fig:10stream_init} but calculating the files in opposite order.}
    \label{fig:10stream_init_backward}
\end{figure}

\clearpage
\section{Scaling}
This section contains the results of how the programs speed changes when the size of the data is varied.\\
\\
The program is expected to be $O(n)$ for the SNPs because the program treats each SNP-environment combination independently, more SNPs simply means doing the same things more times. Figure~\ref{fig:snps_eff_cov_0_stream_4} and~\ref{fig:snps_eff_cov_20_stream_4} shows the calculation time for various number of SNPs relative to the time with 10 000 SNPs according to equation~\ref{eq:snps_eff}. When it is one then the increased time is what was expected. As the figures show all points are close to one except one point. Also due to how long time the calculations take with a large number of SNPs it is also not possible to perform all profiling with large number of SNPs.

\begin{equation}\label{eq:snps_eff}
\frac{time_{X \; SNPs}}{time_{10^4 \; SNPs}} / \frac{X}{10^4}
\end{equation}

Individuals increase work but it does not increase so that double means double the work, shown in figure~\ref{fig:individuals_cov_0_stream_4}. There are no major changes with the addition of covariates. With 20 covariates is shown in figure~\ref{fig:individuals_cov_20_stream_4}. Individuals are also a part of the LR algorithm so might affect things that way.\\
\\
The number of covariates affects the speed when the number of individuals is low, see figure~\ref{fig:cov_individuals2000_stream_4}. However when the number of individuals is large the effect of the covariates is much smaller as shown in figure~\ref{fig:cov_individuals2e+05_stream_4}.

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{snps_cov_0_ind_10000_stream_4.png}
    \caption{Varying the number of SNPs. 10 000 individuals, 0 covariates, 4 streams}
    \label{fig:snps_cov_0_stream_4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{snps_cov_20_ind_10000_stream_4.png}
    \caption{Varying the number of SNPs. 10 000 individuals, 20 covariates, 4 streams}
    \label{fig:snps_cov_20_stream_4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{snps_eff_cov_0_ind_10000_stream_4.png}
    \caption{Relative time versus 10 000 SNPs. 10 000 individuals, 0 covariates, 4 streams}
    \label{fig:snps_eff_cov_0_stream_4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{snps_eff_cov_20_ind_10000_stream_4.png}
    \caption{Relative time versus 10 000 SNPs. 10 000 individuals, 20 covariates, 4 streams}
    \label{fig:snps_eff_cov_20_stream_4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{individuals_cov_0_stream_4.png}
    \caption{Varying the number of individuals. 10 000 SNPs, 0 covariates, 4 streams}
    \label{fig:individuals_cov_0_stream_4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{individuals_cov_20_stream_4.png}
    \caption{Varying the number of individuals. 10 000 SNPs, 20 covariates 4 streams}
    \label{fig:individuals_cov_20_stream_4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{cov_individuals2000_stream_4.png}
    \caption{Varying the number of covariates. 10 000 SNPs, 2 000 individuals}
    \label{fig:cov_individuals2000_stream_4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{cov_individuals2e+05_stream_4.png}
    \caption{Varying the number of covariates. 10 000 SNPs, 200 000 individuals}
    \label{fig:cov_individuals2e+05_stream_4}
\end{figure}

\clearpage
\section{Streams and Multi-GPU}
The number of streams affects the performance significantly. As figure~\ref{fig:gpu_10streams_10ks_10ki} and figure~\ref{fig:gpu_10streams_10ks_100ki} show using too few streams increases the time the program takes. This is because to few streams does not provide enough work for the GPU. The best performance is when the total number of streams is slightly above the number of cores. It likely means that the CPU is the bottleneck. However for some data sizes with one GPU the GPU is the bottleneck as figure~\ref{fig:gpu_10streams_10ks_10ki} shows where increased number of streams beyond six gives little. Using too many streams have some negative effects.\\
\\
NVIDIAs profiler nsight shows warning messages about low overlap between transfers and kernels and between kernels and kernels when using four GPUs and four streams. This also points at the CPU can not provided enough data to the GPUs. However these warnings does not show up for two GPUs and four streams.\\
\\
Figures~\ref{fig:saturated_seconds_ind_cov_0},~\ref{fig:saturated_seconds_cov_ind_2000},~\ref{fig:saturated_speedup_ind_cov_0},~\ref{fig:saturated_speedup_cov_ind_2000},~\ref{fig:saturated_efficiency_ind_cov_0} and~\ref{fig:saturated_efficiency_cov_ind_2000} show how the program behaves and scales with multiple GPUs. The figures~\ref{fig:saturated_seconds_ind_cov_0} and~\ref{fig:saturated_seconds_cov_ind_2000} shows the calculation time in seconds. Figures~\ref{fig:saturated_speedup_ind_cov_0} and~\ref{fig:saturated_speedup_cov_ind_2000} shows the speed up for figures~\ref{fig:saturated_seconds_ind_cov_0} and~\ref{fig:saturated_seconds_cov_ind_2000}, while figures~\ref{fig:saturated_efficiency_ind_cov_0} and~\ref{fig:saturated_efficiency_cov_ind_2000} show their efficiency.\\
\\
Two GPUs show increased speed compared to one GPU. However for three and four GPUs the speed is almost the same as with two GPUs. With low number of individuals(2000) the scaling to two GPUs almost reaches linear efficiency, 10 000 individuals also has decent scaling to two GPUs. Three and four GPUs show almost no increase in speed for any amount of data therefore also shows low efficiency, in some cases there even is a drop in speed.

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{gpu_10streams_10ks_10ki.png}
    \caption{1 to 10 streams, 10 000 SNPs, 10 000 individuals}
    \label{fig:gpu_10streams_10ks_10ki}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{gpu_10streams_10ks_100ki.png}
    \caption{1 to 10 streams, 10 000 SNPs, 100 000 individuals}
    \label{fig:gpu_10streams_10ks_100ki}
\end{figure}

\clearpage

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{gpu_5to10streams_10ks_100ki.png}
    \caption{5 to 10 streams, 10 000 SNPs, 100 000 individuals}
    \label{fig:gpu_5to10streams_10ks_100ki}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{saturated_gpu_seconds_ind_cov_0.png}
    \caption{Varying the number of GPUs and individuals. Number of streams are what give best performance. 10 000 SNPs, 0 covariates}
    \label{fig:saturated_seconds_ind_cov_0}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{saturated_gpu_seconds_cov_ind_2000.png}
    \caption{Varying the number of GPUs and covariates. Number of streams are what give best performance, 10 000 SNPs, 2 000 individuals}
    \label{fig:saturated_seconds_cov_ind_2000}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{saturated_gpu_speedup_ind_cov_0.png}
    \caption{Speedup vs one GPU for figure~\ref{fig:saturated_seconds_ind_cov_0}. Plain diagonal line corresponds to linear efficiency}
    \label{fig:saturated_speedup_ind_cov_0}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{saturated_gpu_speedup_cov_ind_2000.png}
    \caption{Speedup vs one GPU for figure~\ref{fig:saturated_seconds_cov_ind_2000}. Plain diagonal line corresponds to linear efficiency}
    \label{fig:saturated_speedup_cov_ind_2000}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{saturated_gpu_efficiency_ind_cov_0.png}
    \caption{Efficiency vs one GPU for figure~\ref{fig:saturated_seconds_ind_cov_0}. Plain horizontal line corresponds to linear efficiency}
    \label{fig:saturated_efficiency_ind_cov_0}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{saturated_gpu_efficiency_cov_ind_2000.png}
    \caption{Efficiency vs one GPU for figure~\ref{fig:saturated_seconds_cov_ind_2000}. Plain horizontal line corresponds to linear efficiency}
    \label{fig:saturated_efficiency_cov_ind_2000}
\end{figure}

\clearpage
\section{Single versus Double Precision}
Using double precision increases the time in almost all cases. Figure~\ref{fig:single_vs_double_ind_10000} and~\ref{fig:single_vs_double_cov_0} show the relative difference in calculation time for single precision divided by double precision with the optimal number of streams. One point has better performance with double precision than single.
%TODO write about number of iterations

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{double_comp_cov_ind_10000.png}
    \caption[Relative calculation time for single precision divided by the corresponding time for double precision, 10 000 SNPs, 10 000 individuals]{Relative calculation time for single precision divided by the corresponding time for double precision. Number of streams are what give best performance, 10 000 SNPs, 10 000 individuals}
    \label{fig:single_vs_double_ind_10000}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{double_comp_ind_cov_0.png}
    \caption[Relative calculation time for single precision divided by the corresponding time for double precision. 10 000 SNPs, 0 covariates]{Relative calculation time for single precision divided by the corresponding time for double precision. Number of streams are what give best performance, 10 000 SNPs, 0 covariates}
    \label{fig:single_vs_double_cov_0}
\end{figure}

\clearpage
\section{The LR algorithm, Transfers and Synchronisations}
With GPUs Fermi architecture a problem can occur when using asynchronous transfers as talked about in section~\ref{streams}. To examine if it affects the speed of the program several more synchronisations were added so that the CPU synchronises after each kernel or transfer call instead of when only necessary. A comparison between the speed with and without these is shown in figures~\ref{fig:less_sync_vs_always_sync_ind_200k} and~\ref{fig:less_sync_vs_always_sync_ind_2k}. More synchronisation increases the time the calculations took in most cases. However with sufficient number of individuals(100 000 and up) some speed is gained with more synchronisations. 200 000 individuals have increased speed in most cases. The transfer times relative to the calculations in LR decreased with the increased syncing, see figure~\ref{fig:timedist_LR_ind_10ks_0cov_gpu1} and~\ref{fig:timedist_LR_ind_10ks_0cov_gpu1_sync}.\\
\\
As figure~\ref{fig:timedist_LR_cov_10ks_2000i_gpu4} shows with increased number of covariates the CPU time of the LR algorithm increases. However with more individuals the effect of the covariates decreases see figure~\ref{fig:timedist_LR_cov_10ks_2e+05i_gpu4}. When the number of GPUs increase the time spent on transfers increases, compare figure~\ref{fig:timedist_LR_ind_10ks_0cov_gpu1} with figure~\ref{fig:timedist_LR_ind_10ks_0cov_gpu4}.

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{sync_comp_cov_ind_2000.png}
    \caption{The calculation time without the increased synchronisations divided by the time with increased synchronisations. 10 000 SNPs, 2 000 individuals}
    \label{fig:less_sync_vs_always_sync_ind_2k}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{sync_comp_cov_ind_2e+05.png}
    \caption{The calculation time without the increased synchronisations divided by the time with increased synchronisations. 10 000 SNPs, 200 000 individuals}
    \label{fig:less_sync_vs_always_sync_ind_200k}
\end{figure}

%TODO switch with 9 stream
\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{timedist_LR_ind_10ks_0cov_gpu1.png}
    \caption{Distribution of time spent on the LR algorithm. 1 GPU, 3 streams, 10 000 SNPs, 0 covariates}
    \label{fig:timedist_LR_ind_10ks_0cov_gpu1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{timedist_LR_ind_10ks_0cov_gpu4.png}
    \caption{Distribution of time spent on the LR algorithm. 4 GPU, 3 streams, 10 000 SNPs, 0 covariates}
    \label{fig:timedist_LR_ind_10ks_0cov_gpu4}
\end{figure}

%TODO switch with 9 stream
\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{timedist_LR_ind_10ks_0cov_gpu1_sync.png}
    \caption{Distribution of time spent on the LR algorithm with increased synchronisations. 1 GPU, 3 streams, 10 000 SNPs, 0 covariates}
    \label{fig:timedist_LR_ind_10ks_0cov_gpu1_sync}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{timedist_LR_ind_10ks_0cov_gpu4_sync.png}
    \caption{Distribution of time spent on the LR algorithm with increased synchronisations. 4 GPU, 3 streams, 10 000 SNPs, 0 covariates}
    \label{fig:timedist_LR_ind_10ks_0cov_gpu4_sync}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{timedist_LR_cov_10ks_2000i_gpu4.png}
    \caption{Distribution of time spent on the LR algorithm. 4 GPU 3 stream, 10 00 SNPs, 2 000 individuals}
    \label{fig:timedist_LR_cov_10ks_2000i_gpu4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{timedist_LR_cov_10ks_2e+05i_gpu4.png}
    \caption{Distribution of time spent on the LR algorithm. 4 GPU 3 stream, 10 000 SNPs, 200 00 individuals}
    \label{fig:timedist_LR_cov_10ks_2e+05i_gpu4}
\end{figure}

\clearpage
\subsection{Time Distribution for Kernels}
Table~\ref{table:gpu_profile_calc_dist} shows how the time for doing calculations on the GPU is used based on the output from Nsight. Most of the time is spent on gemv2T which is matrix multiplication of a transposed matrix and a vector. It is used in the calculation of the scores vector, line 8 in algorithm~\ref{alg:lr}. The custom kernels takes up a small part of the calculations unless the number of covariates is high, see table~\ref{table:gpu_profile_calc_dist_cov}. About 10\% of the time is spent on the element wise multiplication.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | c | c |}
  \hline
  Kernel & Kernel type & 10ki & 100ki  \\
  \hline
  gemv2T & CUBLAS & 66.6 & 72.2\\
  block dot & CUBLAS & 18.9 & 14.4\\
  gemv2N & CUBLAS & 7.1 & 8.1\\
  ElementWiseMultiplication & Custom & 2.9 & 2.3\\
  LogisticTransform & Custom & 1.1 & 0.9\\
  LogLikelihoodParts & Custom & 0.8 & 0.6\\
  ElementWiseDifference & Custom & 0.7 & 0.6\\
  VectorMultiply1MinusVector & Custom & 0.7 & 0.5\\
  dot kernel & CUBLAS & 0.7 & 0.3\\
  reduce & CUBLAS & 0.5 & 0.1\\
  \hline  
\end{tabular}
\caption[Distribution of the time spent on kernels in the GPU calculations, 0 covariates]{Distribution of the time spent on kernels in the GPU calculations, numbers are in percentage of total time. 1 GPU, 9 streams, 0 covariates}
\label{table:gpu_profile_calc_dist}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{| l | l | c | c |}
  \hline
  Kernel & Kernel type & 10ki & 100ki  \\
  \hline
  gemv2T & CUBLAS & 43 & 53.9\\
  gemm & CUBLAS & 36.1 & 25.8\\
  ElementWiseMultiplication & Custom & 11.8 & 10.6\\
  gemv2N & CUBLAS & 5.9 & 7.4\\
  LogisticTransform & Custom & 0.7 & 0.7\\
  ElementWiseDifference & Custom & 0.5 & 0.4\\
  VectorMultiply1MinusVector & Custom & 0.4 & 0.4\\
  scal & CUBLAS & 0.4 & 0.1\\
  memset & CUBLAS & 0.3 & 0.1\\ 
  LogLikelihoodParts & Custom & 0.6 & 0.5\\
  dot kernel & CUBLAS & 0.3 & 0.2\\
  reduce & CUBLAS & 0.2 & 0.1\\
  \hline  
\end{tabular}
\caption[Distribution of the time spent on kernels in the GPU calculations, 20 covariates]{Distribution of the time spent on kernels in the GPU calculations, numbers are in percentage of total time. 1 GPU, 9 streams, 20 covariates}
\label{table:gpu_profile_calc_dist_cov}
\end{table}

\clearpage
\section{DataHandler}
The \textbf{ResultFileWriter} and \textbf{DataQueue} each has a lock to prevent race conditions. They also have a timer for how much time that has been spent waiting at the locks. There was very little time spent at them, no more than a couple of seconds even for the larger files which took 10-30 mins. The distribution of time spent between reading the SNPs data, recoding and applying the statistical model is mostly the same for all data sizes and number of GPUs. Most of the time is spent reading the SNPs, above 80\%. An example of some times is shown in figure~\ref{fig:timedist_DH_cov_10ks_2e+05i_gpu4}. It is hard to say if it is a problem due to the parallel nature of the program. It is possible that the time to read the SNPs can be hidden by the GPU calculations.

%TODO add 1 gpu 9 stream
\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{timedist_DH_cov_10ks_2e+05i_gpu4.png}
    \caption{Time distribution of reading SNPs, recoding and applying statistical model. 4 GPU, 3 streams, 10 000 SNPs, 200 000 individuals}
    \label{fig:timedist_DH_cov_10ks_2e+05i_gpu4}
\end{figure}

%TODO compare with calc vs read stuff

%\clearpage
%\section{Compiler Optimizations}
%TODO fix the plot and write
%There are several compiler options than could affect the speed. They are disabling aliasing for both CUDA and CPU, fast math option for CUDA as talked about in section~\ref{efficent_cuda} and general optimization options, O2 and O3.\\
%\\
%Results bla bla bla

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=9cm]{opt_comp_cov_ind_10000.png}
%    \caption{The relative time without the compiler options divided by with the compiler options. 10 000 SNPs, 10 000 individuals}
%    \label{fig:less_sync_vs_always_sync_ind_10000}
%\end{figure}

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=9cm]{opt_comp_ind_cov_20.png}
%    \caption{The relative time without the compiler options divided by with the compiler options. 10 000 SNPs, 20 covariates}
%    \label{fig:less_sync_vs_always_sync_cov_20}
%\end{figure}

\clearpage
\section{Comparison with GEISA}
Since CuEira uses the same CPU with the addition of GPUs it is expected that CuEira will have higher speed. The speedup varies depending on the size of the data, higher number of individuals and covariates increases the speedup. GEISA had problems with the dataset with 200 000 individuals, it took much longer than expected and was cancelled before completing. It could be a memory problem since GEISA stores everything in memory. A different computer with more memory(96GB) did not have the same problem.

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{geisa_saturated_speedup_ind.png}
    \caption{Speedup GEISA vs CuEira. 10 000 SNPs, 0 covariates. 200 000 individuals were excluded because GEISA had problems with that amount of data.}
    \label{fig:geisa_saturated_speedup_ind}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{geisa_saturated_speedup_cov.png}
    \caption{Speedup GEISA vs CuEira. 10 000 SNPs, 10 000 individuals}
    \label{fig:geisa_saturated_speedup_cov}
\end{figure}

\clearpage
\chapter{Discussion and Conclusions}
%Discussion and Conclusions (up to 3 pages) %TODO kernel opt sakerna
%TODO compare against GEISA
The CuEira implementation shows promising results, compared with GEISA it shows a significant speedup. CuEira also has almost full unit test coverage and is easy to extend and  modify due to the modularisation. This will allow the program to have a longer lifetime. For instance the logistic regression could be switched for a different method without rewriting the other parts of the program.\\
\\
CUBLAS was not much different to use than regular BLAS, however due to the higher number of parameters in their functions and lack of object orientation neither is user-friendly. Wrappers can make it easier to use. If the algorithm uses larger amounts of data than what fits inside the GPUs memory the complexity of the program increases significantly. Complex kernels can also be hard and time consuming to write and optimize. The wrappers made for CuEira could be used in the future for other projects and also be released as a library. The wrappers makes it easier to use basic CUDA and CUBLAS features.\\
\\ %TODO something about number of iterations
As previously mentioned in earlier chapters, CuEira has several compiling options that change some of its properties. The results show that single precision has better performance than double precision. Increased synchronisation can give better performance when the number of individuals is 100 000 and more. Increased synchronisation does decrease the transfer times but it does not always result in better performance.\\
\\
The programs efficiency with multiple GPUs could potentially be improved by moving more work from the CPU to the GPU. This might reduce the amount of work the CPUs have to perform and might improve the efficiency. If there is CPU power left it could be used for threads that perform the algorithm using CPUs only. Optimizing the custom kernels made for the program could improve the performance for datasets with high number of covariates. However it should be done after more parts are moved to the GPU since that might change which kernels needs to be optimized.\\
\\
Another potential improvement is to move the reading of the SNPs data from the file to separate threads and add a second queue. The new threads would then take SNPs from the old queue, read the SNPs data and put it in the new queue. Then when a worker thread fetches a SNP from the new queue its data is already in the memory. The time needed to read the SNPs would be the same, however it would reduce the time a thread spends between finishing one combination and starting the next combination. However it might be the case that the GPU already has enough work from the other streams to cover the time spent reading the SNPs data.\\
\\
Considering the efficiency of CuEira with multiple GPUs it is best to use it with one GPU, two if the number of individuals is low. However it depends on the GPUs and CPUs relative performance. Better CPU performance relative to the GPU might improve the scaling with multiple GPUs. The optimal number of streams per GPU is when the total number of streams is the same or slightly higher than the total number of CPU cores.

\chapter{Outlook}
GPUs are suitable for interaction depending on the method. They suit well with methods that consider each pair independently and that have extreme parallelism in the calculations for the combination. However doing even small parts on the CPU can reduce the performance. CUDA and CUBLAS could be easier to use after improvements to their interfaces or by using wrappers around them.\\
\\
The next step to improve the speed of CuEira would be to move more parts of the algorithm to the GPU and by using clusters. It is likely that the program would scale well to clusters because of the embarrassingly parallel nature, which SNPs to calculate is the only communication necessary. One possible reason to why CuEiras efficiency for multiple GPUs is low is that the CPU can not provide enough computations to the GPUs. A method for validating the results also needs to be implemented, for instance bootstrap or permutation tests.\\
\\
There is a need for better methods and definitions for interaction for non binary environmental factors. For interaction with binary environmental factors further speed might be gained by using gene-gene interaction methods since these methods often take advantage of the properties of binary variables. The definition of interaction could potentially be extended to equation~\ref{eq:additive_interaction_extended} to cover non-binary environmental factors. $x$ is the value of the environmental factor. The presence of interaction would then depend on the value of the environment factor.

\begin{equation}\label{eq:additive_interaction_extended}
OR_{interaction}^x>OR_{SNP}+OR_{environment}^x-1
\end{equation}

\newpage
\bibliographystyle{ieeetr}
\bibliography{hpc.bib,statistics.bib,misc.bib}

\begin{appendices}

\chapter{Lists}

\cleardoublepage
\addcontentsline{toc}{section}{List of Algorithms}
\listofalgorithmslist

\cleardoublepage
\addcontentsline{toc}{section}{List of Examples}
\listofexample

\cleardoublepage
\addcontentsline{toc}{section}{List of Figures}
\listoffigures

\cleardoublepage
\addcontentsline{toc}{section}{List of Tables}
\listoftables

\chapter{File Formats}
\label{file_formats}

\section{PLINK Data Format}
\label{plink_file}
%TODO plink file stuff
The PLINK files can be in different formats~\cite{plink_format}. The format described here is the binary Plink files. It consists of three files. Bed, bim and fam~\cite{plink_format}. The fam file contains the information about the individuals, the outcome, the persons and familys id, gender and parents id. The bim file contains the information about the SNPs but not their data. It has the SNPs id, chromosome, position on the chromosome and the alleles. The bed file has the actual SNP data in a binary format. Each pair of 0 and 1 is the genotype for one individual. See~\cite{plink_format} for the exact details and information about the other Plink file formats.

\section{Environmental and Covariates File Format}
The environmental factors and covariates are stored in separate files with the same format, one file for environmental factors and one for the covariates if any. One of the columns needs to contain the individual ids, the rest of the columns should be data. The delimiter can be any reasonable string(e.g. something that is not part of a name or number) and there is an option to set the delimiter in the program. The default is tab delimited.

\section{CuEira Result File Format}
The results from CuEira are written in a csv file. The columns are described below.

\begin{description}
\item[snp\_id] \hfill \\
  The id of the SNP from the bim file
\item[pos] \hfill \\
  The row which the SNP had in the plink files
\item[skip] \hfill \\
  Numbers explaining why the SNP was excluded from calculation. 1=missing data. 2=low MAF, 3=low cell count, 4=negative position in bim file
\item[risk\_allele] \hfill \\
  The risk allele, not changed based on recoding
\item[minor] \hfill \\
  The minor allele
\item[major] \hfill \\
  The major allele
\item[env\_id] \hfill \\
  The id of the environmental factor, it is the column from the environmental file
\item[no\_alleles\_X] \hfill \\
  The numbers of the alleles in group X
\item[freq\_alleles\_X] \hfill \\
  The frequencies of the alleles group X
\item[no\_snpX\_envY] \hfill \\
  Cell distribution with exposure X and Y
\item[numberOfIterations] \hfill \\
  The number of iterations the LR model took to converge, one column for the multiplicative model and one for the additive
\item[ap] \hfill \\
  The AP value for the additive model described in~\ref{statistic_measures}
\item[reri] \hfill \\
  The RERI value for the additive model described in~\ref{statistic_measures}
\item[OR] \hfill \\
  The odds ratios for the SNP, environmental factor and interaction. Add is for the additive model and mult is for the multiplicative. L is the lower confidence interval and H is the upper. The interval is from using the delta method~\cite{uvehag_master_thesis}
\item[recode] \hfill \\
  Case of recoding, 0=no recoding, 1=snp protective, 2=environment protective, 3=interaction protective
\end{description}

Each line after the header is a combination of a SNP and an environmental factor. Because of the parallel nature of the program the rows are not in any specific order. The column pos contains the row number that the SNP had in the plink files. This can be used to sort the data in.\\
\\
The data in some columns are changed depending on the recoding. The multiplicative model is calculated using the recode from the additive model. The cell frequencies are also changed. However the risk allele is not changed.

\chapter{Risk Allele Definition Differences}
\label{risk_allele_appendix}
This appendix contains a more detailed look at different risk allele definitions than section~\ref{risk_allele}.\
\\
The code for how JEIRA and GEISA calculates the risk allele is not the same as its definition~\cite{geira, uvehag_master_thesis} talked about previously in section~\ref{risk_allele}. The algorithm JEIRA and GEISA uses is shown in algorithm~\ref{alg:jeira_geisa_risk_allele}.

\begin{algorithm}
\DontPrintSemicolon
\SetKwInOut{data}{Data}
\BlankLine \BlankLine

\data{
$caseMaxAllele$ is the most common allele in case and $controlMaxAllele$ is the most common in control\\
$caseMaxRatio$ and $controlMaxRatio$ is the frequency of $caseMaxAllele$ and $controlMaxAllele$ respectively.\\
}

\BlankLine \BlankLine

\If{$caseMaxRatio > controlMaxRatio$ \KwSty{and} $caseMaxAllele=controlMaxAllele$}{
  $riskAllele \longleftarrow caseMaxAllele$\;
}\Else{
  $riskAllele \longleftarrow caseMinAllele$\;
}

\caption{JEIRA and GEISA pseudo code to determine the risk allele}
\algorithmslist{\quad JEIRA and GEISA pseudo code to determine the risk allele}
\label{alg:jeira_geisa_risk_allele}
\end{algorithm}

The allele frequencies can be shown in a $2 \times 2$ table. The columns are the frequencies for the different alleles and the rows are the groups(case and control). It can be split into four cases shown in table~\ref{table:risk_allele_cases}. The four cases comes from the four cases of possible directions of the inequality between the maximum frequencies of the two groups, shown as the bold arrows. There is up, down, down diagonally and up diagonally, call them case 1 to 4. The rest of the inequalities can be found by using symmetry and that the sum of each row is 1.\\

\begin{table}[h]
\centering
\begin{tabular}{| l | c c c | c | c c c |}
  \hline
   & Allele1 & & Allele2 & & Allele1 & & Allele2  \\
  \hline
  Case & Max & $\rightarrow$ & Min & & Max & $\rightarrow$ & Min \\
   & $\boldsymbol{\downarrow}$ & $\nese$ & $\uparrow$ & & $\boldsymbol{\uparrow}$ & $\nese$ & $\downarrow$ \\
  Control & Max & $\rightarrow$ & Min & & Max & $\rightarrow$ & Min \\
  \hline 
  & & & & & & &\\
  \hline
  Case & Max & $\rightarrow$ & Min & & Max & $\rightarrow$ & Min \\
   & $\downarrow$ & $\nesecol$ & $\uparrow$ & & $\downarrow$ & $\nwsw$ & $\uparrow$ \\
  Control & Min & $\leftarrow$ & Max & & Min & $\leftarrow$ & Max \\
  \hline  
\end{tabular}
\caption[The four cases of allele frequencies]{The four cases of allele frequencies. Case 1 upper left, case 2 upper right, case 3 lower left, case 4 lower right}
\label{table:risk_allele_cases}
\end{table}

By using the previously mentioned definitions in section~\ref{risk_allele} and the JEIRAs and GEISAs algorithm~\ref{alg:jeira_geisa_risk_allele} what the risk allele is according to them for each case is shown in table~\ref{table:risk_allele_comp}. Each definition correspond to examining the inequalities in the cases. For the definition using MAF the direction of the inequality from case minimum to control minimum determines it. If it points towards control the the case minor allele is the risk allele. For JEIRA and GEISA if the maximum for case points straight down to the maximum for control then the major case allele is the risk allele. This only happens in case 1. For increased frequency in case it is the allele which has the inequality pointing straight down.\\

\begin{table}[h]
\centering
\begin{tabular}{| l | c c c c |}
  \hline
  Definition & Case 1 & Case 2 & Case 3 & Case 4  \\
  \hline
  MAF based & Allele1 & Allele2 & Allele1 & Allele2 \\
  JEIRA GEISA algorithm & Allele1 & Allele2 & Allele2 & Allele2 \\
  Increase frequency in case & Allele1 & Allele2 & Allele1 & Allele1 \\
  \hline  
\end{tabular}
\caption{Risk alleles for the cases in table~\ref{table:risk_allele_cases} based on the different definitions}
\label{table:risk_allele_comp}
\end{table}

The risk allele is different for case 3 and case 4. For case 3 it is clear that it should be Allele1 because it has the highest frequency in case and also higher in case than control An extreme case of case 3 is shown in table~\ref{table:risk_allele_extreme_case_3}.\\
\\
By examining a similar extreme case of case 4 it is clear that Allele1 should be the risk allele in that case too. The case is shown in table~\ref{table:risk_allele_extreme_case_4}. Almost everyone in the case group has Allele1 while no one in the control group does, it is unlikely that Allele2 would increase the risk in that case.

\begin{table}[h]
\centering
\begin{tabular}{| l | c c |}
  \hline
   & Allele1 & Allele2  \\
  \hline
  Case & 1 & 0.00  \\
  Control & 0.01 & 0.99\\
  \hline  
\end{tabular}
\caption{An extreme case of frequencies of case 3 in table~\ref{table:risk_allele_cases}}
\label{table:risk_allele_extreme_case_3}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{| l | c c |}
  \hline
   & Allele1 & Allele2  \\
  \hline
  Case & 0.99 & 0.01  \\
  Control & 0 & 1  \\
  \hline  
\end{tabular}
\caption{An extreme case of frequencies of case 4 in table~\ref{table:risk_allele_cases}}
\label{table:risk_allele_extreme_case_4}
\end{table}

\chapter{Bugs and Issues}
This is a small collection of various bugs and limitations encountered in the course of the project that can be good to know for anyone that wants to compile CuEira or if the libraries are used in other projects.

\section{Bugs}
\label{found_bugs}
There are several problems with Intel compilers and libraries. Most libraries work fine with gnu C/C++ compiler but have problems with Intel compilers. Boost 1.55 does not work with Intel compilers, 1.54 works. AllOf in Google Test also does not work together with Intel compilers.\\
\\
JEIRA prints the wrong results for the multiplicative model and as mentioned in appendix~\ref{risk_allele_appendix} the risk allele algorithm JEIRA uses is not the same as the definition.

\section{Issues}
Nvcc(i.e. the compiler for CUDA) can not compile C++11 code and no interface included in the code that nvcc compiles can use c++11. This problem can largely be avoided by using separate compilation. The CUDA find package module for CMake(included in version 2.8 and later) has a default flag that should be removed as it can cause problems. The flag tells nvcc to propagate the host code flags, if the compiler for the host code then uses C++11 that flag is propagated to nvcc and it will not compile The flag is disabled in CMake by using set(CUDA\_PROPAGATE\_HOST\_FLAGS off).

\chapter{How to Compile and Use CuEira}
\label{compile_cueira}
The source code for the program is available at github, \url{https://github.com/Berjiz/CuEira}.

\begin{description}
    \item[List of dependencies. Listed version is the version used.]
\end{description}
\begin{itemize}
 \item Boost, tested with version 1.54, 1.55 does not work due to a bug with Intel compilers
 \item CMake, version 2.8
 \item CUDA, version 5.5
 \item MKL, version 13.1
 \item Intel compiler version 14.0
 \item Google Test and Google Mock, already included in the folder.
\end{itemize}

First install the dependencies, they might need to be compiled from source. Open a terminal and make a new directory where you want the program to be compiled. Enter the directory and write:\\
cmake /path/to/CuEira/\\
Then type:\\
make\\
\\
The program should now be compiling. CMake migh not finding all dependencies. If it does you need to tell CMake where it is. The path to boost is set by using EXPORT BOOST\_ROOT = /path/to/boost/ before using CMake. The path to the compilers is done with two options to the CMake command, CXX = /path/to/source for the C++ compiler and C = /path/to/source for the C compiler.\\ %TODO how to set cuda
\\
In the build/bin directory there is two executable files, CuEira and CuEira\_Test. CuEira is the program itself while CuEira\_Test runs all the tests for the program. The option -h or --help lists possible options for the program. The most basic command to run CuEira is -b data -e environment file -x name of column with snp ids
%TODO usage

\end{appendices}

\end{document}